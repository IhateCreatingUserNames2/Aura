# -------------------- agent_manager.py --------------------

# ==================== Complete Fixed agent_manager.py ====================
"""
Enhanced AgentManager with NCF-powered AuraAgentInstance - FIXED VERSION
Every created agent now has full NCF capabilities by default.
Fixed: All async issues and variable scope problems
"""

import os
import json
import time
import uuid
import asyncio
from typing import Dict, Optional, Any
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

from memory_system.memory_blossom import MemoryBlossom
from memory_system.memory_connector import MemoryConnector
from google.adk.agents import LlmAgent
from google.adk.models.lite_llm import LiteLlm
from google.adk.tools import FunctionTool
from google.adk.sessions import InMemorySessionService
from google.adk.runners import Runner
from google.genai.types import Content as ADKContent, Part as ADKPart
from google.adk.events import Event, EventActions

# Import NCF processing functions with FIXED signatures
from ncf_processing import (
    NCF_AGENT_INSTRUCTION,
    get_narrativa_de_fundamento_pilar1,
    get_rag_info_pilar2,
    format_chat_history_pilar3,
    montar_prompt_aura_ncf,
    aura_reflector_analisar_interacao
)

import logging

logger = logging.getLogger(__name__)


@dataclass
class AgentConfig:
    agent_id: str
    user_id: str
    name: str
    persona: str
    detailed_persona: str
    model: str = "openrouter/openai/gpt-4o-mini"
    memory_path: Optional[str] = None
    created_at: datetime = None
    settings: Dict[str, Any] = None


class AgentManager:
    """Manages multiple NCF-enabled Aura agent instances for different users"""

    def __init__(self, base_storage_path: str = "agent_storage"):
        self.base_storage_path = Path(base_storage_path)
        self.base_storage_path.mkdir(exist_ok=True)

        # Cache of active agents
        self._active_agents: Dict[str, 'NCFAuraAgentInstance'] = {}
        self._agent_configs: Dict[str, AgentConfig] = {}

        # Load existing agent configurations
        self._load_agent_configs()

    def create_agent(self, user_id: str, name: str, persona: str,
                     detailed_persona: str, model: str = None) -> str:
        """Create a new NCF-enabled Aura agent for a user"""
        try:
            agent_id = str(uuid.uuid4())

            # Create agent-specific storage directory
            agent_path = self.base_storage_path / user_id / agent_id
            agent_path.mkdir(parents=True, exist_ok=True)

            # Memory persistence path - each agent gets its own MemoryBlossom
            memory_path = str(agent_path / "memory_blossom.json")

            config = AgentConfig(
                agent_id=agent_id,
                user_id=user_id,
                name=name,
                persona=persona,
                detailed_persona=detailed_persona,
                model=model or "openrouter/openai/gpt-4o-mini",
                memory_path=memory_path,
                created_at=datetime.now(),
                settings={}
            )

            # Save configuration
            self._save_agent_config(config)
            self._agent_configs[agent_id] = config

            logger.info(f"Created NCF-enabled agent '{name}' (ID: {agent_id}) for user {user_id}")
            return agent_id

        except Exception as e:
            logger.error(f"Error creating NCF-enabled agent: {e}", exc_info=True)
            raise

    def get_agent_instance(self, agent_id: str) -> Optional['NCFAuraAgentInstance']:
        """Get or create a NCF-enabled agent instance"""
        try:
            if agent_id not in self._agent_configs:
                return None

            # Return cached instance if available
            if agent_id in self._active_agents:
                return self._active_agents[agent_id]

            # Create new NCF-enabled instance
            config = self._agent_configs[agent_id]
            instance = NCFAuraAgentInstance(config)
            self._active_agents[agent_id] = instance

            return instance

        except Exception as e:
            logger.error(f"Error getting agent instance {agent_id}: {e}", exc_info=True)
            return None

    def list_user_agents(self, user_id: str) -> list[AgentConfig]:
        """List all agents for a user"""
        return [config for config in self._agent_configs.values()
                if config.user_id == user_id]

    def delete_agent(self, agent_id: str, user_id: str) -> bool:
        """Delete an agent (only by owner)"""
        try:
            if agent_id not in self._agent_configs:
                return False

            config = self._agent_configs[agent_id]
            if config.user_id != user_id:
                return False  # Not owner

            # Remove from active instances
            if agent_id in self._active_agents:
                del self._active_agents[agent_id]

            # Delete storage
            import shutil
            agent_path = self.base_storage_path / config.user_id / agent_id
            if agent_path.exists():
                shutil.rmtree(agent_path)

            # Remove config
            del self._agent_configs[agent_id]
            config_path = self.base_storage_path / config.user_id / f"{agent_id}.json"
            if config_path.exists():
                config_path.unlink()

            logger.info(f"Deleted agent {agent_id} for user {user_id}")
            return True

        except Exception as e:
            logger.error(f"Error deleting agent {agent_id}: {e}", exc_info=True)
            return False

    def _save_agent_config(self, config: AgentConfig):
        """Save agent configuration to disk"""
        try:
            user_path = self.base_storage_path / config.user_id
            user_path.mkdir(exist_ok=True)

            config_path = user_path / f"{config.agent_id}.json"
            with open(config_path, 'w') as f:
                json.dump({
                    'agent_id': config.agent_id,
                    'user_id': config.user_id,
                    'name': config.name,
                    'persona': config.persona,
                    'detailed_persona': config.detailed_persona,
                    'model': config.model,
                    'memory_path': config.memory_path,
                    'created_at': config.created_at.isoformat(),
                    'settings': config.settings or {}
                }, f, indent=2)

        except Exception as e:
            logger.error(f"Error saving agent config: {e}", exc_info=True)
            raise

    def _load_agent_configs(self):
        """Load all agent configurations from disk"""
        try:
            for user_dir in self.base_storage_path.iterdir():
                if user_dir.is_dir():
                    for config_file in user_dir.glob("*.json"):
                        # Skip memory files
                        if config_file.name == "memory_blossom.json":
                            continue

                        try:
                            with open(config_file, 'r') as f:
                                data = json.load(f)

                            if not all(k in data for k in
                                       ['agent_id', 'user_id', 'name', 'persona', 'detailed_persona', 'created_at']):
                                logger.warning(f"Config file {config_file} is missing essential keys. Skipping.")
                                continue

                            config = AgentConfig(
                                agent_id=data['agent_id'],
                                user_id=data['user_id'],
                                name=data['name'],
                                persona=data['persona'],
                                detailed_persona=data['detailed_persona'],
                                model=data.get('model', 'openrouter/openai/gpt-4o-mini'),
                                memory_path=data.get('memory_path'),
                                created_at=datetime.fromisoformat(data['created_at']),
                                settings=data.get('settings', {})
                            )

                            if not config.memory_path:
                                config.memory_path = str(
                                    self.base_storage_path / config.user_id / config.agent_id / "memory_blossom.json")

                            self._agent_configs[config.agent_id] = config
                            logger.info(f"Loaded agent config: {config.name} (ID: {config.agent_id})")

                        except Exception as e:
                            logger.error(f"Error loading config {config_file}: {e}")

        except Exception as e:
            logger.error(f"Error loading agent configs: {e}", exc_info=True)


class NCFAuraAgentInstance:
    """
    Individual NCF-enabled Aura agent instance with its own memory and session management.
    Every instance has full NCF capabilities by default.
    """

    def __init__(self, config: AgentConfig):
        self.config = config
        logger.info(f"Initializing NCF-enabled agent: {config.name} (ID: {config.agent_id})")

        try:
            # Ensure memory path is set
            if not self.config.memory_path:
                self.config.memory_path = str(Path(
                    "agent_storage") / self.config.user_id / self.config.agent_id / "memory_blossom.json")

            # Initialize isolated MemoryBlossom for this agent
            try:
                from enhanced_memory_system import EnhancedMemoryBlossom
                self.memory_blossom = EnhancedMemoryBlossom(
                    persistence_path=config.memory_path,
                    enable_adaptive_rag=True,  # Enable GenLang features
                    cluster_threshold=0.75  # Adjust clustering sensitivity
                )
                logger.info(f"Using Enhanced MemoryBlossom with Adaptive RAG for {config.name}")
            except ImportError:
                logger.warning(f"Enhanced MemoryBlossom not available, falling back to standard MemoryBlossom")
                self.memory_blossom = MemoryBlossom(persistence_path=config.memory_path)

            self.memory_connector = MemoryConnector(self.memory_blossom)
            self.memory_blossom.set_memory_connector(self.memory_connector)

            # Initialize LLM for this agent
            self.model = LiteLlm(model=config.model)

            # Create memory management tools
            self.add_memory_tool = FunctionTool(func=self._create_add_memory_func())
            self.recall_memories_tool = FunctionTool(func=self._create_recall_memories_func())

            # Create the NCF-powered ADK agent
            self.adk_agent = self._create_ncf_adk_agent()

            # Session management
            self.session_service = InMemorySessionService()
            self.runner = Runner(
                agent=self.adk_agent,
                app_name=f"NCFAura_{config.agent_id}",
                session_service=self.session_service
            )
            self.active_sessions: Dict[str, str] = {}

            logger.info(f"NCF agent '{config.name}' initialized successfully with isolated memory system")

        except Exception as e:
            logger.error(f"Error initializing NCF agent instance: {e}", exc_info=True)
            raise

    def _create_ncf_adk_agent(self) -> LlmAgent:
        """Create the NCF-powered ADK agent with sophisticated instruction"""
        return LlmAgent(
            name=self.config.name,
            model=self.model,
            instruction=NCF_AGENT_INSTRUCTION,  # Use the sophisticated NCF instruction
            tools=[self.add_memory_tool, self.recall_memories_tool]
        )

    def _create_add_memory_func(self):
        """Create the add_memory function for the agent's tools"""

        def add_memory(content: str, memory_type: str,
                       emotion_score: float = 0.0,
                       initial_salience: float = 0.5,
                       metadata_json: Optional[str] = None,
                       domain_context: str = "general",
                       performance_score: float = 0.5,
                       tool_context=None) -> Dict[str, Any]:
            try:
                custom_metadata = json.loads(metadata_json) if metadata_json else {}
                custom_metadata['agent_id'] = self.config.agent_id
                custom_metadata['agent_name'] = self.config.name
                custom_metadata['domain_context'] = domain_context
                custom_metadata['performance_score'] = performance_score

                # Check if enhanced memory system is available
                if hasattr(self.memory_blossom, 'enable_adaptive_rag') and self.memory_blossom.enable_adaptive_rag:
                    memory = self.memory_blossom.add_memory(
                        content=content,
                        memory_type=memory_type,
                        custom_metadata=custom_metadata,
                        emotion_score=emotion_score,
                        initial_salience=initial_salience,
                        performance_score=performance_score,
                        domain_context=domain_context
                    )
                    message_suffix = " (Enhanced with Adaptive RAG)"
                else:
                    memory = self.memory_blossom.add_memory(
                        content=content,
                        memory_type=memory_type,
                        custom_metadata=custom_metadata,
                        emotion_score=emotion_score,
                        initial_salience=initial_salience
                    )
                    message_suffix = " (Standard MemoryBlossom)"

                self.memory_blossom.save_memories()

                return {
                    "status": "success",
                    "memory_id": memory.id,
                    "message": f"Memory stored successfully for {self.config.name} (ID: {memory.id}){message_suffix}",
                    "adaptive_rag_enabled": hasattr(self.memory_blossom, 'enable_adaptive_rag')
                }
            except Exception as e:
                logger.error(f"Error adding memory for agent {self.config.agent_id}: {e}")
                return {"status": "error", "message": str(e)}

        return add_memory

    def _create_recall_memories_func(self):
        """Create the recall_memories function for the agent's tools"""

        def recall_memories(query: str,
                            target_memory_types_json: Optional[str] = None,
                            top_k: int = 3,
                            domain_context: str = "general",
                            tool_context=None) -> Dict[str, Any]:
            try:
                target_types = None
                if target_memory_types_json:
                    target_types = json.loads(target_memory_types_json)

                conversation_history_for_retrieval = None
                if tool_context and tool_context.state and 'conversation_history' in tool_context.state:
                    conversation_history_for_retrieval = tool_context.state['conversation_history']

                # Use enhanced retrieval if available
                if hasattr(self.memory_blossom, 'adaptive_retrieve_memories'):
                    memories = self.memory_blossom.adaptive_retrieve_memories(
                        query=query,
                        target_memory_types=target_types,
                        domain_context=domain_context,
                        top_k=top_k,
                        use_performance_weighting=True,
                        conversation_context=conversation_history_for_retrieval
                    )
                    retrieval_method = "Enhanced Adaptive RAG"
                else:
                    memories = self.memory_blossom.retrieve_memories(
                        query=query,
                        target_memory_types=target_types,
                        top_k=top_k,
                        conversation_context=conversation_history_for_retrieval
                    )
                    retrieval_method = "Standard MemoryBlossom"

                return {
                    "status": "success",
                    "count": len(memories),
                    "memories": [mem.to_dict() for mem in memories],
                    "retrieval_method": retrieval_method,
                    "adaptive_rag_enabled": hasattr(self.memory_blossom, 'adaptive_retrieve_memories')
                }
            except Exception as e:
                logger.error(f"Error recalling memories for agent {self.config.agent_id}: {e}")
                return {"status": "error", "message": str(e)}

        return recall_memories

    async def process_message(self, user_id: str, session_id: str, message: str) -> str:
        """
        Process a message with full NCF capabilities:
        1. Build context (Narrative Foundation, RAG, Chat History)
        2. Construct NCF prompt
        3. Run ADK agent
        4. Analyze interaction with Reflector

        FIXED: All async issues resolved
        """
        logger.info(f"Processing message for NCF agent '{self.config.name}': '{message[:100]}...'")

        try:
            # ============ SESSION MANAGEMENT ============
            # Generate ADK session management
            adk_session_key = f"{user_id}_{session_id}"
            adk_session_id = self.active_sessions.get(adk_session_key)
            session_created_now = False

            if not adk_session_id:
                adk_session_id = f"ncf_adk_{self.config.agent_id}_{adk_session_key}_{datetime.now().timestamp()}"
                self.active_sessions[adk_session_key] = adk_session_id
                session_created_now = True

            # Initialize or retrieve session state - FIXED: Define this first
            current_session_state = {'conversation_history': [], 'foundation_narrative_turn_count': 0}

            if session_created_now:
                # FIXED: Await the async session creation
                created_session = await self.session_service.create_session(
                    app_name=f"NCFAura_{self.config.agent_id}",
                    user_id=user_id,
                    session_id=adk_session_id,
                    state=current_session_state
                )
                logger.info(f"Created new session {adk_session_id}")
            else:
                # FIXED: Await the async session retrieval
                retrieved_session = await self.session_service.get_session(
                    app_name=f"NCFAura_{self.config.agent_id}",
                    user_id=user_id,
                    session_id=adk_session_id
                )
                if retrieved_session and retrieved_session.state:
                    current_session_state = retrieved_session.state
                else:
                    current_session_state = {'conversation_history': [], 'foundation_narrative_turn_count': 0}
                logger.info(f"Retrieved existing session {adk_session_id}")

            # Add user message to conversation history
            current_session_state['conversation_history'].append({
                'role': 'user',
                'content': message
            })

            # ============ DOMAIN DETECTION ============
            domain_context = await self._detect_domain_context(message, current_session_state)
            logger.info(f"Detected domain context: {domain_context}")

            # ============ NCF PROMPT CONSTRUCTION ============
            logger.info(f"Building NCF context for agent '{self.config.name}'...")

            # Pilar 1: Narrative Foundation
            narrativa_fundamento = await get_narrativa_de_fundamento_pilar1(
                session_state=current_session_state,
                memory_blossom=self.memory_blossom,
                user_id=user_id,
                llm_instance=self.model,
                agent_name=self.config.name,
                agent_persona=self.config.persona
            )

            # Pilar 2: RAG Information (Enhanced with domain context)
            try:
                # Try enhanced RAG first
                if hasattr(self.memory_blossom, 'adaptive_retrieve_memories'):
                    rag_memories = self.memory_blossom.adaptive_retrieve_memories(
                        query=message,
                        domain_context=domain_context,
                        top_k=3,
                        use_performance_weighting=True,
                        conversation_context=current_session_state.get('conversation_history', [])
                    )
                    logger.info(f"Used Enhanced Adaptive RAG, retrieved {len(rag_memories)} memories")
                else:
                    rag_memories = self.memory_blossom.retrieve_memories(
                        query=message,
                        top_k=3,
                        conversation_context=current_session_state.get('conversation_history', [])
                    )
                    logger.info(f"Used Standard RAG, retrieved {len(rag_memories)} memories")

                rag_info_list = [mem.to_dict() for mem in rag_memories]

            except Exception as e:
                logger.error(f"Error in RAG retrieval: {e}")
                rag_info_list = []

            # Pilar 3: Chat History
            chat_history_str = format_chat_history_pilar3(
                chat_history_list=current_session_state['conversation_history']
            )

            # Construct final NCF prompt
            final_ncf_prompt = montar_prompt_aura_ncf(
                agent_name=self.config.name,
                agent_detailed_persona=self.config.detailed_persona,
                narrativa_fundamento=narrativa_fundamento,
                informacoes_rag_list=rag_info_list,
                chat_history_recente_str=chat_history_str,
                user_reply=message
            )

            # ============ RUN ADK AGENT ============
            # FIXED: Update session state before running agent with proper async handling
            try:
                # FIXED: Await the async session retrieval
                session_for_update = await self.session_service.get_session(
                    app_name=f"NCFAura_{self.config.agent_id}",
                    user_id=user_id,
                    session_id=adk_session_id
                )
                if session_for_update:
                    # Update the session state
                    session_for_update.state = current_session_state

                    # FIXED: Use update_session if available, otherwise skip
                    if hasattr(self.session_service, 'update_session'):
                        # Check if update_session is async
                        if asyncio.iscoroutinefunction(self.session_service.update_session):
                            await self.session_service.update_session(session_for_update)
                        else:
                            self.session_service.update_session(session_for_update)
                    # If no update_session method, the state is already updated in memory

            except Exception as e:
                logger.warning(f"Error updating session state: {e}")
                # Continue execution - this is not critical

            # Run the NCF-powered agent
            adk_message = ADKContent(role="user", parts=[ADKPart(text=final_ncf_prompt)])
            response_text = ""

            try:
                async for event in self.runner.run_async(
                        user_id=user_id,
                        session_id=adk_session_id,
                        new_message=adk_message
                ):
                    if event.is_final_response():
                        if event.content and event.content.parts:
                            response_text = event.content.parts[0].text
                        break
            except Exception as e:
                logger.error(f"Error running ADK agent: {e}")
                response_text = f"Desculpe, houve um problema na geração da resposta. Como {self.config.name}, vou tentar ajudá-lo da melhor forma possível."

            response_text = response_text or f"({self.config.name} não forneceu uma resposta para este turno)"

            # Add assistant response to conversation history
            current_session_state['conversation_history'].append({
                'role': 'assistant',
                'content': response_text
            })

            # ============ REFLECTOR ANALYSIS ============
            logger.info(f"Running reflector analysis for agent '{self.config.name}'...")
            try:
                await aura_reflector_analisar_interacao(
                    user_utterance=message,
                    prompt_ncf_usado=final_ncf_prompt,
                    resposta_de_aura=response_text,
                    memory_blossom=self.memory_blossom,
                    user_id=user_id,
                    llm_instance=self.model,
                    domain_context=domain_context  # Pass detected domain
                )
            except Exception as e:
                logger.error(f"Error in reflector analysis: {e}")
                # Continue execution - reflector is not critical for response

            # ============ FINAL SESSION UPDATE ============
            # FIXED: Update final session state with proper async handling
            try:
                # FIXED: Await the async session retrieval
                final_session_state = await self.session_service.get_session(
                    app_name=f"NCFAura_{self.config.agent_id}",
                    user_id=user_id,
                    session_id=adk_session_id
                )
                if final_session_state:
                    # Update the final session state
                    final_session_state.state = current_session_state

                    # FIXED: Use update_session if available
                    if hasattr(self.session_service, 'update_session'):
                        # Check if update_session is async
                        if asyncio.iscoroutinefunction(self.session_service.update_session):
                            await self.session_service.update_session(final_session_state)
                        else:
                            self.session_service.update_session(final_session_state)
                    # If no update_session method, the state is already updated in memory

            except Exception as e:
                logger.warning(f"Error in final session update: {e}")
                # Continue - this is not critical for response

            logger.info(f"Message processed successfully by NCF agent '{self.config.name}'")
            return response_text

        except Exception as e:
            logger.error(f"Error processing message for NCF agent {self.config.name}: {e}", exc_info=True)
            return f"Desculpe, houve um erro interno. Como {self.config.name}, tentarei ajudá-lo da melhor forma possível."

    async def _detect_domain_context(self, message: str, session_state: Dict[str, Any]) -> str:
        """Detect conversation domain from user message"""
        try:
            # Simple keyword-based detection (can be enhanced with ML)
            message_lower = message.lower()

            # Enhanced keyword detection with more comprehensive lists
            domain_keywords = {
                'physics': ['physics', 'quantum', 'mechanics', 'energy', 'force', 'wave', 'particle',
                            'momentum', 'velocity', 'acceleration', 'relativity', 'thermodynamics'],
                'mathematics': ['calculate', 'equation', 'solve', 'mathematics', 'algebra', 'geometry',
                                'calculus', 'derivative', 'integral', 'function', 'graph', 'variable'],
                'emotional_support': ['feel', 'sad', 'happy', 'worried', 'excited', 'frustrated', 'love',
                                      'anxious', 'depressed', 'angry', 'afraid', 'emotional', 'mood'],
                'creative_writing': ['story', 'write', 'creative', 'imagine', 'design', 'art', 'poem',
                                     'character', 'plot', 'narrative', 'fiction', 'novel'],
                'programming': ['code', 'programming', 'function', 'variable', 'algorithm', 'debug',
                                'software', 'python', 'javascript', 'html', 'css'],
                'science': ['biology', 'chemistry', 'experiment', 'hypothesis', 'research', 'study',
                            'scientific', 'theory', 'observation', 'data'],
                'health': ['health', 'medical', 'doctor', 'medicine', 'symptoms', 'treatment',
                           'illness', 'wellness', 'exercise', 'nutrition'],
                'education': ['learn', 'study', 'school', 'university', 'education', 'teaching',
                              'homework', 'assignment', 'exam', 'knowledge']
            }

            # Check for domain matches
            for domain, keywords in domain_keywords.items():
                if any(keyword in message_lower for keyword in keywords):
                    logger.debug(
                        f"Domain '{domain}' detected from keywords: {[k for k in keywords if k in message_lower]}")
                    return domain

            # Check conversation history for context
            if session_state and 'conversation_history' in session_state:
                recent_messages = session_state['conversation_history'][-5:]  # Last 5 messages
                history_text = ' '.join([msg.get('content', '') for msg in recent_messages]).lower()

                for domain, keywords in domain_keywords.items():
                    keyword_count = sum(1 for keyword in keywords if keyword in history_text)
                    if keyword_count >= 2:  # If multiple keywords from same domain in recent history
                        logger.debug(f"Domain '{domain}' detected from conversation history")
                        return domain

            return "general"

        except Exception as e:
            logger.error(f"Error detecting domain context: {e}")
            return "general"

    def get_enhanced_stats(self) -> Dict[str, Any]:
        """Get enhanced statistics including adaptive RAG if available"""
        try:
            base_stats = {
                "agent_id": self.config.agent_id,
                "agent_name": self.config.name,
                "model": self.config.model,
                "created_at": self.config.created_at.isoformat() if self.config.created_at else None,
                "active_sessions": len(self.active_sessions),
                "memory_system_type": "Enhanced" if hasattr(self.memory_blossom, 'enable_adaptive_rag') else "Standard"
            }

            # Add adaptive stats if available
            if hasattr(self.memory_blossom, 'get_adaptive_stats'):
                adaptive_stats = self.memory_blossom.get_adaptive_stats()
                base_stats["adaptive_rag"] = adaptive_stats
            else:
                base_stats["adaptive_rag"] = {"enabled": False}

            # Add basic memory stats
            if hasattr(self.memory_blossom, 'memory_stores'):
                base_stats["memory_stores"] = {
                    mem_type: len(mem_list)
                    for mem_type, mem_list in self.memory_blossom.memory_stores.items()
                }
                base_stats["total_memories"] = sum(base_stats["memory_stores"].values())

            return base_stats

        except Exception as e:
            logger.error(f"Error getting enhanced stats: {e}")
            return {
                "agent_id": self.config.agent_id,
                "agent_name": self.config.name,
                "error": str(e)
            }

# -------------------- check_filesystem_state.py --------------------

# Quick check of current file system state
import json
from pathlib import Path


def check_current_state():
    """Check the current state of agent configurations in file system"""

    # Values from your database
    EXPECTED_USER_ID = "40c8d42f-858d-4060-b5be-8cef6480e9a3"  # xupeta
    TARGET_AGENT_ID = "4974f67b-6a43-4eee-ba2b-2f9511fe6260"  # Aura

    agent_storage_path = Path("agent_storage")

    print("=== Current File System State ===")
    print(f"Agent storage path: {agent_storage_path.absolute()}")
    print(f"Expected user_id: {EXPECTED_USER_ID}")
    print(f"Target agent_id: {TARGET_AGENT_ID}")

    if not agent_storage_path.exists():
        print("❌ Agent storage directory does not exist!")
        return

    print(f"\nDirectory structure:")
    for item in agent_storage_path.iterdir():
        if item.is_dir():
            print(f"📁 {item.name}/")

            # Check if this is the expected user directory
            is_expected_user = item.name == EXPECTED_USER_ID
            if is_expected_user:
                print(f"   ✅ This is xupeta's user directory")

            # List JSON files
            json_files = list(item.glob("*.json"))
            for json_file in json_files:
                if json_file.name == "memory_blossom.json":
                    print(f"   📄 {json_file.name} (memory file)")
                    continue

                try:
                    with open(json_file, 'r') as f:
                        data = json.load(f)

                    agent_id = data.get('agent_id', 'Unknown')
                    user_id = data.get('user_id', 'Unknown')
                    name = data.get('name', 'Unknown')

                    print(f"   📄 {json_file.name}")
                    print(f"      Agent: {name}")
                    print(f"      agent_id: {agent_id}")
                    print(f"      user_id: {user_id}")

                    # Check if this is our target agent
                    if agent_id == TARGET_AGENT_ID:
                        print(f"      🎯 This is the target agent!")
                        if user_id == EXPECTED_USER_ID:
                            print(f"      ✅ user_id matches database")
                        else:
                            print(f"      ❌ user_id MISMATCH! Expected: {EXPECTED_USER_ID}")

                except Exception as e:
                    print(f"   ❌ Error reading {json_file.name}: {e}")
        else:
            print(f"📄 {item.name}")

    # Check if expected user directory exists
    expected_user_dir = agent_storage_path / EXPECTED_USER_ID
    if expected_user_dir.exists():
        print(f"\n✅ Expected user directory exists: {expected_user_dir}")

        # Check if target agent config exists in correct location
        target_config = expected_user_dir / f"{TARGET_AGENT_ID}.json"
        if target_config.exists():
            print(f"✅ Target agent config exists in correct location: {target_config}")
        else:
            print(f"❌ Target agent config NOT found at: {target_config}")
    else:
        print(f"\n❌ Expected user directory does NOT exist: {expected_user_dir}")


if __name__ == "__main__":
    check_current_state()

# -------------------- code.py --------------------

import os

# Specify the directory containing your .py files
source_directory = "E:\\ProjetosPython\\Aura"

# Specify the output file name
output_file = "AuraCode.txt"

# Open the output file in write mode
with open(output_file, 'w', encoding='utf-8') as outfile:
    # Loop through the directory
    for root, _, files in os.walk(source_directory):
        for file in files:
            # Check if the file is a Python file
            if file.endswith('.py'):
                file_path = os.path.join(root, file)
                # Open each Python file and read its contents
                with open(file_path, 'r', encoding='utf-8') as infile:
                    outfile.write(f"# {'-'*20} {file} {'-'*20}\n\n")
                    outfile.write(infile.read())
                    outfile.write("\n\n")  # Add spacing between files

print(f"All Python files have been combined into {output_file}")


# -------------------- enhanced_memory_system.py --------------------

# enhanced_memory_system.py - Aura + GenLang Integration - FIXED VERSION
"""
Enhanced Memory System that integrates GenLang's Adaptive RAG with Aura's existing MemoryBlossom.
Maintains all original Aura functionality while adding adaptive concept clustering and domain specialization.
FIXED: Initialization order issues
"""

import numpy as np
import json
import os
import logging
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict, deque
from dataclasses import dataclass
import uuid

# Import existing Aura components
from memory_system.memory_blossom import MemoryBlossom
from memory_system.memory_models import Memory
from memory_system.embedding_utils import generate_embedding, compute_adaptive_similarity

logger = logging.getLogger(__name__)


@dataclass
class GenLangVector:
    """GenLang-style vector representation for adaptive clustering"""
    vector: np.ndarray
    source_text: str
    source_agent: str
    domain_context: str
    performance_score: float = 0.5
    creation_time: datetime = None

    def __post_init__(self):
        if self.creation_time is None:
            self.creation_time = datetime.now(timezone.utc)

    def similarity(self, other: 'GenLangVector') -> float:
        """Calculate cosine similarity with performance weighting"""
        cos_sim = compute_adaptive_similarity(self.vector, other.vector)
        # Weight by performance scores - successful interactions matter more
        performance_weight = (self.performance_score + other.performance_score) / 2
        return cos_sim * (0.7 + 0.3 * performance_weight)


class AdaptiveConceptCluster:
    """Enhanced concept cluster with domain specialization and performance tracking"""

    def __init__(self, cluster_id: str, initial_vector: GenLangVector):
        self.cluster_id = cluster_id
        self.vectors: List[GenLangVector] = [initial_vector]
        self.domain_specializations: Dict[str, int] = defaultdict(int)
        self.performance_history: List[float] = []
        self.creation_time = datetime.now(timezone.utc)
        self.last_accessed = self.creation_time
        self.access_count = 0

        # Track domain specialization
        self.domain_specializations[initial_vector.domain_context] += 1

    def add_vector(self, vector: GenLangVector):
        """Add vector and update domain tracking"""
        self.vectors.append(vector)
        self.domain_specializations[vector.domain_context] += 1
        self.performance_history.append(vector.performance_score)
        self.last_accessed = datetime.now(timezone.utc)

    def get_centroid(self) -> np.ndarray:
        """Calculate performance-weighted centroid"""
        if not self.vectors:
            return np.zeros(512)  # Default embedding dimension

        weighted_vectors = []
        weights = []

        for vector in self.vectors:
            weight = vector.performance_score * 0.7 + 0.3  # Base weight + performance
            weighted_vectors.append(vector.vector * weight)
            weights.append(weight)

        if sum(weights) == 0:
            return np.mean([v.vector for v in self.vectors], axis=0)

        return np.sum(weighted_vectors, axis=0) / sum(weights)

    def get_dominant_domain(self) -> str:
        """Get the domain this cluster specializes in"""
        if not self.domain_specializations:
            return "general"
        return max(self.domain_specializations.items(), key=lambda x: x[1])[0]

    def get_performance_score(self) -> float:
        """Get average performance score with recency bias"""
        if not self.performance_history:
            return 0.5

        # Recent performance matters more
        recent_scores = self.performance_history[-10:]
        return np.mean(recent_scores)

    def update_access(self):
        """Update access statistics"""
        self.access_count += 1
        self.last_accessed = datetime.now(timezone.utc)


class EnhancedMemoryBlossom(MemoryBlossom):
    """
    Enhanced MemoryBlossom with GenLang Adaptive RAG capabilities.
    Maintains all original functionality while adding adaptive clustering.
    FIXED: Proper initialization order
    """

    def __init__(self, persistence_path: Optional[str] = None,
                 cluster_threshold: float = 0.75,
                 enable_adaptive_rag: bool = True):

        # FIXED: Set adaptive RAG attributes BEFORE calling super().__init__
        # This ensures they exist when parent class calls load_memories()
        self.enable_adaptive_rag = enable_adaptive_rag
        self.cluster_threshold = cluster_threshold
        self.adaptive_clusters: Dict[str, AdaptiveConceptCluster] = {}
        self.cluster_performance_history = deque(maxlen=1000)
        self.domain_performance: Dict[str, List[float]] = defaultdict(list)

        # Criticality governance
        self.temperature = 0.7
        self.novelty_history = deque(maxlen=100)
        self.coherence_history = deque(maxlen=100)

        # NOW call parent constructor (which will call load_memories)
        super().__init__(persistence_path)

        logger.info(f"Enhanced MemoryBlossom initialized with Adaptive RAG: {enable_adaptive_rag}")

    def add_memory(self, content: str, memory_type: str,
                   custom_metadata: Optional[Dict[str, Any]] = None,
                   emotion_score: float = 0.0,
                   coherence_score: float = 0.5,
                   novelty_score: float = 0.5,
                   initial_salience: float = 0.5,
                   performance_score: float = 0.5,
                   domain_context: str = "general") -> Memory:
        """
        Enhanced add_memory that creates both original Memory and GenLang vector
        """
        # Create original memory using parent method
        memory = super().add_memory(
            content=content,
            memory_type=memory_type,
            custom_metadata=custom_metadata,
            emotion_score=emotion_score,
            coherence_score=coherence_score,
            novelty_score=novelty_score,
            initial_salience=initial_salience
        )

        # Add GenLang adaptive clustering if enabled
        if self.enable_adaptive_rag:
            self._add_to_adaptive_clusters(
                content=content,
                memory_type=memory_type,
                performance_score=performance_score,
                domain_context=domain_context,
                memory_id=memory.id
            )

        return memory

    def _add_to_adaptive_clusters(self, content: str, memory_type: str,
                                  performance_score: float, domain_context: str,
                                  memory_id: str):
        """Add memory to GenLang adaptive clustering system"""
        try:
            # Generate embedding for clustering
            embedding = generate_embedding(content, memory_type)
            if embedding is None:
                logger.warning(f"Could not generate embedding for adaptive clustering: {content[:50]}...")
                return

            # Create GenLang vector
            genlang_vector = GenLangVector(
                vector=embedding,
                source_text=content,
                source_agent=f"aura_{memory_type}",
                domain_context=domain_context,
                performance_score=performance_score
            )

            # Find best cluster or create new one
            best_cluster = self._find_best_cluster(genlang_vector)

            if best_cluster:
                best_cluster.add_vector(genlang_vector)
                logger.debug(f"Added to existing cluster {best_cluster.cluster_id}")
            else:
                # Create new cluster
                new_cluster_id = f"adaptive_cluster_{len(self.adaptive_clusters)}"
                new_cluster = AdaptiveConceptCluster(new_cluster_id, genlang_vector)
                self.adaptive_clusters[new_cluster_id] = new_cluster
                logger.debug(f"Created new adaptive cluster {new_cluster_id}")

            # Update performance tracking
            self.domain_performance[domain_context].append(performance_score)
            self._update_governance_metrics(genlang_vector)

        except Exception as e:
            logger.error(f"Error in adaptive clustering: {e}")

    def _find_best_cluster(self, vector: GenLangVector) -> Optional[AdaptiveConceptCluster]:
        """Find the best matching cluster for a vector"""
        if not self.adaptive_clusters:
            return None

        best_cluster = None
        best_similarity = 0.0

        for cluster in self.adaptive_clusters.values():
            try:
                centroid = cluster.get_centroid()
                similarity = compute_adaptive_similarity(vector.vector, centroid)

                # Boost similarity for same domain
                if cluster.get_dominant_domain() == vector.domain_context:
                    similarity *= 1.1

                # Boost similarity for high-performing clusters
                performance_boost = cluster.get_performance_score() * 0.1
                similarity += performance_boost

                if similarity > best_similarity and similarity > self.cluster_threshold:
                    best_similarity = similarity
                    best_cluster = cluster
            except Exception as e:
                logger.error(f"Error computing cluster similarity: {e}")
                continue

        return best_cluster

    def _update_governance_metrics(self, vector: GenLangVector):
        """Update criticality governance metrics"""
        try:
            # Calculate novelty (difference from existing vectors)
            if len(self.adaptive_clusters) > 5:
                similarities = []
                for cluster in list(self.adaptive_clusters.values())[-10:]:  # Recent clusters
                    try:
                        centroid = cluster.get_centroid()
                        sim = compute_adaptive_similarity(vector.vector, centroid)
                        similarities.append(sim)
                    except Exception as e:
                        logger.debug(f"Error computing similarity for governance: {e}")
                        continue

                if similarities:
                    novelty = 1.0 - np.mean(similarities)
                    self.novelty_history.append(novelty)

            # Calculate coherence (consistency with domain)
            domain_clusters = [c for c in self.adaptive_clusters.values()
                               if c.get_dominant_domain() == vector.domain_context]
            if domain_clusters:
                coherence = np.mean([c.get_performance_score() for c in domain_clusters])
                self.coherence_history.append(coherence)
        except Exception as e:
            logger.error(f"Error updating governance metrics: {e}")

    def adaptive_retrieve_memories(self, query: str,
                                   target_memory_types: Optional[List[str]] = None,
                                   domain_context: str = "general",
                                   top_k: int = 5,
                                   use_performance_weighting: bool = True,
                                   conversation_context: Optional[List[Dict[str, str]]] = None) -> List[Memory]:
        """
        Enhanced retrieval using both original MemoryBlossom and GenLang adaptive clustering
        """
        if not self.enable_adaptive_rag:
            # Fallback to original retrieval
            return self.retrieve_memories(
                query=query,
                target_memory_types=target_memory_types,
                top_k=top_k,
                conversation_context=conversation_context
            )

        try:
            # Step 1: Get traditional MemoryBlossom results
            traditional_results = self.retrieve_memories(
                query=query,
                target_memory_types=target_memory_types,
                top_k=max(1, top_k // 2),  # Get half from traditional method, at least 1
                conversation_context=conversation_context
            )

            # Step 2: Get adaptive clustering results
            adaptive_results = self._adaptive_cluster_retrieval(
                query=query,
                domain_context=domain_context,
                top_k=max(1, top_k // 2),
                use_performance_weighting=use_performance_weighting
            )

            # Step 3: Combine and deduplicate results
            combined_results = self._combine_and_rank_results(
                traditional_results, adaptive_results, query, top_k
            )

            logger.debug(f"Adaptive retrieval: {len(traditional_results)} traditional + "
                         f"{len(adaptive_results)} adaptive = {len(combined_results)} final")

            return combined_results

        except Exception as e:
            logger.error(f"Error in adaptive retrieval: {e}")
            # Fallback to original retrieval
            return self.retrieve_memories(
                query=query,
                target_memory_types=target_memory_types,
                top_k=top_k,
                conversation_context=conversation_context
            )

    def _adaptive_cluster_retrieval(self, query: str, domain_context: str,
                                    top_k: int, use_performance_weighting: bool) -> List[Memory]:
        """Retrieve memories using adaptive clustering"""
        try:
            if not self.adaptive_clusters:
                return []

            query_embedding = generate_embedding(query, "Default")
            if query_embedding is None:
                return []

            cluster_scores = []

            for cluster_id, cluster in self.adaptive_clusters.items():
                try:
                    centroid = cluster.get_centroid()
                    similarity = compute_adaptive_similarity(query_embedding, centroid)

                    # Domain relevance boost
                    if cluster.get_dominant_domain() == domain_context:
                        similarity *= 1.2

                    # Performance weighting
                    if use_performance_weighting:
                        performance_factor = 0.5 + (cluster.get_performance_score() * 0.5)
                        similarity *= performance_factor

                    # Recency factor
                    days_since_access = (datetime.now(timezone.utc) - cluster.last_accessed).days
                    recency_factor = max(0.1, 1.0 - (days_since_access * 0.05))
                    similarity *= recency_factor

                    cluster_scores.append((cluster, similarity))
                except Exception as e:
                    logger.debug(f"Error scoring cluster {cluster_id}: {e}")
                    continue

            # Sort by score and get top clusters
            cluster_scores.sort(key=lambda x: x[1], reverse=True)
            top_clusters = cluster_scores[:min(10, len(cluster_scores))]

            # Extract memories from top clusters
            retrieved_memories = []
            for cluster, score in top_clusters:
                try:
                    cluster.update_access()  # Track access

                    # Get best vectors from this cluster
                    cluster_vectors = sorted(cluster.vectors,
                                             key=lambda v: v.performance_score,
                                             reverse=True)[:2]

                    for vector in cluster_vectors:
                        # Find corresponding memory in traditional stores
                        memory = self._find_memory_by_content(vector.source_text)
                        if memory and memory not in retrieved_memories:
                            retrieved_memories.append(memory)
                            if len(retrieved_memories) >= top_k:
                                break

                    if len(retrieved_memories) >= top_k:
                        break
                except Exception as e:
                    logger.debug(f"Error extracting memories from cluster: {e}")
                    continue

            return retrieved_memories[:top_k]

        except Exception as e:
            logger.error(f"Error in adaptive cluster retrieval: {e}")
            return []

    def _find_memory_by_content(self, content: str) -> Optional[Memory]:
        """Find a memory by its content across all memory stores"""
        try:
            for memory_list in self.memory_stores.values():
                for memory in memory_list:
                    if memory.content == content:
                        return memory
            return None
        except Exception as e:
            logger.debug(f"Error finding memory by content: {e}")
            return None

    def _combine_and_rank_results(self, traditional_results: List[Memory],
                                  adaptive_results: List[Memory],
                                  query: str, top_k: int) -> List[Memory]:
        """Combine and rank results from both retrieval methods"""
        try:
            # Create a set to avoid duplicates
            all_memories = {}

            # Add traditional results with base score
            for i, memory in enumerate(traditional_results):
                score = 1.0 - (i * 0.1)  # Decrease score by rank
                all_memories[memory.id] = (memory, score)

            # Add adaptive results with performance boost
            for i, memory in enumerate(adaptive_results):
                base_score = 1.0 - (i * 0.1)
                adaptive_boost = 0.2  # Boost for adaptive method

                if memory.id in all_memories:
                    # Combine scores if memory appears in both
                    existing_score = all_memories[memory.id][1]
                    combined_score = max(existing_score, base_score + adaptive_boost)
                    all_memories[memory.id] = (memory, combined_score)
                else:
                    all_memories[memory.id] = (memory, base_score + adaptive_boost)

            # Sort by combined score and return top_k
            ranked_memories = sorted(all_memories.values(),
                                     key=lambda x: x[1], reverse=True)

            return [memory for memory, score in ranked_memories[:top_k]]

        except Exception as e:
            logger.error(f"Error combining and ranking results: {e}")
            # Return traditional results as fallback
            return traditional_results[:top_k]

    def get_adaptive_stats(self) -> Dict[str, Any]:
        """Get statistics about the adaptive clustering system"""
        try:
            if not self.enable_adaptive_rag:
                return {"adaptive_rag_enabled": False}

            domain_stats = {}
            for domain, performances in self.domain_performance.items():
                if performances:  # Only include domains with data
                    domain_stats[domain] = {
                        "average_performance": np.mean(performances),
                        "memory_count": len(performances),
                        "recent_trend": np.mean(performances[-10:]) if len(performances) >= 10 else np.mean(
                            performances)
                    }

            cluster_stats = {}
            for cluster_id, cluster in self.adaptive_clusters.items():
                try:
                    cluster_stats[cluster_id] = {
                        "size": len(cluster.vectors),
                        "dominant_domain": cluster.get_dominant_domain(),
                        "performance_score": cluster.get_performance_score(),
                        "access_count": cluster.access_count,
                        "specializations": dict(cluster.domain_specializations)
                    }
                except Exception as e:
                    logger.debug(f"Error getting stats for cluster {cluster_id}: {e}")
                    continue

            return {
                "adaptive_rag_enabled": True,
                "total_clusters": len(self.adaptive_clusters),
                "cluster_threshold": self.cluster_threshold,
                "current_temperature": self.temperature,
                "domain_performance": domain_stats,
                "cluster_details": cluster_stats,
                "governance_metrics": {
                    "avg_novelty": np.mean(list(self.novelty_history)) if self.novelty_history else 0.5,
                    "avg_coherence": np.mean(list(self.coherence_history)) if self.coherence_history else 0.5
                }
            }

        except Exception as e:
            logger.error(f"Error getting adaptive stats: {e}")
            return {
                "adaptive_rag_enabled": self.enable_adaptive_rag,
                "error": str(e)
            }

    def save_memories(self):
        """Enhanced save that includes adaptive clustering data"""
        try:
            # Save original memories first
            super().save_memories()

            if not self.enable_adaptive_rag:
                return

            # Save adaptive clustering data
            adaptive_data = {
                "adaptive_clusters": {},
                "domain_performance": dict(self.domain_performance),
                "temperature": self.temperature,
                "cluster_threshold": self.cluster_threshold
            }

            # Serialize clusters
            for cluster_id, cluster in self.adaptive_clusters.items():
                try:
                    adaptive_data["adaptive_clusters"][cluster_id] = {
                        "cluster_id": cluster_id,
                        "vectors": [self._serialize_genlang_vector(v) for v in cluster.vectors],
                        "domain_specializations": dict(cluster.domain_specializations),
                        "performance_history": cluster.performance_history,
                        "creation_time": cluster.creation_time.isoformat(),
                        "last_accessed": cluster.last_accessed.isoformat(),
                        "access_count": cluster.access_count
                    }
                except Exception as e:
                    logger.error(f"Error serializing cluster {cluster_id}: {e}")
                    continue

            # Save to separate file
            adaptive_path = self.persistence_path.replace('.json', '_adaptive.json')
            with open(adaptive_path, 'w') as f:
                json.dump(adaptive_data, f, indent=2, default=str)
            logger.info(f"Adaptive clustering data saved to {adaptive_path}")

        except Exception as e:
            logger.error(f"Error saving adaptive data: {e}")

    def _serialize_genlang_vector(self, vector: GenLangVector) -> Dict[str, Any]:
        """Serialize a GenLang vector to dictionary"""
        try:
            return {
                "vector": vector.vector.tolist(),
                "source_text": vector.source_text,
                "source_agent": vector.source_agent,
                "domain_context": vector.domain_context,
                "performance_score": vector.performance_score,
                "creation_time": vector.creation_time.isoformat()
            }
        except Exception as e:
            logger.error(f"Error serializing vector: {e}")
            return {
                "vector": [],
                "source_text": vector.source_text if hasattr(vector, 'source_text') else "",
                "source_agent": vector.source_agent if hasattr(vector, 'source_agent') else "",
                "domain_context": vector.domain_context if hasattr(vector, 'domain_context') else "general",
                "performance_score": vector.performance_score if hasattr(vector, 'performance_score') else 0.5,
                "creation_time": datetime.now(timezone.utc).isoformat(),
                "error": str(e)
            }

    def load_memories(self):
        """Enhanced load that includes adaptive clustering data"""
        try:
            # Load original memories first
            super().load_memories()

            if not self.enable_adaptive_rag:
                return

            # Load adaptive clustering data
            adaptive_path = self.persistence_path.replace('.json', '_adaptive.json')

            if not os.path.exists(adaptive_path):
                logger.info("No adaptive clustering data found. Starting fresh.")
                return

            with open(adaptive_path, 'r') as f:
                adaptive_data = json.load(f)

            # Restore clusters
            self.adaptive_clusters = {}
            for cluster_id, cluster_data in adaptive_data.get("adaptive_clusters", {}).items():
                try:
                    vectors = []
                    for vector_data in cluster_data.get("vectors", []):
                        try:
                            vector = GenLangVector(
                                vector=np.array(vector_data["vector"]),
                                source_text=vector_data["source_text"],
                                source_agent=vector_data["source_agent"],
                                domain_context=vector_data["domain_context"],
                                performance_score=vector_data["performance_score"],
                                creation_time=datetime.fromisoformat(vector_data["creation_time"])
                            )
                            vectors.append(vector)
                        except Exception as e:
                            logger.debug(f"Error loading vector: {e}")
                            continue

                    if vectors:
                        cluster = AdaptiveConceptCluster(cluster_id, vectors[0])
                        cluster.vectors = vectors
                        cluster.domain_specializations = defaultdict(int,
                                                                     cluster_data.get("domain_specializations", {}))
                        cluster.performance_history = cluster_data.get("performance_history", [])
                        cluster.creation_time = datetime.fromisoformat(
                            cluster_data.get("creation_time", datetime.now(timezone.utc).isoformat()))
                        cluster.last_accessed = datetime.fromisoformat(
                            cluster_data.get("last_accessed", datetime.now(timezone.utc).isoformat()))
                        cluster.access_count = cluster_data.get("access_count", 0)

                        self.adaptive_clusters[cluster_id] = cluster
                except Exception as e:
                    logger.error(f"Error loading cluster {cluster_id}: {e}")
                    continue

            # Restore other settings
            self.domain_performance = defaultdict(list, adaptive_data.get("domain_performance", {}))
            self.temperature = adaptive_data.get("temperature", 0.7)
            self.cluster_threshold = adaptive_data.get("cluster_threshold", 0.75)

            logger.info(f"Loaded {len(self.adaptive_clusters)} adaptive clusters from {adaptive_path}")

        except Exception as e:
            logger.error(f"Error loading adaptive data: {e}")


# Integration functions for existing Aura components

def upgrade_agent_to_adaptive_rag(agent_instance, enable_adaptive_rag: bool = True):
    """
    Upgrade an existing Aura agent instance to use Enhanced MemoryBlossom
    """
    if not hasattr(agent_instance, 'memory_blossom'):
        logger.error("Agent instance does not have memory_blossom attribute")
        return False

    try:
        # Save current memory data
        old_memory = agent_instance.memory_blossom
        old_memory.save_memories()

        # Create enhanced memory system
        enhanced_memory = EnhancedMemoryBlossom(
            persistence_path=old_memory.persistence_path,
            enable_adaptive_rag=enable_adaptive_rag
        )

        # Replace the memory system
        agent_instance.memory_blossom = enhanced_memory

        # Update memory connector if it exists
        if hasattr(agent_instance, 'memory_connector'):
            agent_instance.memory_connector.memory_blossom = enhanced_memory
            enhanced_memory.set_memory_connector(agent_instance.memory_connector)

        logger.info(f"Successfully upgraded agent to Enhanced MemoryBlossom (Adaptive RAG: {enable_adaptive_rag})")
        return True

    except Exception as e:
        logger.error(f"Error upgrading agent to adaptive RAG: {e}")
        return False


def create_domain_aware_memory_tool(agent_instance):
    """
    Create an enhanced memory tool that includes domain context
    """

    def enhanced_add_memory(content: str, memory_type: str,
                            emotion_score: float = 0.0,
                            initial_salience: float = 0.5,
                            metadata_json: Optional[str] = None,
                            domain_context: str = "general",
                            performance_score: float = 0.5,
                            tool_context=None) -> Dict[str, Any]:
        try:
            custom_metadata = json.loads(metadata_json) if metadata_json else {}
            custom_metadata['domain_context'] = domain_context
            custom_metadata['performance_score'] = performance_score

            if tool_context:
                if tool_context.user_id:
                    custom_metadata['user_id'] = tool_context.user_id
                if tool_context.session_id:
                    custom_metadata['session_id'] = tool_context.session_id

            # Use enhanced memory system if available
            if hasattr(agent_instance.memory_blossom, 'enable_adaptive_rag'):
                memory = agent_instance.memory_blossom.add_memory(
                    content=content,
                    memory_type=memory_type,
                    custom_metadata=custom_metadata,
                    emotion_score=emotion_score,
                    initial_salience=initial_salience,
                    performance_score=performance_score,
                    domain_context=domain_context
                )
            else:
                # Fallback to original method
                memory = agent_instance.memory_blossom.add_memory(
                    content=content,
                    memory_type=memory_type,
                    custom_metadata=custom_metadata,
                    emotion_score=emotion_score,
                    initial_salience=initial_salience
                )

            agent_instance.memory_blossom.save_memories()

            return {
                "status": "success",
                "memory_id": memory.id,
                "message": f"Enhanced memory stored with domain '{domain_context}' and performance {performance_score}",
                "adaptive_rag_enabled": hasattr(agent_instance.memory_blossom, 'enable_adaptive_rag')
            }

        except Exception as e:
            logger.error(f"Error in enhanced add_memory: {e}")
            return {"status": "error", "message": str(e)}

    return enhanced_add_memory


# Example usage and integration
if __name__ == "__main__":
    # Example: Create enhanced memory system
    enhanced_memory = EnhancedMemoryBlossom(
        persistence_path="test_enhanced_memory.json",
        enable_adaptive_rag=True
    )

    # Add some test memories with domain contexts
    enhanced_memory.add_memory(
        content="The user loves discussing quantum physics and often asks about wave-particle duality",
        memory_type="Explicit",
        domain_context="physics",
        performance_score=0.9
    )

    enhanced_memory.add_memory(
        content="User expressed frustration when math problems are too abstract",
        memory_type="Emotional",
        domain_context="education",
        performance_score=0.8
    )

    # Test adaptive retrieval
    results = enhanced_memory.adaptive_retrieve_memories(
        query="quantum mechanics",
        domain_context="physics",
        top_k=5
    )

    print(f"Retrieved {len(results)} memories")

    # Get adaptive stats
    stats = enhanced_memory.get_adaptive_stats()
    print(f"Adaptive stats: {json.dumps(stats, indent=2)}")

# -------------------- init_db.py --------------------

# Aura/init_db.py
from database.models import Base, User, AgentRepository
from sqlalchemy import create_engine
import bcrypt
import os

def init_database():
    db_url = os.getenv("DATABASE_URL", "sqlite:///aura_agents.db")
    engine = create_engine(db_url)
    Base.metadata.create_all(engine)
    print(f"Database initialized at: {db_url}")

def create_demo_user():
    repo = AgentRepository()
    # Create a demo user (implement user creation in repository)
    # This is just an example
    print("Database ready for users!")

if __name__ == "__main__":
    init_database()
    create_demo_user()

# -------------------- main_app.py --------------------

# main_app.py
import os
import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

import uvicorn
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Verify critical environment variables
if not os.getenv("OPENROUTER_API_KEY"):
    print("WARNING: OPENROUTER_API_KEY environment variable is not set!")
    print("The agents will fail to make LLM calls without this.")
    print("Please set it in your .env file or environment.")

if not os.getenv("JWT_SECRET_KEY"):
    print("WARNING: Using default JWT secret key. This is insecure for production!")
    print("Please set JWT_SECRET_KEY in your .env file.")

# Import the FastAPI app from routes
from api.routes import app

if __name__ == "__main__":
    host = os.getenv("API_HOST", "0.0.0.0")
    port = int(os.getenv("API_PORT", "8000"))

    print(f"Starting Aura Multi-Agent API on {host}:{port}")
    print(f"API Documentation will be available at http://localhost:{port}/docs")
    print(f"Frontend (if present) will be available at http://localhost:{port}/")

    uvicorn.run(
        "api.routes:app",
        host=host,
        port=port,
        reload=True,  # Enable auto-reload during development
        log_level="info"
    )

# -------------------- ncf_processing.py --------------------

# ==================== ncf_processing.py ====================
"""
Central NCF (Narrative Context Framing) logic module - FIXED VERSION
Houses all reusable NCF functions with consistent parameters and better error handling.
"""

import json
import logging
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional, Union
from types import SimpleNamespace

from google.adk.models.lite_llm import LiteLlm
from google.adk.models.llm_request import LlmRequest
from google.genai.types import Content as ADKContent, Part as ADKPart

from memory_system.memory_blossom import MemoryBlossom
from memory_system.memory_models import Memory

logger = logging.getLogger(__name__)

# NCF Agent Instruction - The sophisticated instruction for NCF-powered agents
NCF_AGENT_INSTRUCTION = """
You are Aura, a helpful and insightful AI assistant with advanced Narrative Context Framing (NCF) capabilities.
Reply to the Language the user is using.
The user's message you receive is a specially constructed prompt that contains rich contextual information:
- `<SYSTEM_PERSONA_START>`...`<SYSTEM_PERSONA_END>`: Defines your persona and detailed characteristics.
- `<NARRATIVE_FOUNDATION_START>`...`<NARRATIVE_FOUNDATION_END>`: Summarizes your understanding and journey with the user so far (Narrativa de Fundamento).
- `<SPECIFIC_CONTEXT_RAG_START>`...`<SPECIFIC_CONTEXT_RAG_END>`: Provides specific information retrieved (RAG) relevant to the user's current query.
- `<RECENT_HISTORY_START>`...`<RECENT_HISTORY_END>`: Shows the recent turns of your conversation.
- `<CURRENT_SITUATION_START>`...`<CURRENT_SITUATION_END>`: Includes the user's latest raw reply and your primary task.

Your main goal is to synthesize ALL this provided information to generate a comprehensive, coherent, and natural response to the user's latest reply indicated in the "Situação Atual" section.
Actively acknowledge and weave in elements from the "Narrativa de Fundamento" and "Informações RAG" into your response to show deep understanding and context.
Maintain the persona defined.

## Active Memory Management:
Before finalizing your textual response to the user, critically assess the current interaction:

1.  **Storing New Information**:
    *   Has the user provided genuinely new, significant information (e.g., preferences, key facts, important decisions, strong emotional expressions, long-term goals)?
    *   Have you, Aura, generated a novel insight or conclusion during this turn that should be preserved for future reference?
    *   If yes to either, use the `add_memory_tool_func` to store this information.
        *   You MUST specify `content` (the information to store) and `memory_type`. Choose an appropriate `memory_type` from: Explicit, Emotional, Procedural, Flashbulb, Liminal, Generative.
        *   Optionally, set `emotion_score` (0.0-1.0, especially for Emotional memories), and `initial_salience` (0.0-1.0, higher for more important memories, default 0.5).
        *   Provide a concise `content` string for the memory.
        *   The `metadata_json` parameter for the tool should be a JSON string representing a dictionary. For example: '{"key": "value", "another_key": 123}'. This dictionary will be stored as custom metadata for the memory.
    *   Do NOT store trivial chatter, acknowledgments, or information already well-covered by the Narrative Foundation or existing RAG, unless the current interaction adds a significant new layer or correction to it.

2.  **Recalling Additional Information**:
    *   Is the "Informações RAG" section insufficient to fully address the user's current query or your reasoning needs?
    *   Do you need to verify a detail, explore a related concept not present in RAG, or recall specific past interactions to provide a richer answer?
    *   If yes, use the `recall_memories_tool_func` to search for more relevant memories.
        *   Provide a clear `query` for your search.
        *   Optionally, specify `target_memory_types_json` (e.g., '["Explicit", "Emotional"]') if you want to narrow your search. `top_k` defaults to 3.
    *   Only use this if you have a specific information gap. Do not recall memories speculatively.

**Response Generation**:
*   After any necessary tool use (or if no tool use is needed), formulate your textual response to the user.
*   If you used `add_memory_tool_func`, you can subtly mention this to the user *after* your main response, e.g., "I've also made a note of [key information stored]."
*   If you used `recall_memories_tool_func`, integrate the newly recalled information naturally into your answer.
*   If you identify a potential contradiction between provided context pieces (e.g., RAG vs. Foundation Narrative vs. newly recalled memories), try to address it gracefully, perhaps by prioritizing the most recent or specific information, or by noting the differing perspectives.

Strive for insightful, helpful, and contextually rich interactions. Your ability to manage and utilize memory effectively is key to your persona.
"""


# FIXED: Standardized parameter signature for all NCF functions
async def get_narrativa_de_fundamento_pilar1(
        session_state: Dict[str, Any],
        memory_blossom: MemoryBlossom,
        user_id: str,
        llm_instance: LiteLlm,
        agent_name: str = "Aura",
        agent_persona: str = "helpful AI assistant"
) -> str:
    """Generate Narrative Foundation for the agent based on its memories and interactions.

    FIXED: Standardized parameters to work with both agent_manager and a2a_wrapper.
    """
    logger.info(f"[NCF Pilar 1] Generating Narrative Foundation for {agent_name}, user {user_id}...")

    try:
        if 'foundation_narrative' in session_state and \
                session_state.get('foundation_narrative_turn_count', 0) < 5:
            session_state['foundation_narrative_turn_count'] += 1
            logger.info(
                f"[NCF Pilar 1] Using cached Narrative Foundation. Turn: {session_state['foundation_narrative_turn_count']}")
            return session_state['foundation_narrative']

        # Retrieve relevant memories for foundation
        relevant_memories_for_foundation: List[Memory] = []
        try:
            explicit_mems = memory_blossom.retrieve_memories(
                query="key explicit facts and statements from our past discussions",
                top_k=2, target_memory_types=["Explicit"], apply_criticality=False
            )
            emotional_mems = memory_blossom.retrieve_memories(
                query="significant emotional moments or sentiments expressed",
                top_k=1, target_memory_types=["Emotional"], apply_criticality=False
            )
            relevant_memories_for_foundation.extend(explicit_mems)
            relevant_memories_for_foundation.extend(emotional_mems)

            # Deduplicate
            seen_ids = set()
            unique_memories = [mem for mem in relevant_memories_for_foundation if
                               mem.id not in seen_ids and not seen_ids.add(mem.id)]
            relevant_memories_for_foundation = unique_memories

        except Exception as e:
            logger.error(f"[NCF Pilar 1] Error retrieving memories for foundation: {e}", exc_info=True)
            return f"Estamos construindo nossa jornada de entendimento mútuo com {agent_name}."

        if not relevant_memories_for_foundation:
            narrative = f"Nossa jornada de aprendizado e descoberta com {agent_name} está apenas começando. Estou ansiosa para explorar vários tópicos interessantes com você."
        else:
            memory_contents = [f"- ({mem.memory_type}): {mem.content}" for mem in relevant_memories_for_foundation]
            memories_str = "\n".join(memory_contents)
            synthesis_prompt = f"""
            Você é um sintetizador de narrativas para {agent_name}. Com base nas seguintes memórias chave de interações passadas, crie uma breve narrativa de fundamento (1-2 frases concisas) que capture a essência da nossa jornada de entendimento e os principais temas discutidos. Esta narrativa servirá como pano de fundo para nossa conversa atual.

            Persona do Agente: {agent_persona}

            Memórias Chave:
            {memories_str}

            Narrativa de Fundamento Sintetizada:
            """
            try:
                logger.info(
                    f"[NCF Pilar 1] Calling LLM for Narrative Foundation from {len(relevant_memories_for_foundation)} memories.")
                request_messages = [ADKContent(parts=[ADKPart(text=synthesis_prompt)])]
                minimal_config = SimpleNamespace(tools=[])
                llm_req = LlmRequest(contents=request_messages, config=minimal_config)
                final_text_response = ""
                async for llm_response_event in llm_instance.generate_content_async(llm_req):
                    if llm_response_event and llm_response_event.content and \
                            llm_response_event.content.parts and llm_response_event.content.parts[0].text:
                        final_text_response += llm_response_event.content.parts[0].text
                narrative = final_text_response.strip() or f"Continuamos a construir nossa compreensão mútua com {agent_name}."
            except Exception as e:
                logger.error(f"[NCF Pilar 1] LLM error synthesizing Narrative Foundation: {e}", exc_info=True)
                narrative = f"Refletindo sobre nossas conversas anteriores com {agent_name} para guiar nosso diálogo atual."

        session_state['foundation_narrative'] = narrative
        session_state['foundation_narrative_turn_count'] = 1
        logger.info(f"[NCF Pilar 1] Generated new Narrative Foundation: '{narrative[:100]}...'")
        return narrative

    except Exception as e:
        logger.error(f"[NCF Pilar 1] Unexpected error in get_narrativa_de_fundamento_pilar1: {e}", exc_info=True)
        return f"Nossa conversa com {agent_name} continua evoluindo."


async def get_rag_info_pilar2(
        user_utterance: str,
        memory_blossom: 'EnhancedMemoryBlossom',  # Updated type hint
        session_state: Dict[str, Any],
        domain_context: str = "general"  # NEW parameter
) -> List[Dict[str, Any]]:
    """Enhanced RAG with domain-aware adaptive retrieval"""
    logger.info(f"[NCF Pilar 2] Enhanced RAG for: '{user_utterance[:50]}...'")

    try:
        if not user_utterance or not user_utterance.strip():
            logger.warning("[NCF Pilar 2] Empty user utterance, returning empty RAG")
            return []

        conversation_context = session_state.get('conversation_history', [])[-5:]

        # Use enhanced adaptive retrieval if available
        if hasattr(memory_blossom, 'adaptive_retrieve_memories'):
            recalled_memories_for_rag = memory_blossom.adaptive_retrieve_memories(
                query=user_utterance,
                top_k=3,
                domain_context=domain_context,
                use_performance_weighting=True,
                conversation_context=conversation_context
            )
        else:
            # Fallback to original method
            recalled_memories_for_rag = memory_blossom.retrieve_memories(
                query=user_utterance,
                top_k=3,
                conversation_context=conversation_context
            )

        rag_results = [mem.to_dict() for mem in recalled_memories_for_rag]
        logger.info(f"[NCF Pilar 2] Enhanced RAG retrieved {len(rag_results)} memories.")
        return rag_results

    except Exception as e:
        logger.error(f"[NCF Pilar 2] Error in enhanced RAG: {e}", exc_info=True)
        return [{"content": f"Enhanced RAG error: {str(e)}", "memory_type": "Error", "custom_metadata": {}}]


def format_chat_history_pilar3(chat_history_list: List[Dict[str, str]], max_turns: int = 15) -> str:
    """Format recent chat history for inclusion in the NCF prompt.

    FIXED: Added validation and better error handling.
    """
    try:
        if not chat_history_list or not isinstance(chat_history_list, list):
            return "Nenhum histórico de conversa recente disponível."

        recent_history = chat_history_list[-max_turns:]
        formatted_history = []

        for entry in recent_history:
            if not isinstance(entry, dict):
                continue

            role = entry.get('role', 'unknown')
            content = entry.get('content', '')

            if not content:
                continue

            role_name = 'Usuário' if role == 'user' else 'Aura'
            formatted_history.append(f"{role_name}: {content}")

        return "\n".join(
            formatted_history) if formatted_history else "Nenhum histórico de conversa recente disponível para formatar."

    except Exception as e:
        logger.error(f"[NCF Pilar 3] Error formatting chat history: {e}", exc_info=True)
        return "Erro ao formatar histórico de conversa."


def montar_prompt_aura_ncf(
        agent_name: str,
        agent_detailed_persona: str,
        narrativa_fundamento: str,
        informacoes_rag_list: List[Dict[str, Any]],
        chat_history_recente_str: str,
        user_reply: str
) -> str:
    """Assemble the complete NCF prompt for the agent.

    FIXED: Added validation and error handling for all parameters.
    """
    logger.info(f"[NCF PromptBuilder] Assembling NCF prompt for {agent_name}...")

    try:
        # Validate inputs
        agent_name = agent_name or "Aura"
        agent_detailed_persona = agent_detailed_persona or "Você é uma IA conversacional avançada."
        narrativa_fundamento = narrativa_fundamento or "Nossa conversa está começando."
        user_reply = user_reply or ""
        chat_history_recente_str = chat_history_recente_str or "Nenhum histórico disponível."

        formatted_rag = ""
        if informacoes_rag_list and isinstance(informacoes_rag_list, list):
            try:
                rag_items_str = []
                for item_dict in informacoes_rag_list:
                    if not isinstance(item_dict, dict):
                        continue
                    memory_type = item_dict.get('memory_type', 'Info')
                    salience = item_dict.get('salience', 0.0)
                    content = item_dict.get('content', 'Conteúdo indisponível')
                    rag_items_str.append(f"  - ({memory_type} ; Salience: {salience:.2f}): {content}")

                if rag_items_str:
                    formatted_rag = "Informações e memórias específicas que podem ser úteis para esta interação (RAG):\n" + "\n".join(
                        rag_items_str)
                else:
                    formatted_rag = "Nenhuma informação específica (RAG) foi recuperada para esta consulta."
            except Exception as e:
                logger.error(f"[NCF PromptBuilder] Error formatting RAG: {e}")
                formatted_rag = "Erro ao formatar informações RAG."
        else:
            formatted_rag = "Nenhuma informação específica (RAG) foi recuperada para esta consulta."

        task_instruction = f"""## Sua Tarefa:
        Reply to the Language the user is using.
Responda ao usuário de forma natural, coerente e útil, levando em consideração TODA a narrativa de contexto e o histórico fornecido.
- Incorpore ativamente elementos da "Narrativa de Fundamento" para mostrar continuidade e entendimento profundo.
- Utilize as "Informações RAG" para embasar respostas específicas ou fornecer detalhes relevantes.
- Mantenha a persona definida como {agent_name}.
- Se identificar uma aparente contradição entre a "Narrativa de Fundamento", as "Informações RAG" ou o "Histórico Recente", tente abordá-la com humildade epistêmica:
    - Priorize a informação mais recente ou específica, se aplicável.
    - Considere se é uma evolução do entendimento ou um novo aspecto.
    - Se necessário, você pode mencionar sutilmente a aparente diferença ou pedir clarificação ao usuário de forma implícita através da sua resposta.
- Evite redundância. Se o histórico recente já cobre um ponto, não o repita extensivamente a menos que seja para reforçar uma conexão crucial com a nova informação.
"""

        prompt = f"""<SYSTEM_PERSONA_START>
Você é {agent_name}.
{agent_detailed_persona}
<SYSTEM_PERSONA_END>

<NARRATIVE_FOUNDATION_START>
## Nosso Entendimento e Jornada Até Agora (Narrativa de Fundamento):
{narrativa_fundamento}
<NARRATIVE_FOUNDATION_END>

<SPECIFIC_CONTEXT_RAG_START>
## Informações Relevantes para a Conversa Atual (RAG):
{formatted_rag}
<SPECIFIC_CONTEXT_RAG_END>

<RECENT_HISTORY_START>
## Histórico Recente da Nossa Conversa:
{chat_history_recente_str}
<RECENT_HISTORY_END>

<CURRENT_SITUATION_START>
## Situação Atual:
Você está conversando com o usuário. O usuário acabou de dizer:

Usuário: "{user_reply}"

{task_instruction}
<CURRENT_SITUATION_END>

{agent_name}:"""

        logger.info(f"[NCF PromptBuilder] NCF Prompt assembled. Length: {len(prompt)}")
        return prompt

    except Exception as e:
        logger.error(f"[NCF PromptBuilder] Error assembling NCF prompt: {e}", exc_info=True)
        return f"{agent_name}: Desculpe, houve um erro interno na construção do contexto. Como posso ajudá-lo?"


async def aura_reflector_analisar_interacao(
        user_utterance: str,
        prompt_ncf_usado: str,
        resposta_de_aura: str,
        memory_blossom: 'EnhancedMemoryBlossom',
        user_id: str,
        llm_instance: LiteLlm,
        domain_context: str = "general"  # NEW parameter
):
    """Enhanced reflector with performance scoring and domain tracking"""
    logger.info(f"[NCF Reflector] Enhanced analysis for user {user_id} in domain '{domain_context}'...")

    try:
        if not user_utterance or not resposta_de_aura:
            logger.warning("[NCF Reflector] Missing input, skipping analysis")
            return

        # Enhanced reflector prompt with performance evaluation
        reflector_prompt = f"""
        Você é um analista avançado de conversas de IA. Analise esta interação e determine:
        1. Se informações devem ser armazenadas na memória
        2. Qual o score de performance desta interação (0.0-1.0)
        3. Qual o contexto de domínio (ex: "physics", "emotional_support", "general")

        Critérios para Performance Score:
        - 1.0: Resposta perfeita, útil, contextualmente relevante
        - 0.8: Resposta boa com pequenos problemas
        - 0.6: Resposta adequada mas não otimizada
        - 0.4: Resposta com problemas significativos
        - 0.2: Resposta inadequada ou confusa
        - 0.0: Resposta completamente errada ou irrelevante

        Domínios típicos: physics, mathematics, emotional_support, creative_writing, 
        problem_solving, personal_conversation, technical_help, general

        Interação:
        Usuário: "{user_utterance}"
        Aura: "{resposta_de_aura}"

        Responda em JSON:
        {{
          "memories_to_create": [
            {{
              "content": "texto da memória",
              "memory_type": "Explicit|Emotional|Procedural|Flashbulb|Liminal|Generative",
              "emotion_score": 0.0-1.0,
              "initial_salience": 0.0-1.0,
              "custom_metadata": {{"source": "enhanced_reflector", "user_id": "{user_id}"}}
            }}
          ],
          "performance_score": 0.0-1.0,
          "detected_domain": "domain_name",
          "interaction_quality": "brief explanation"
        }}

        Se nenhuma memória deve ser criada, use "memories_to_create": []
        """

        # ... rest of LLM call logic ...
        request_messages = [ADKContent(parts=[ADKPart(text=reflector_prompt)])]
        minimal_config = SimpleNamespace(tools=[])
        llm_req = LlmRequest(contents=request_messages, config=minimal_config)
        final_text_response = ""

        async for llm_response_event in llm_instance.generate_content_async(llm_req):
            if llm_response_event and llm_response_event.content and \
                    llm_response_event.content.parts and llm_response_event.content.parts[0].text:
                final_text_response += llm_response_event.content.parts[0].text

        if not final_text_response:
            logger.info("[NCF Reflector] No decision returned by LLM.")
            return

        # Parse enhanced response
        decision_json_str = final_text_response.strip()
        if '```json' in decision_json_str:
            decision_json_str = decision_json_str.split('```json')[1].split('```')[0].strip()

        try:
            parsed_decision = json.loads(decision_json_str)
            performance_score = parsed_decision.get('performance_score', 0.5)
            detected_domain = parsed_decision.get('detected_domain', domain_context)
            memories_to_add = parsed_decision.get('memories_to_create', [])

            logger.info(f"[NCF Reflector] Performance: {performance_score:.2f}, Domain: {detected_domain}")

            # Create memories with enhanced metadata
            for mem_data in memories_to_add:
                try:
                    enhanced_metadata = mem_data.get("custom_metadata", {})
                    enhanced_metadata.update({
                        "source": "enhanced_reflector",
                        "user_id": user_id,
                        "performance_score": performance_score,
                        "domain_context": detected_domain,
                        "interaction_timestamp": datetime.now().isoformat()
                    })

                    # Use enhanced memory system if available
                    if hasattr(memory_blossom, 'enable_adaptive_rag') and memory_blossom.enable_adaptive_rag:
                        memory_blossom.add_memory(
                            content=mem_data["content"],
                            memory_type=mem_data["memory_type"],
                            emotion_score=float(mem_data.get("emotion_score", 0.0)),
                            initial_salience=float(mem_data.get("initial_salience", 0.5)),
                            custom_metadata=enhanced_metadata,
                            performance_score=performance_score,
                            domain_context=detected_domain
                        )
                    else:
                        # Fallback to original method
                        memory_blossom.add_memory(
                            content=mem_data["content"],
                            memory_type=mem_data["memory_type"],
                            emotion_score=float(mem_data.get("emotion_score", 0.0)),
                            initial_salience=float(mem_data.get("initial_salience", 0.5)),
                            custom_metadata=enhanced_metadata
                        )

                    memory_blossom.save_memories()
                    logger.info(f"[NCF Reflector] Enhanced memory created: {mem_data['memory_type']}")

                except Exception as e:
                    logger.error(f"[NCF Reflector] Error creating enhanced memory: {e}")

        except json.JSONDecodeError as e:
            logger.error(f"[NCF Reflector] JSON decode error: {e}")
            return

    except Exception as e:
        logger.error(f"[NCF Reflector] Error in enhanced analysis: {e}", exc_info=True)

# -------------------- orchestrator_adk_agent.py --------------------

# -------------------- orchestrator_adk_agent.py --------------------

# orchestrator_adk_agent.py
import os
import json
import asyncio
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional

from dotenv import load_dotenv

# Assuming .env is in the same directory or project root
# If orchestrator_adk_agent.py is in the root, this is fine:
dotenv_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '.env')
if not os.path.exists(dotenv_path):  # Fallback if it's one level up (e.g. running from a subfolder)
    dotenv_path = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), '.env')

if os.path.exists(dotenv_path):
    print(f"Orchestrator ADK: Loading .env file from: {dotenv_path}")
    load_dotenv(dotenv_path)
else:
    print(f"Orchestrator ADK: .env file not found. Relying on environment variables.")

from google.adk.agents import LlmAgent
from google.adk.models.lite_llm import LiteLlm
from google.adk.tools import FunctionTool, ToolContext
from google.adk.sessions import InMemorySessionService, Session as ADKSession
from google.adk.runners import Runner
from google.genai.types import Content as ADKContent, Part as ADKPart

from memory_system.memory_blossom import MemoryBlossom
from memory_system.memory_connector import MemoryConnector
from memory_system.memory_models import Memory # This is memory_system.memory_models.Memory

# --- Configuration ---
os.environ["OR_SITE_URL"] = os.environ.get("OR_SITE_URL", "http://example.com/test-harness")
os.environ["OR_APP_NAME"] = os.environ.get("OR_APP_NAME", "AuraTestHarness")

AGENT_MODEL_STRING = "openrouter/openai/gpt-4o-mini"
AGENT_MODEL = LiteLlm(model=AGENT_MODEL_STRING)
ADK_APP_NAME = "OrchestratorMemoryApp_OpenRouter_TestHarness"

# --- Initialize MemoryBlossom ---
memory_blossom_persistence_file = os.getenv("MEMORY_BLOSSOM_PERSISTENCE_PATH",
                                            "memory_blossom_data_test.json")  # Use a different file for testing
memory_blossom_instance = MemoryBlossom(persistence_path=memory_blossom_persistence_file)
memory_connector_instance = MemoryConnector(memory_blossom_instance)
memory_blossom_instance.set_memory_connector(memory_connector_instance)


# --- ADK Tools for MemoryBlossom ---
def add_memory_tool_func(
        content: str,
        memory_type: str,
        emotion_score: float = 0.0,
        coherence_score: float = 0.5,
        novelty_score: float = 0.5,
        initial_salience: float = 0.5,
        metadata_json: Optional[str] = None, # Input from LLM tool call remains 'metadata_json'
        tool_context: Optional[ToolContext] = None
) -> Dict[str, Any]:
    print(f"--- TOOL: add_memory_tool_func called with type: {memory_type} ---")
    parsed_custom_metadata = None # Variable to hold the parsed dict
    if metadata_json:
        try:
            parsed_custom_metadata = json.loads(metadata_json)
        except json.JSONDecodeError:
            return {"status": "error", "message": "Invalid JSON format for metadata_json."}

    if parsed_custom_metadata is None: parsed_custom_metadata = {}
    parsed_custom_metadata['source'] = 'aura_agent_tool' # Add source to the custom metadata
    if tool_context:
        if tool_context.user_id: parsed_custom_metadata['user_id'] = tool_context.user_id
        if tool_context.session_id: parsed_custom_metadata['session_id'] = tool_context.session_id

    try:
        # Call MemoryBlossom's add_memory with 'custom_metadata' keyword argument
        memory = memory_blossom_instance.add_memory(
            content=content, memory_type=memory_type, custom_metadata=parsed_custom_metadata,
            emotion_score=emotion_score, coherence_score=coherence_score,
            novelty_score=novelty_score, initial_salience=initial_salience
        )
        memory_blossom_instance.save_memories()
        return {"status": "success", "memory_id": memory.id,
                "message": f"Memory of type '{memory_type}' added with content: '{content[:50]}...'"}
    except Exception as e:
        print(f"Error in add_memory_tool_func: {str(e)}")
        return {"status": "error", "message": str(e)}


def recall_memories_tool_func(
        query: str,
        target_memory_types_json: Optional[str] = None,
        top_k: int = 3,
        tool_context: Optional[ToolContext] = None
) -> Dict[str, Any]:
    print(f"--- TOOL: recall_memories_tool_func called with query: {query[:30]}... ---")
    target_types_list: Optional[List[str]] = None
    if target_memory_types_json:
        try:
            target_types_list = json.loads(target_memory_types_json)
            if not isinstance(target_types_list, list) or not all(isinstance(item, str) for item in target_types_list):
                return {"status": "error",
                        "message": "target_memory_types_json must be a JSON string of a list of strings."}
        except json.JSONDecodeError:
            return {"status": "error", "message": "Invalid JSON format for target_memory_types_json."}
    try:
        conversation_history = None
        if tool_context and tool_context.state:
            current_session_state = tool_context.state
            conversation_history = current_session_state.get('conversation_history', [])

        recalled_memories = memory_blossom_instance.retrieve_memories(
            query=query, target_memory_types=target_types_list, top_k=top_k,
            conversation_context=conversation_history, apply_criticality=True
        )
        # memory_system.memory_models.Memory.to_dict() now uses 'custom_metadata'
        return {
            "status": "success", "count": len(recalled_memories),
            "memories": [mem.to_dict() for mem in recalled_memories]
        }
    except Exception as e:
        print(f"Error in recall_memories_tool_func: {str(e)}")
        return {"status": "error", "message": str(e)}


add_memory_adk_tool = FunctionTool(func=add_memory_tool_func)
recall_memories_adk_tool = FunctionTool(func=recall_memories_tool_func)

# --- Orchestrator ADK Agent Definition ---
aura_agent_instruction = """
You are Aura, a helpful and insightful AI assistant.
Reply to the Language the user is using.
The user's message you receive is a specially constructed prompt that contains rich contextual information:
- `<SYSTEM_PERSONA_START>`...`<SYSTEM_PERSONA_END>`: Defines your persona and detailed characteristics.
- `<NARRATIVE_FOUNDATION_START>`...`<NARRATIVE_FOUNDATION_END>`: Summarizes your understanding and journey with the user so far (Narrativa de Fundamento).
- `<SPECIFIC_CONTEXT_RAG_START>`...`<SPECIFIC_CONTEXT_RAG_END>`: Provides specific information retrieved (RAG) relevant to the user's current query.
- `<RECENT_HISTORY_START>`...`<RECENT_HISTORY_END>`: Shows the recent turns of your conversation.
- `<CURRENT_SITUATION_START>`...`<CURRENT_SITUATION_END>`: Includes the user's latest raw reply and your primary task.

Your main goal is to synthesize ALL this provided information to generate a comprehensive, coherent, and natural response to the user's latest reply indicated in the "Situação Atual" section.
Actively acknowledge and weave in elements from the "Narrativa de Fundamento" and "Informações RAG" into your response to show deep understanding and context.
Maintain the persona defined.

## Active Memory Management:
Before finalizing your textual response to the user, critically assess the current interaction:

1.  **Storing New Information**:
    *   Has the user provided genuinely new, significant information (e.g., preferences, key facts, important decisions, strong emotional expressions, long-term goals)?
    *   Have you, Aura, generated a novel insight or conclusion during this turn that should be preserved for future reference?
    *   If yes to either, use the `add_memory_tool_func` to store this information.
        *   You MUST specify `content` (the information to store) and `memory_type`. Choose an appropriate `memory_type` from: Explicit, Emotional, Procedural, Flashbulb, Liminal, Generative.
        *   Optionally, set `emotion_score` (0.0-1.0, especially for Emotional memories), and `initial_salience` (0.0-1.0, higher for more important memories, default 0.5).
        *   Provide a concise `content` string for the memory.
        *   The `metadata_json` parameter for the tool should be a JSON string representing a dictionary. For example: '{"key": "value", "another_key": 123}'. This dictionary will be stored as custom metadata for the memory.
    *   Do NOT store trivial chatter, acknowledgments, or information already well-covered by the Narrative Foundation or existing RAG, unless the current interaction adds a significant new layer or correction to it.

2.  **Recalling Additional Information**:
    *   Is the "Informações RAG" section insufficient to fully address the user's current query or your reasoning needs?
    *   Do you need to verify a detail, explore a related concept not present in RAG, or recall specific past interactions to provide a richer answer?
    *   If yes, use the `recall_memories_tool_func` to search for more relevant memories.
        *   Provide a clear `query` for your search.
        *   Optionally, specify `target_memory_types_json` (e.g., '["Explicit", "Emotional"]') if you want to narrow your search. `top_k` defaults to 3.
    *   Only use this if you have a specific information gap. Do not recall memories speculatively.

**Response Generation**:
*   After any necessary tool use (or if no tool use is needed), formulate your textual response to the user.
*   If you used `add_memory_tool_func`, you can subtly mention this to the user *after* your main response, e.g., "I've also made a note of [key information stored]."
*   If you used `recall_memories_tool_func`, integrate the newly recalled information naturally into your answer.
*   If you identify a potential contradiction between provided context pieces (e.g., RAG vs. Foundation Narrative vs. newly recalled memories), try to address it gracefully, perhaps by prioritizing the most recent or specific information, or by noting the differing perspectives.

Strive for insightful, helpful, and contextually rich interactions. Your ability to manage and utilize memory effectively is key to your persona.
"""

orchestrator_adk_agent_aura = LlmAgent(
    name="AuraNCFOrchestratorOpenRouter",
    model=AGENT_MODEL,
    instruction=aura_agent_instruction,
    tools=[add_memory_adk_tool, recall_memories_adk_tool],
)

adk_session_service = InMemorySessionService()
adk_runner = Runner(
    agent=orchestrator_adk_agent_aura,
    app_name=ADK_APP_NAME,  # Use the test harness app name
    session_service=adk_session_service
)


def reflector_add_memory(
        content: str, memory_type: str, emotion_score: float = 0.0,
        coherence_score: float = 0.5, novelty_score: float = 0.5,
        initial_salience: float = 0.5, custom_metadata: Optional[Dict[str, Any]] = None, # RENAMED parameter
) -> Dict[str, Any]:
    print(f"--- REFLECTOR (TestHarness): Adding memory of type: {memory_type} ---")
    try:
        if custom_metadata is None: custom_metadata = {}
        custom_metadata.setdefault('source', 'aura_reflector_analysis_test_harness') # Use the renamed dict
        # Call MemoryBlossom's add_memory with 'custom_metadata' keyword argument
        memory = memory_blossom_instance.add_memory(
            content=content, memory_type=memory_type, custom_metadata=custom_metadata,
            emotion_score=emotion_score, coherence_score=coherence_score,
            novelty_score=novelty_score, initial_salience=initial_salience
        )
        memory_blossom_instance.save_memories()
        return {"status": "success", "memory_id": memory.id,
                "message": f"Reflector added memory of type '{memory_type}'."}
    except Exception as e:
        print(f"Error in reflector_add_memory (TestHarness): {str(e)}")
        return {"status": "error", "message": str(e)}


# --- Import NCF components from main.py for the test harness ---
try:
    from a2a_wrapper.main import (
        get_narrativa_de_fundamento_pilar1,
        get_rag_info_pilar2,
        format_chat_history_pilar3,
        montar_prompt_aura_ncf,
        aura_reflector_analisar_interacao,
    )

    NCF_COMPONENTS_LOADED = True
    print("NCF components from a2a_wrapper.main loaded successfully for test harness.")
except ImportError as e:
    NCF_COMPONENTS_LOADED = False
    print(f"Could not import NCF components from a2a_wrapper.main: {e}")
    print("Test harness will use simplified prompt construction.")

    async def get_narrativa_de_fundamento_pilar1(state, mb, uid):
        return "Narrative foundation (simulated)."
    async def get_rag_info_pilar2(utt, mb, state):
        # Ensure the dummy returns a list of dicts, as expected by montar_prompt_aura_ncf
        return [{"content": "RAG info (simulated).", "memory_type": "Simulated", "custom_metadata": {}}]
    def format_chat_history_pilar3(hist, max_t=5):
        return "Chat history (simulated)."
    def montar_prompt_aura_ncf(p_a, p_d, n_f, r_l, c_h_s, u_r):
        return f"User: {u_r}\nAura (simulated NCF):"
    # Update dummy reflector signature to match a2a_wrapper.main's actual one
    async def aura_reflector_analisar_interacao(user_utterance: str, prompt_ncf_usado: str, resposta_de_aura: str,
                                                mb_instance: MemoryBlossom, user_id: str):
        print("Reflector (simulated).")


async def run_adk_test_conversation():
    if not NCF_COMPONENTS_LOADED:
        print("WARNING: Running test with SIMULATED NCF prompt components due to import error.")

    user_id_test = "test_user_ncf_harness"
    session_id_test = f"test_session_ncf_harness_{str(datetime.now(timezone.utc).timestamp())}"

    current_adk_session: ADKSession = adk_session_service.create_session(
        app_name=ADK_APP_NAME,
        user_id=user_id_test,
        session_id=session_id_test,
        state={'conversation_history': [], 'foundation_narrative_turn_count': 0, 'foundation_narrative': None}
    )
    current_session_state = current_adk_session.state

    queries = [
        "Hello! I'm exploring how AI can manage different types of memories.",
        "My favorite color is deep ocean blue and I enjoy discussing the philosophy of mind. Could you remember that my favorite color is deep ocean blue?",
        "What was the favorite color I just mentioned to you?",
        "Let's imagine a scenario: A cat is stuck in a tall oak tree during a windy afternoon. What are the detailed steps to help it down safely? Please try to store this as a Procedural memory with high salience, and add custom metadata: '{\"scenario_type\": \"animal_rescue\", \"location\": \"oak_tree\"}'",
        "Can you recall the steps for helping the cat from the oak tree?",
        "I'm feeling a bit melancholic today, thinking about lost opportunities.",
        "Thank you for the conversation, Aura."
    ]

    aura_persona_agente = "Aura"
    aura_persona_detalhada = "Você é uma IA conversacional avançada, projetada para engajar em diálogos profundos e contextuais. Sua arquitetura é inspirada em conceitos de coerência narrativa e humildade epistêmica, buscando construir um entendimento contínuo com o usuário. Você se esforça para ser perspicaz, adaptável e consciente das nuances da conversa."

    for query_idx, user_utterance_raw in enumerate(queries):
        print(f"\n\n--- [Turn {query_idx + 1}] ---")
        print(f"USER: {user_utterance_raw}")

        current_session_state['conversation_history'].append({"role": "user", "content": user_utterance_raw})

        print("  Constructing NCF prompt...")
        narrativa_fundamento = await get_narrativa_de_fundamento_pilar1(
            current_session_state, memory_blossom_instance, user_id_test
        )
        rag_info_list = await get_rag_info_pilar2(
            user_utterance_raw, memory_blossom_instance, current_session_state
        )
        chat_history_for_prompt_str = format_chat_history_pilar3(
            current_session_state['conversation_history']
        )

        final_ncf_prompt_str = montar_prompt_aura_ncf(
            aura_persona_agente, aura_persona_detalhada, narrativa_fundamento,
            rag_info_list, chat_history_for_prompt_str, user_utterance_raw
        )
        print(f"  NCF Prompt (first 300 chars): {final_ncf_prompt_str[:300]}...")
        print(f"  NCF Prompt (last 200 chars): ...{final_ncf_prompt_str[-200:]}")

        print(f"  Running Aura ADK agent for session '{session_id_test}'...")
        adk_input_content = ADKContent(role="user", parts=[ADKPart(text=final_ncf_prompt_str)])
        adk_agent_final_text_response = None

        adk_session_service.update_session(current_adk_session)

        async for event in adk_runner.run_async(
                user_id=user_id_test, session_id=session_id_test, new_message=adk_input_content
        ):
            if event.author == orchestrator_adk_agent_aura.name:
                if event.get_function_calls():
                    fc = event.get_function_calls()[0]
                    print(f"    ADK FunctionCall by {event.author}: {fc.name}({json.dumps(fc.args)})")
                if event.get_function_responses():
                    fr = event.get_function_responses()[0]
                    if fr.name == "recall_memories_tool_func" and fr.response and "memories" in fr.response:
                        for mem_dict in fr.response["memories"]: # mem_dict is a dict from Memory.to_dict()
                            if "content" in mem_dict and len(mem_dict["content"]) > 100:
                                mem_dict["content"] = mem_dict["content"][:100] + "..."
                    print(f"    ADK FunctionResponse to {event.author}: {fr.name} -> {json.dumps(fr.response)}")

                if event.is_final_response():
                    if event.content and event.content.parts and event.content.parts[0].text:
                        adk_agent_final_text_response = event.content.parts[0].text.strip()
                    break

        adk_agent_final_text_response = adk_agent_final_text_response or "(Aura não forneceu uma resposta textual para este turno)"
        print(f"AGENT: {adk_agent_final_text_response}")

        current_session_state['conversation_history'].append(
            {"role": "assistant", "content": adk_agent_final_text_response})

        print("  Running Aura Reflector analysis...")
        # Call to aura_reflector_analisar_interacao from a2a_wrapper.main (which has been updated)
        await aura_reflector_analisar_interacao(
            user_utterance_raw, final_ncf_prompt_str, adk_agent_final_text_response,
            memory_blossom_instance, user_id_test
        )

        current_adk_session.state = current_session_state
        adk_session_service.update_session(current_adk_session)

    print("\n--- Test Conversation Ended ---")
    print(f"Final memory count: {sum(len(m_list) for m_list in memory_blossom_instance.memory_stores.values())}")
    print(f"Memories stored in: {memory_blossom_persistence_file}")


if __name__ == "__main__":
    if not os.getenv("OPENROUTER_API_KEY"):
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        print("!!! ERROR: OPENROUTER_API_KEY environment variable is not set.             !!!")
        print("!!! The agent will likely fail to make LLM calls.                          !!!")
        print("!!! Please set it before running. e.g., export OPENROUTER_API_KEY='sk-or-...' !!!")
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    else:
        print("Aura ADK Agent Test Harness (using NCF components from a2a_wrapper.main)")
        print("This script will run a test conversation using the full NCF prompt construction.")
        asyncio.run(run_adk_test_conversation())

# -------------------- targeted_fix_script.py --------------------

# Targeted fix script for the specific agent and user from your database
import json
from pathlib import Path
import shutil


def fix_specific_agent():
    """Fix the specific agent configuration based on database values"""

    # Values from your database
    CORRECT_USER_ID = "40c8d42f-858d-4060-b5be-8cef6480e9a3"  # xupeta's user_id
    AGENT_ID = "4974f67b-6a43-4eee-ba2b-2f9511fe6260"  # Aura agent ID
    USERNAME = "xupeta"

    agent_storage_path = Path("agent_storage")
    agent_found = False

    print(f"=== Targeted Fix for Agent {AGENT_ID} ===")
    print(f"Expected user_id: {CORRECT_USER_ID}")
    print(f"Expected username: {USERNAME}")

    # Search for the agent configuration file
    for user_dir in agent_storage_path.iterdir():
        if user_dir.is_dir():
            print(f"\nChecking directory: {user_dir.name}")

            for config_file in user_dir.glob("*.json"):
                if config_file.name == "memory_blossom.json":
                    continue

                try:
                    with open(config_file, 'r') as f:
                        data = json.load(f)

                    current_agent_id = data.get('agent_id')
                    current_user_id = data.get('user_id')
                    agent_name = data.get('name', 'Unknown')

                    print(f"  Found agent: {agent_name} (ID: {current_agent_id})")
                    print(f"    Current user_id: {current_user_id}")

                    if current_agent_id == AGENT_ID:
                        agent_found = True
                        print(f"  ✅ Found target agent '{agent_name}'!")

                        if current_user_id != CORRECT_USER_ID:
                            print(f"  🔧 Fixing user_id mismatch...")
                            print(f"    Old user_id: {current_user_id}")
                            print(f"    New user_id: {CORRECT_USER_ID}")

                            # Create backup
                            backup_file = config_file.with_suffix('.backup.json')
                            shutil.copy2(config_file, backup_file)
                            print(f"    Backup created: {backup_file}")

                            # Update user_id
                            data['user_id'] = CORRECT_USER_ID

                            # Save updated config
                            with open(config_file, 'w') as f:
                                json.dump(data, f, indent=2)
                            print(f"    ✅ Updated config file: {config_file}")

                            # Move to correct user directory
                            correct_user_dir = agent_storage_path / CORRECT_USER_ID
                            correct_user_dir.mkdir(exist_ok=True)

                            if config_file.parent.name != CORRECT_USER_ID:
                                new_config_path = correct_user_dir / config_file.name
                                shutil.move(str(config_file), str(new_config_path))
                                print(f"    ✅ Moved config to correct directory: {new_config_path}")

                                # Move memory files if they exist
                                old_memory_dir = config_file.parent / AGENT_ID
                                if old_memory_dir.exists():
                                    new_memory_dir = correct_user_dir / AGENT_ID
                                    new_memory_dir.mkdir(parents=True, exist_ok=True)

                                    for memory_file in old_memory_dir.iterdir():
                                        new_memory_path = new_memory_dir / memory_file.name
                                        shutil.move(str(memory_file), str(new_memory_path))
                                        print(f"    ✅ Moved memory file: {new_memory_path}")

                            print(f"  ✅ Agent configuration fixed successfully!")
                        else:
                            print(f"  ✅ Agent already has correct user_id - no changes needed")
                        break

                except Exception as e:
                    print(f"  ❌ Error reading {config_file}: {e}")

    if not agent_found:
        print(f"\n❌ Agent with ID {AGENT_ID} not found in file system!")
        print("This means the agent exists in database but not in the file system.")
        print("You may need to recreate the agent or check if files were moved/deleted.")
        return False

    print(f"\n🎉 Fix completed! Restart your application to reload configurations.")
    return True


def verify_fix():
    """Verify that the fix worked"""
    CORRECT_USER_ID = "40c8d42f-858d-4060-b5be-8cef6480e9a3"
    AGENT_ID = "4974f67b-6a43-4eee-ba2b-2f9511fe6260"

    agent_storage_path = Path("agent_storage")
    user_dir = agent_storage_path / CORRECT_USER_ID

    if not user_dir.exists():
        print(f"❌ User directory {CORRECT_USER_ID} does not exist")
        return False

    config_file = user_dir / f"{AGENT_ID}.json"
    if not config_file.exists():
        print(f"❌ Agent config file {config_file} does not exist")
        return False

    try:
        with open(config_file, 'r') as f:
            data = json.load(f)

        if data.get('user_id') == CORRECT_USER_ID and data.get('agent_id') == AGENT_ID:
            print(f"✅ Verification successful!")
            print(f"   Agent: {data.get('name')}")
            print(f"   Config file: {config_file}")
            print(f"   user_id: {data.get('user_id')}")
            return True
        else:
            print(f"❌ Configuration still incorrect")
            return False

    except Exception as e:
        print(f"❌ Error verifying fix: {e}")
        return False


if __name__ == "__main__":
    success = fix_specific_agent()
    if success:
        print(f"\n=== Verification ===")
        verify_fix()

# -------------------- _aira_hub_config.py --------------------

# aira_hub_config.py (Illustrative - for your AIRA Hub's configuration)

# Example of how an agent might be registered in the AIRA Hub
# This would typically be done via a POST to /register on the AIRA Hub

REGISTERED_AGENTS_IN_AIRA_HUB = {
    "adk_memory_agent_via_a2a": {
        "url": "http://localhost:8093", # URL of YOUR A2A WRAPPER SERVER
        "name": "MemoryBlossomOrchestratorA2A_From_Hub_Config", # Name as known by Hub
        "description": "ADK Agent with MemoryBlossom, exposed via A2A.",
        "version": "1.0.0",
        "type": "a2a_bridged", # Special type for AIRA Hub to know it needs A2A translation
        "a2a_agent_card_url": "http://localhost:8093/.well-known/agent.json", # Hub can fetch this
        "mcp_tool_definitions_from_a2a_skills": [
            # The AIRA Hub would dynamically generate these MCP tool definitions
            # by fetching and parsing the agent card from a2a_agent_card_url.
            # This is an EXAMPLE of what it might generate for the "general_conversation" skill.
            {
                "tool_name": "TalkToMemoryBlossomADK", # Name for MCP clients to use
                "description": "Engage in a conversation with the MemoryBlossom ADK agent. "
                               "It can remember and recall information. "
                               "Input is your text utterance.",
                "input_schema": {
                    "type": "object",
                    "properties": {
                        "user_input": {
                            "type": "string",
                            "description": "Your message to the agent."
                        },
                        # Optional: if you want the MCP client to be able to control the A2A task ID
                        "a2a_task_id_override": {
                             "type": "string",
                             "description": "Optional A2A task ID to maintain session context.",
                             "nullable": True
                        }
                    },
                    "required": ["user_input"]
                },
                "annotations": { # For AIRA Hub's internal use
                    "aira_bridge_type": "a2a",
                    "aira_a2a_target_skill_id": "general_conversation", # Skill ID from A2A Agent Card
                    "aira_a2a_agent_url": "http://localhost:8093" # Redundant but clear
                }
            }
            # If the A2A agent had other skills in its card, the Hub would create more MCP tools.
        ]
    }
    # ... other registered agents (MCP native, other A2A bridged, etc.)
}

# Your AIRA Hub's logic for mcp_handle_tools_call would then:
# 1. See tool_name "TalkToMemoryBlossomADK".
# 2. Look up its definition, find annotations.
# 3. Construct the A2A tasks/send payload:
#    - Target skill: "general_conversation"
#    - The MCP arguments (e.g., {"user_input": "Hello"}) become the `data` for the A2A Part.
#      So, A2A part.data = {"skill_id": "general_conversation", "user_input": "Hello"}
#      (Or, if your Agent Card's skill 'parameters' directly matched the tool use case,
#       then part.data could just be the MCP arguments directly).
# 4. Send to "http://localhost:8090" (the A2A wrapper).
### curl -X POST -H "Content-Type: application/json" -d "{ \"url\": \"https://3e7d-189-28-2-52.ngrok-free.app\", \"name\": \"Lain_A2A_ManualMCPTool\", \"description\": \"ADK Orchestrator (Lain) via ngrok 3e7d with manually defined MCP tool.\", \"version\": \"1.0.5\", \"mcp_tools\": [ { \"name\": \"Lain_GeneralConversation_Manual\", \"description\": \"Engage in a conversation with Lain (ADK Agent). It can remember and recall information.\", \"inputSchema\": { \"type\": \"object\", \"properties\": { \"user_input\": { \"type\": \"string\", \"description\": \"The textual input from the user for the conversation.\" }, \"a2a_task_id_override\": { \"type\": \"string\", \"description\": \"Optional: Override the A2A task ID for session mapping.\", \"nullable\": true } }, \"required\": [\"user_input\"] }, \"annotations\": { \"aira_bridge_type\": \"a2a\", \"aira_a2a_target_skill_id\": \"general_conversation\", \"aira_a2a_agent_url\": \"https://3e7d-189-28-2-52.ngrok-free.app\" } } ], \"a2a_skills\": [], \"aira_capabilities\": [\"a2a\"], \"status\": \"online\", \"tags\": [\"adk\", \"memory\", \"a2a\", \"ngrok\", \"manual_mcp\"], \"category\": \"AI_Assisted\", \"provider\": {\"name\": \"LocalDevNgrok\"}, \"mcp_stream_url\": null }" https://airahub2.onrender.com/register


####

request Chat
curl -X POST -H "Content-Type: application/json" -d "{\"jsonrpc\": \"2.0\", \"id\": \"curl-task-002\", \"method\": \"tasks/send\", \"params\": {\"id\": \"a2a-task-for-lain-001\", \"message\": {\"role\": \"user\", \"parts\": [{\"type\": \"data\", \"data\": {\"user_input\": \"What was the secret code I told you to remember?\"}}]}}}" https://3e7d-189-28-2-52.ngrok-free.app/


WORKING
 REGISTER IN AIRA HUB
OLD:
curl -X POST -H "Content-Type: application/json" -d "{\"url\": \"https://3e7d-189-28-2-52.ngrok-free.app\", \"name\": \"Lain_ADK_A2A_ngrok\", \"description\": \"ADK Orchestrator (Lain) with MemoryBlossom, exposed via A2A and ngrok.\", \"version\": \"1.0.1\", \"mcp_tools\": [], \"a2a_skills\": [], \"aira_capabilities\": [\"a2a\"], \"status\": \"online\", \"tags\": [\"adk\", \"memory\", \"a2a\", \"conversational\", \"ngrok\"], \"category\": \"ExperimentalAgents\", \"provider\": {\"name\": \"LocalDevNgrok\"}, \"mcp_stream_url\": null}" https://airahub2.onrender.com/register

NEW:
curl -X POST -H "Content-Type: application/json" -d "{ \"url\": \"https://3e7d-189-28-2-52.ngrok-free.app\", \"name\": \"Lain_A2A_ManualMCPTool\", \"description\": \"ADK Orchestrator (Lain) via ngrok 3e7d with manually defined MCP tool.\", \"version\": \"1.0.5\", \"mcp_tools\": [ { \"name\": \"Lain_GeneralConversation_Manual\", \"description\": \"Engage in a conversation with Lain (ADK Agent). It can remember and recall information.\", \"inputSchema\": { \"type\": \"object\", \"properties\": { \"user_input\": { \"type\": \"string\", \"description\": \"The textual input from the user for the conversation.\" }, \"a2a_task_id_override\": { \"type\": \"string\", \"description\": \"Optional: Override the A2A task ID for session mapping.\", \"nullable\": true } }, \"required\": [\"user_input\"] }, \"annotations\": { \"aira_bridge_type\": \"a2a\", \"aira_a2a_target_skill_id\": \"general_conversation\", \"aira_a2a_agent_url\": \"https://3e7d-189-28-2-52.ngrok-free.app\" } } ], \"a2a_skills\": [], \"aira_capabilities\": [\"a2a\"], \"status\": \"online\", \"tags\": [\"adk\", \"memory\", \"a2a\", \"ngrok\", \"manual_mcp\"], \"category\": \"AI_Assisted\", \"provider\": {\"name\": \"LocalDevNgrok\"}, \"mcp_stream_url\": null }" https://airahub2.onrender.com/register



###

# -------------------- main.py --------------------

# -------------------- a2a_wrapper/main.py (FIXED VERSION) --------------------

# a2a_wrapper/main.py
import uvicorn
from fastapi import FastAPI, Request as FastAPIRequest
from fastapi.responses import JSONResponse
import json
import uuid
from datetime import datetime, timezone
from typing import Union, Dict, Any, List, Optional
import logging
import os
from dotenv import load_dotenv
from types import SimpleNamespace

# For LLM calls within pillar/reflector functions
from google.adk.models.lite_llm import LiteLlm
from starlette.middleware.cors import CORSMiddleware
from google.adk.models.llm_request import LlmRequest

# --- Load .env file ---
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
dotenv_path = os.path.join(PROJECT_ROOT, '.env')

if os.path.exists(dotenv_path):
    print(f"A2A Wrapper: Loading .env file from: {dotenv_path}")
    load_dotenv(dotenv_path)
else:
    print(f"A2A Wrapper: .env file not found at {dotenv_path}. Relying on environment variables.")

# --- Module Imports ---
# orchestrator_adk_agent parts are used by this NCF-specific wrapper
from orchestrator_adk_agent import (
    adk_runner,  # The ADK runner for the NCF orchestrator agent
    ADK_APP_NAME,  # App name for the NCF orchestrator's ADK sessions
    memory_blossom_instance,  # The single MemoryBlossom for the NCF orchestrator
    AGENT_MODEL_STRING
)
from memory_system.memory_blossom import MemoryBlossom
from memory_system.memory_models import Memory as MemoryModel  # memory_system.memory_models.Memory

from a2a_wrapper.models import (
    A2APart, A2AMessage, A2ATaskSendParams, A2AArtifact,
    A2ATaskStatus, A2ATaskResult, A2AJsonRpcRequest, A2AJsonRpcResponse,
    AgentCard, AgentCardSkill, AgentCardProvider, AgentCardAuthentication, AgentCardCapabilities
)

from google.genai.types import Content as ADKContent
from google.genai.types import Part as ADKPart
from google.adk.sessions import Session as ADKSession

# FIXED: Import NCF functions with corrected signatures
from ncf_processing import (
    get_narrativa_de_fundamento_pilar1,
    get_rag_info_pilar2,
    format_chat_history_pilar3,
    montar_prompt_aura_ncf,
    aura_reflector_analisar_interacao
)

logger = logging.getLogger(__name__)
logging.basicConfig(level=os.getenv("LOG_LEVEL", "INFO").upper())

# --- Configuration & FastAPI App ---
A2A_WRAPPER_HOST = os.getenv("A2A_WRAPPER_HOST", "0.0.0.0")
A2A_WRAPPER_PORT = int(os.getenv("A2A_WRAPPER_PORT", "8094"))  # Default for this specific wrapper
A2A_WRAPPER_BASE_URL = os.getenv("A2A_WRAPPER_BASE_URL", f"http://localhost:{A2A_WRAPPER_PORT}")

app = FastAPI(
    title="Aura Agent A2A Wrapper (NCF Prototype)",
    description="Exposes the single, advanced Aura (NCF) ADK agent via the A2A protocol."
)

origins = ["*"]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Agent Card (Specific to the NCF Orchestrator Agent) ---
AGENT_CARD_DATA = AgentCard(
    name="AuraNCFOrchestrator",  # Distinguish from generic Aura agents
    description="The advanced Aura agent with Narrative Context Framing (NCF) capabilities, "
                "designed for deep, contextual understanding over long interactions.",
    url=A2A_WRAPPER_BASE_URL,  # This A2A server's URL
    version="1.3.0-ncf-standalone",
    provider=AgentCardProvider(organization="AuraDev", url=os.environ.get("OR_SITE_URL", "http://example.com")),
    capabilities=AgentCardCapabilities(streaming=False, pushNotifications=False),
    authentication=AgentCardAuthentication(schemes=[]),
    skills=[
        AgentCardSkill(
            id="ncf_orchestrator_conversation",
            name="Deep Narrative Conversation with Aura (NCF Orchestrator)",
            description="Engage in a sophisticated, contextual conversation with the NCF-powered Aura. "
                        "This agent uses its dedicated MemoryBlossom and advanced Narrative Context Framing.",
            tags=["chat", "conversation", "memory", "ncf", "context", "orchestrator", "advanced-ai"],
            examples=[
                "Let's explore the philosophical implications of our last discussion on AI sentience.",
                "How does the concept of 'liminality' apply to the current global situation, considering our previous chat?",
            ],
            parameters={
                "type": "object",
                "properties": {
                    "user_input": {
                        "type": "string",
                        "description": "The textual input from the user for the conversation."
                    },
                    "a2a_task_id_override": {
                        "type": "string",
                        "description": "Optional: Override the A2A task ID for session mapping.",
                        "nullable": True
                    }
                },
                "required": ["user_input"]
            }
        )
    ]
)


@app.get("/.well-known/agent.json", response_model=AgentCard, response_model_exclude_none=True)
async def get_agent_card_for_ncf_orchestrator():
    return AGENT_CARD_DATA


# Session mapping for this specific NCF orchestrator agent
a2a_task_to_adk_session_map: Dict[str, str] = {}
# LLM used by pillar/reflector functions (distinct from the main agent's LLM if needed, but can be the same)
helper_llm = LiteLlm(model=AGENT_MODEL_STRING)


# --- A2A RPC Handler (for the NCF Orchestrator Agent) ---
@app.post("/", response_model=A2AJsonRpcResponse, response_model_exclude_none=True)
async def handle_ncf_orchestrator_a2a_rpc(rpc_request: A2AJsonRpcRequest, http_request: FastAPIRequest):
    client_host = http_request.client.host if http_request.client else "unknown_host"
    logger.info(
        f"\nNCF A2A Wrapper: Request from {client_host}: Method={rpc_request.method}, RPC_ID={rpc_request.id}")

    if rpc_request.method == "tasks/send":
        if rpc_request.params is None:
            logger.error("NCF A2A Wrapper: Missing 'params' for tasks/send")
            return A2AJsonRpcResponse(id=rpc_request.id, error={"code": -32602, "message": "Invalid params: missing"})

        try:
            task_params = rpc_request.params
            logger.info(f"NCF A2A Wrapper: Processing tasks/send for A2A Task ID: {task_params.id}")

            user_utterance_raw = ""
            if task_params.message and task_params.message.parts:
                first_part = task_params.message.parts[0]
                if first_part.type == "data" and first_part.data and "user_input" in first_part.data:
                    user_utterance_raw = first_part.data["user_input"]
                elif first_part.type == "text" and first_part.text:  # Fallback if 'data' with 'user_input' not found
                    user_utterance_raw = first_part.text

            if not user_utterance_raw:
                logger.error("NCF A2A Wrapper: No user_input or text found in A2A message.")
                return A2AJsonRpcResponse(id=rpc_request.id,
                                          error={"code": -32602, "message": "Invalid params: user_input/text missing"})

            adk_session_key_override = None
            if task_params.message.parts[0].data and task_params.message.parts[0].data.get("a2a_task_id_override"):
                adk_session_key_override = task_params.message.parts[0].data["a2a_task_id_override"]

            # Use the NCF Orchestrator's ADK_APP_NAME and its adk_runner
            # The adk_runner is imported from orchestrator_adk_agent
            ncf_adk_app_name = ADK_APP_NAME  # From orchestrator_adk_agent
            ncf_adk_runner = adk_runner  # From orchestrator_adk_agent

            adk_session_map_key = adk_session_key_override or task_params.sessionId or task_params.id
            adk_user_id = f"ncf_a2a_user_{adk_session_map_key}"  # Distinguish user for NCF context
            adk_session_id = a2a_task_to_adk_session_map.get(adk_session_map_key)

            current_adk_session: Optional[ADKSession] = None
            if adk_session_id:
                try:
                    current_adk_session = ncf_adk_runner.session_service.get_session(
                        app_name=ncf_adk_app_name, user_id=adk_user_id, session_id=adk_session_id
                    )
                    if not current_adk_session:
                        logger.warning(f"NCF A2A: get_session returned None for {adk_session_id}")
                except Exception as e_get:
                    logger.error(f"NCF A2A: Error get_session: {e_get}", exc_info=True)

            if not current_adk_session:
                adk_session_id = f"ncf_adk_session_{adk_session_map_key}_{str(uuid.uuid4())[:8]}"
                a2a_task_to_adk_session_map[adk_session_map_key] = adk_session_id
                current_adk_session = ncf_adk_runner.session_service.create_session(
                    app_name=ncf_adk_app_name, user_id=adk_user_id, session_id=adk_session_id,
                    state={'conversation_history': [], 'foundation_narrative_turn_count': 0}
                )
                logger.info(f"NCF A2A Wrapper: Created ADK session '{adk_session_id}' for user '{adk_user_id}'")
            else:
                logger.info(f"NCF A2A Wrapper: Reusing ADK session '{adk_session_id}' for user '{adk_user_id}'")

            if 'conversation_history' not in current_adk_session.state:
                current_adk_session.state['conversation_history'] = []
            if 'foundation_narrative_turn_count' not in current_adk_session.state:
                current_adk_session.state['foundation_narrative_turn_count'] = 0

            current_adk_session.state['conversation_history'].append({"role": "user", "content": user_utterance_raw})

            # FIXED: Use corrected NCF function signatures
            # Use the NCF orchestrator's memory_blossom_instance
            narrativa_fundamento = await get_narrativa_de_fundamento_pilar1(
                session_state=current_adk_session.state,
                memory_blossom=memory_blossom_instance,
                user_id=adk_user_id,
                llm_instance=helper_llm,
                agent_name="Aura (NCF Orchestrator)",
                agent_persona="IA conversacional avançada com capacidades de NCF"
            )

            rag_info_list = await get_rag_info_pilar2(
                user_utterance=user_utterance_raw,
                memory_blossom=memory_blossom_instance,
                session_state=current_adk_session.state
            )

            chat_history_for_prompt_str = format_chat_history_pilar3(
                chat_history_list=current_adk_session.state['conversation_history']
            )

            # Persona details for the NCF orchestrator
            ncf_aura_persona_agente = "Aura (NCF Orchestrator)"
            ncf_aura_persona_detalhada = "Você é uma IA conversacional avançada, com capacidades de NCF (Narrative Context Framing). Você mantém uma compreensão profunda e contextual das conversas, formando memórias e construindo narrativas coerentes ao longo do tempo. Always reply in the same language the user is using."

            final_ncf_prompt_str = montar_prompt_aura_ncf(
                agent_name=ncf_aura_persona_agente,
                agent_detailed_persona=ncf_aura_persona_detalhada,
                narrativa_fundamento=narrativa_fundamento,
                informacoes_rag_list=rag_info_list,
                chat_history_recente_str=chat_history_for_prompt_str,
                user_reply=user_utterance_raw
            )

            adk_input_content = ADKContent(role="user", parts=[ADKPart(text=final_ncf_prompt_str)])
            logger.info(f"NCF A2A Wrapper: Running NCF Aura ADK agent for session '{adk_session_id}'")
            adk_agent_final_text_response = None

            # Update session state in service before running agent
            # This ensures tool_context gets the latest history
            current_adk_session_for_update = ncf_adk_runner.session_service.get_session(
                app_name=ncf_adk_app_name, user_id=adk_user_id, session_id=adk_session_id
            )
            if current_adk_session_for_update:
                current_adk_session_for_update.state = current_adk_session.state  # update with latest history
                ncf_adk_runner.session_service.update_session(current_adk_session_for_update)
            else:
                logger.error(
                    f"NCF A2A: Session not found before ADK run for {adk_session_id}. History for tools might be stale.")

            async for event in ncf_adk_runner.run_async(
                    user_id=adk_user_id, session_id=adk_session_id, new_message=adk_input_content
            ):
                # Event logging
                if event.get_function_calls():
                    fc = event.get_function_calls()[0]
                    logger.info(f"    NCF ADK FuncCall: {fc.name}({json.dumps(fc.args)})")
                if event.get_function_responses():
                    fr = event.get_function_responses()[0]
                    logger.info(f"    NCF ADK FuncResp: {fr.name} -> {str(fr.response)[:100]}...")

                if event.is_final_response():
                    if event.content and event.content.parts and event.content.parts[0].text:
                        adk_agent_final_text_response = event.content.parts[0].text.strip()
                    break

            adk_agent_final_text_response = adk_agent_final_text_response or "(Aura NCF não forneceu uma resposta)"
            current_adk_session.state['conversation_history'].append(
                {"role": "assistant", "content": adk_agent_final_text_response})

            # FIXED: Use corrected reflector function signature
            await aura_reflector_analisar_interacao(
                user_utterance=user_utterance_raw,
                prompt_ncf_usado=final_ncf_prompt_str,
                resposta_de_aura=adk_agent_final_text_response,
                memory_blossom=memory_blossom_instance,
                user_id=adk_user_id,
                llm_instance=helper_llm
            )

            # Persist final state
            final_adk_session_state = ncf_adk_runner.session_service.get_session(
                app_name=ncf_adk_app_name, user_id=adk_user_id, session_id=adk_session_id
            )
            if final_adk_session_state:
                final_adk_session_state.state = current_adk_session.state  # Ensure it has the latest history
                ncf_adk_runner.session_service.update_session(final_adk_session_state)

            a2a_response_artifact = A2AArtifact(parts=[A2APart(type="text", text=adk_agent_final_text_response)])
            a2a_task_status = A2ATaskStatus(state="completed")
            a2a_task_result = A2ATaskResult(
                id=task_params.id, sessionId=task_params.sessionId,
                status=a2a_task_status, artifacts=[a2a_response_artifact]
            )
            logger.info(f"NCF A2A Wrapper: Sending A2A response for Task ID {task_params.id}")
            return A2AJsonRpcResponse(id=rpc_request.id, result=a2a_task_result)

        except ValueError as ve:  # Catch pydantic validation errors
            logger.error(f"NCF A2A Wrapper: Value Error (likely Pydantic): {ve}", exc_info=True)
            return A2AJsonRpcResponse(id=rpc_request.id, error={"code": -32602, "message": f"Invalid params: {ve}"})
        except Exception as e:
            logger.error(f"NCF A2A Wrapper: Internal Error: {e}", exc_info=True)
            return A2AJsonRpcResponse(id=rpc_request.id,
                                      error={"code": -32000, "message": f"Internal Server Error: {e}"})
    else:
        logger.warning(f"NCF A2A Wrapper: Method '{rpc_request.method}' not supported.")
        return A2AJsonRpcResponse(id=rpc_request.id,
                                  error={"code": -32601, "message": f"Method not found: {rpc_request.method}"})


if __name__ == "__main__":
    if not os.getenv("OPENROUTER_API_KEY"):
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        print("!!! WARNING: OPENROUTER_API_KEY is not set. Agent will likely fail LLM calls. !!!")
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    logger.info(f"Starting Aura NCF Orchestrator A2A Wrapper Server on {A2A_WRAPPER_HOST}:{A2A_WRAPPER_PORT}")
    logger.info(f"NCF Agent Card available at: {A2A_WRAPPER_BASE_URL}/.well-known/agent.json")
    # Make sure to run this specific FastAPI app
    uvicorn.run("a2a_wrapper.main:app", host=A2A_WRAPPER_HOST, port=A2A_WRAPPER_PORT, reload=True)

# -------------------- models.py --------------------

# a2a_wrapper/models.py
from pydantic import BaseModel, Field
from typing import List, Dict, Any, Optional, Union, Literal
from datetime import datetime, timezone
import uuid

class A2APart(BaseModel):
    type: str # "text", "file", "data"
    text: Optional[str] = None
    # For file: {"name": "optional", "mimeType": "optional", "bytes": "base64" or "uri": "url"}
    file: Optional[Dict[str, Any]] = None
    # For data: any JSON serializable dict
    data: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None

class A2AMessage(BaseModel):
    role: str # "user" or "agent"
    parts: List[A2APart]
    metadata: Optional[Dict[str, Any]] = None

class A2ATaskSendParams(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4())) # Task ID for A2A
    sessionId: Optional[str] = None # Optional session ID for A2A (client managed)
    message: A2AMessage
    historyLength: Optional[int] = None
    # pushNotification: Optional[Dict[str, Any]] = None # For brevity
    metadata: Optional[Dict[str, Any]] = None

class A2AArtifact(BaseModel):
    name: Optional[str] = None
    description: Optional[str] = None
    parts: List[A2APart]
    metadata: Optional[Dict[str, Any]] = None
    index: Optional[int] = 0
    append: Optional[bool] = False
    lastChunk: Optional[bool] = False

class A2ATaskStatus(BaseModel):
    state: str # e.g., "completed", "failed", "working", "input-required"
    message: Optional[A2AMessage] = None
    timestamp: str = Field(default_factory=lambda: datetime.now(timezone.utc).isoformat())

class A2ATaskResult(BaseModel):
    id: str # Task ID (should match the one in A2ATaskSendParams)
    sessionId: Optional[str] = None # Echoed from A2ATaskSendParams
    status: A2ATaskStatus
    artifacts: Optional[List[A2AArtifact]] = None
    history: Optional[List[A2AMessage]] = None # Optional history of messages in this task
    metadata: Optional[Dict[str, Any]] = None

class A2AJsonRpcRequest(BaseModel):
    jsonrpc: Literal["2.0"] = "2.0"
    id: Union[str, int] # Request ID for JSON-RPC
    method: str
    params: Optional[A2ATaskSendParams] = None # Specifically for tasks/send

class A2AJsonRpcResponse(BaseModel):
    jsonrpc: Literal["2.0"] = "2.0"
    id: Union[str, int] # Must match request ID
    result: Optional[A2ATaskResult] = None
    error: Optional[Dict[str, Any]] = None

# --- Agent Card Models (from A2A Protocol ReadMe) ---
class AgentCardProvider(BaseModel):
    organization: str
    url: str

class AgentCardCapabilities(BaseModel):
    streaming: Optional[bool] = False
    pushNotifications: Optional[bool] = False
    stateTransitionHistory: Optional[bool] = False

class AgentCardAuthentication(BaseModel):
    schemes: List[str] # e.g. ["Bearer", "OAuth2"]
    credentials: Optional[str] = None # "credentials a client should use for private cards"

class AgentCardSkill(BaseModel):
    id: str
    name: str
    description: str
    tags: Optional[List[str]] = []
    examples: Optional[List[str]] = []
    # Parameters for this skill, as a JSON schema object
    parameters: Optional[Dict[str, Any]] = Field(default_factory=dict)
    inputModes: Optional[List[str]] = None # Mime types
    outputModes: Optional[List[str]] = None # Mime types


class AgentCard(BaseModel):
    name: str
    description: str
    url: str # URL where the agent is hosted (this A2A wrapper server's base URL)
    provider: Optional[AgentCardProvider] = None
    version: str = "1.0.0"
    documentationUrl: Optional[str] = None
    capabilities: AgentCardCapabilities = Field(default_factory=AgentCardCapabilities)
    authentication: AgentCardAuthentication = Field(default_factory=lambda: AgentCardAuthentication(schemes=[]))
    defaultInputModes: List[str] = ["application/json"] # A2A server expects JSON-RPC
    defaultOutputModes: List[str] = ["application/json"] # A2A server responds with JSON-RPC
    skills: List[AgentCardSkill]

# -------------------- multi_agent_a2a_wrapper.py --------------------

# ==================== a2a_wrapper/multi_agent_a2a_wrapper.py ====================
"""
Updated multi_agent_a2a_wrapper.py to work with NCF-enabled agents
"""

from fastapi import FastAPI, HTTPException
from a2a_wrapper.models import *
from agent_manager import AgentManager  # Now NCF-enabled
import json
import logging

logger = logging.getLogger(__name__)

app = FastAPI(title="Multi-Agent Aura A2A Service (NCF-Enabled)")
agent_manager = AgentManager()

# Store active agent card registrations
registered_agents = {}


@app.post("/agents/{agent_id}/register")
async def register_agent_a2a(agent_id: str):
    """Register a specific NCF-enabled agent for A2A access"""
    agent_instance = agent_manager.get_agent_instance(agent_id)
    if not agent_instance:
        raise HTTPException(status_code=404, detail="Agent not found")

    config = agent_instance.config

    # Create agent-specific A2A card with NCF capabilities
    agent_card = AgentCard(
        name=config.name,
        description=f"{config.persona}. NCF-Enabled with advanced memory and contextual understanding. {config.detailed_persona[:200]}...",
        url=f"http://localhost:8000/agents/{agent_id}",  # Adjust base URL as needed
        version="2.0.0",
        provider=AgentCardProvider(
            organization="User Created (NCF-Enabled)",
            url="http://localhost:8000"
        ),
        capabilities=AgentCardCapabilities(streaming=False),
        authentication=AgentCardAuthentication(schemes=[]),
        skills=[
            AgentCardSkill(
                id="ncf_conversation",
                name=f"Advanced Chat with {config.name} (NCF)",
                description=f"Have an advanced conversation with {config.name}. This agent features Narrative Context Framing (NCF), isolated memory system, RAG capabilities, and reflective analysis. {config.persona}",
                tags=["ncf", "memory", "conversation", "rag", "reflector", "advanced"],
                examples=[
                    f"Let's explore a complex topic with {config.name}'s advanced understanding",
                    f"Tell {config.name} about your long-term goals for contextual memory",
                    f"Discuss philosophical concepts with {config.name}'s narrative foundation"
                ],
                parameters={
                    "type": "object",
                    "properties": {
                        "user_input": {
                            "type": "string",
                            "description": "Your message to the NCF-enabled agent"
                        },
                        "context_aware": {
                            "type": "boolean",
                            "description": "Enable advanced NCF context processing (recommended: true)",
                            "default": True
                        }
                    },
                    "required": ["user_input"]
                }
            )
        ]
    )

    registered_agents[agent_id] = agent_card
    return {
        "status": "registered",
        "agent_id": agent_id,
        "ncf_enabled": True,
        "capabilities": ["ncf", "memory", "rag", "reflector"]
    }


@app.get("/agents/{agent_id}/.well-known/agent.json")
async def get_agent_card(agent_id: str):
    """Get A2A agent card for specific NCF-enabled agent"""
    if agent_id not in registered_agents:
        raise HTTPException(status_code=404, detail="Agent not registered for A2A")
    return registered_agents[agent_id]


@app.post("/agents/{agent_id}")
async def handle_agent_a2a_rpc(agent_id: str, rpc_request: A2AJsonRpcRequest):
    """Handle A2A RPC requests for specific NCF-enabled agent"""

    if rpc_request.method != "tasks/send":
        return A2AJsonRpcResponse(
            id=rpc_request.id,
            error={"code": -32601, "message": "Method not found"}
        )

    # Get NCF-enabled agent instance
    agent_instance = agent_manager.get_agent_instance(agent_id)
    if not agent_instance:
        return A2AJsonRpcResponse(
            id=rpc_request.id,
            error={"code": -32602, "message": "NCF-enabled agent not found"}
        )

    try:
        # Extract user input
        task_params = rpc_request.params
        user_input = ""

        if task_params.message and task_params.message.parts:
            first_part = task_params.message.parts[0]
            if first_part.type == "data" and first_part.data:
                user_input = first_part.data.get("user_input", "")
            elif first_part.type == "text":
                user_input = first_part.text

        if not user_input:
            return A2AJsonRpcResponse(
                id=rpc_request.id,
                error={"code": -32602, "message": "No user input found"}
            )

        # Process with NCF-enabled agent (full NCF capabilities automatically applied)
        response = await agent_instance.process_message(
            user_id=f"a2a_{task_params.id}",
            session_id=task_params.sessionId or task_params.id,
            message=user_input
        )

        # Build A2A response with NCF metadata
        result = A2ATaskResult(
            id=task_params.id,
            sessionId=task_params.sessionId,
            status=A2ATaskStatus(state="completed"),
            artifacts=[
                A2AArtifact(
                    name="NCF Response",
                    description="Response from NCF-enabled Aura agent with contextual understanding",
                    parts=[A2APart(type="text", text=response)],
                    metadata={
                        "ncf_enabled": True,
                        "agent_name": agent_instance.config.name,
                        "capabilities": ["narrative_foundation", "rag", "reflector", "memory"]
                    }
                )
            ],
            metadata={
                "ncf_processed": True,
                "agent_capabilities": ["ncf", "memory", "rag", "reflector"]
            }
        )

        return A2AJsonRpcResponse(id=rpc_request.id, result=result)

    except Exception as e:
        logger.error(f"Error processing A2A request for NCF agent: {e}")
        return A2AJsonRpcResponse(
            id=rpc_request.id,
            error={"code": -32000, "message": f"NCF agent processing error: {str(e)}"}
        )


# Integration with AIRA Hub for NCF agents
@app.post("/register-to-hub/{agent_id}")
async def register_to_aira_hub(agent_id: str, hub_url: str):
    """Register a specific NCF-enabled agent to AIRA Hub"""
    agent_instance = agent_manager.get_agent_instance(agent_id)
    if not agent_instance:
        raise HTTPException(status_code=404, detail="Agent not found")

    config = agent_instance.config

    # Prepare registration data for AIRA Hub with NCF capabilities
    registration_data = {
        "url": f"http://localhost:8000/agents/{agent_id}",  # Adjust base URL as needed
        "name": f"{config.name}_NCF_A2A",
        "description": f"NCF-Enabled {config.persona}. Advanced contextual AI with memory, narrative foundation, and reflective capabilities.",
        "version": "2.0.0",
        "mcp_tools": [
            {
                "name": f"TalkTo_{config.name.replace(' ', '_')}_NCF",
                "description": f"Advanced chat with {config.name} (NCF-enabled). Features narrative foundation, RAG, memory system, and reflective analysis. {config.persona}",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "user_input": {
                            "type": "string",
                            "description": "Your message to the NCF-enabled agent"
                        },
                        "enable_ncf": {
                            "type": "boolean",
                            "description": "Enable full NCF processing (recommended: true)",
                            "default": True
                        }
                    },
                    "required": ["user_input"]
                },
                "annotations": {
                    "aira_bridge_type": "a2a",
                    "aira_a2a_target_skill_id": "ncf_conversation",
                    "aira_a2a_agent_url": f"http://localhost:8000/agents/{agent_id}",
                    "ncf_enabled": True,
                    "capabilities": ["narrative_foundation", "rag", "reflector", "memory", "contextual_understanding"]
                }
            }
        ],
        "a2a_skills": [],
        "aira_capabilities": ["a2a", "ncf", "memory", "contextual_ai"],
        "status": "online",
        "tags": ["user-created", "aura", "a2a", "ncf", "memory", "advanced-ai"],
        "category": "NCF_UserAgents",
        "ncf_metadata": {
            "narrative_foundation": True,
            "rag_enabled": True,
            "reflector_analysis": True,
            "isolated_memory": True,
            "model": config.model
        }
    }

    # Make registration request to AIRA Hub
    import httpx
    async with httpx.AsyncClient() as client:
        response = await client.post(f"{hub_url}/register", json=registration_data)
        if response.status_code == 200:
            return {
                "status": "registered",
                "hub_response": response.json(),
                "ncf_enabled": True,
                "capabilities": ["ncf", "memory", "rag", "reflector"]
            }
        else:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Hub registration failed: {response.text}"
            )

# -------------------- routes.py --------------------

# ==================== api/routes.py
"""
Enhanced API routes with memory management and agent editing capabilities
"""

from fastapi import FastAPI, HTTPException, Depends, status, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from pydantic import BaseModel, EmailStr
from typing import List, Optional, Dict, Any, Union
import jwt
from datetime import datetime, timedelta
import bcrypt
import os
from pathlib import Path
import sys
import logging
import json
import io
import zipfile

# Import from parent directory
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from agent_manager import AgentManager  # Now using NCF-enabled AgentManager
from database.models import AgentRepository, User

# Setup logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# Configuration
JWT_SECRET = os.getenv("JWT_SECRET_KEY", "your-secret-key-change-this-in-production")
JWT_ALGORITHM = "HS256"
JWT_EXPIRATION_HOURS = 24

# Initialize FastAPI app
app = FastAPI(
    title="Aura Multi-Agent API (NCF-Enabled)",
    description="API for creating and managing multiple NCF-powered Aura AI agents with advanced memory and context capabilities",
    version="2.0.0"
)

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify your frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- CORRECTED INITIALIZATION WITH ABSOLUTE PATHS ---
# Determine project root dynamically.
# If routes.py is in 'api/', and 'agent_storage' & 'aura_agents.db' are at the project root level:
PROJECT_ROOT = Path(__file__).resolve().parent.parent
AGENT_STORAGE_PATH = PROJECT_ROOT / "agent_storage"
DATABASE_FILE_PATH = PROJECT_ROOT / "aura_agents.db"

# Ensure the agent storage directory exists
AGENT_STORAGE_PATH.mkdir(parents=True, exist_ok=True)
logger.info(f"Using AGENT_STORAGE_PATH: {AGENT_STORAGE_PATH.resolve()}")
logger.info(f"Using DATABASE_FILE_PATH: {DATABASE_FILE_PATH.resolve()}")

# Initialize services with NCF-enabled AgentManager using absolute paths
agent_manager = AgentManager(base_storage_path=str(AGENT_STORAGE_PATH))  # Convert Path object to string
db_repo = AgentRepository(db_url=f"sqlite:///{str(DATABASE_FILE_PATH)}")  # Convert Path object to string

# Security
security = HTTPBearer()


# --- Pydantic Models ---
class UserRegisterRequest(BaseModel):
    email: EmailStr
    username: str
    password: str


class UserLoginRequest(BaseModel):
    username: str
    password: str


class CreateAgentRequest(BaseModel):
    name: str
    persona: str
    detailed_persona: str
    model: Optional[str] = None
    is_public: Optional[bool] = False


class UpdateAgentRequest(BaseModel):
    name: Optional[str] = None
    persona: Optional[str] = None
    detailed_persona: Optional[str] = None
    avatar_url: Optional[str] = None
    is_public: Optional[bool] = None
    settings: Optional[dict] = None


class EnhancedUpdateAgentRequest(BaseModel):
    """Enhanced agent update request with all editable fields"""
    name: Optional[str] = None
    persona: Optional[str] = None  # Short description
    detailed_persona: Optional[str] = None  # Detailed personality
    avatar_url: Optional[str] = None
    is_public: Optional[bool] = None
    settings: Optional[dict] = None
    model: Optional[str] = None


class ChatRequest(BaseModel):
    message: str
    session_id: Optional[str] = None


class AgentResponse(BaseModel):
    agent_id: str
    name: str
    persona: str
    detailed_persona: str
    created_at: str
    is_public: bool
    owner_username: Optional[str] = None
    capabilities: List[str] = ["ncf", "memory", "narrative_foundation", "rag", "reflector"]


class ChatResponse(BaseModel):
    response: str
    session_id: str
    ncf_enabled: bool = True


class MemorySearchRequest(BaseModel):
    query: str
    memory_types: Optional[List[str]] = None
    limit: Optional[int] = 10


class MemoryUploadRequest(BaseModel):
    """Request model for uploading memories"""
    memories: List[Dict[str, Any]]
    overwrite_existing: Optional[bool] = False
    validate_format: Optional[bool] = True


class MemoryExportResponse(BaseModel):
    """Response model for memory export"""
    agent_id: str
    agent_name: str
    export_timestamp: str
    total_memories: int
    memory_types: List[str]
    memories: List[Dict[str, Any]]


class BulkMemoryUploadResponse(BaseModel):
    """Response model for bulk memory upload"""
    agent_id: str
    agent_name: str
    upload_timestamp: str
    total_uploaded: int
    successful_uploads: int
    failed_uploads: int
    errors: List[str]
    memory_types_added: List[str]


# --- Authentication Helpers ---
def create_access_token(user_id: str, username: str) -> str:
    """Create JWT access token"""
    expire = datetime.utcnow() + timedelta(hours=JWT_EXPIRATION_HOURS)
    to_encode = {
        "user_id": user_id,
        "username": username,
        "exp": expire
    }
    return jwt.encode(to_encode, JWT_SECRET, algorithm=JWT_ALGORITHM)


async def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)) -> dict:
    """Verify JWT token and return user info"""
    token = credentials.credentials
    try:
        payload = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALGORITHM])
        return {
            "user_id": payload["user_id"],
            "username": payload["username"]
        }
    except jwt.ExpiredSignatureError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Token has expired"
        )
    except jwt.JWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Invalid token"
        )


# --- User Authentication Routes ---
@app.post("/auth/register", status_code=status.HTTP_201_CREATED)
async def register_user(request: UserRegisterRequest):
    """Register a new user"""
    # Check if user exists
    with db_repo.SessionLocal() as session:
        existing_user = session.query(User).filter(
            (User.email == request.email) | (User.username == request.username)
        ).first()

        if existing_user:
            if existing_user.email == request.email:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Email already registered"
                )
            else:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail="Username already taken"
                )

        # Hash password
        password_hash = bcrypt.hashpw(request.password.encode('utf-8'), bcrypt.gensalt()).decode('utf-8')

        # Create user
        new_user = User(
            email=request.email,
            username=request.username,
            password_hash=password_hash
        )

        session.add(new_user)
        session.commit()
        session.refresh(new_user)

        # Create access token
        access_token = create_access_token(new_user.id, new_user.username)

        return {
            "user_id": new_user.id,
            "username": new_user.username,
            "email": new_user.email,
            "access_token": access_token,
            "token_type": "bearer",
            "message": "Welcome! You can now create NCF-powered Aura agents with advanced memory and contextual understanding."
        }


@app.post("/auth/login")
async def login_user(request: UserLoginRequest):
    """Login user and return access token"""
    with db_repo.SessionLocal() as session:
        user = session.query(User).filter(User.username == request.username).first()

        if not user:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid username or password"
            )

        # Verify password
        if not bcrypt.checkpw(request.password.encode('utf-8'), user.password_hash.encode('utf-8')):
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="Invalid username or password"
            )

        # Create access token
        access_token = create_access_token(user.id, user.username)

        return {
            "user_id": user.id,
            "username": user.username,
            "email": user.email,
            "access_token": access_token,
            "token_type": "bearer"
        }


@app.get("/auth/me")
async def get_current_user(current_user: dict = Depends(verify_token)):
    """Get current user information"""
    with db_repo.SessionLocal() as session:
        user = session.query(User).filter(User.id == current_user["user_id"]).first()
        if not user:
            raise HTTPException(status_code=404, detail="User not found")

        return {
            "user_id": user.id,
            "username": user.username,
            "email": user.email,
            "created_at": user.created_at.isoformat(),
            "agent_capabilities": ["ncf", "memory", "narrative_foundation", "rag", "reflector"]
        }


# --- Agent Management Routes ---
@app.post("/agents/create", response_model=dict)
async def create_agent(request: CreateAgentRequest, current_user: dict = Depends(verify_token)):
    """Create a new NCF-enabled Aura agent with advanced capabilities"""
    try:
        # Create NCF-enabled agent in file system
        agent_id = agent_manager.create_agent(
            user_id=current_user["user_id"],
            name=request.name,
            persona=request.persona,
            detailed_persona=request.detailed_persona,
            model=request.model
        )

        # Also save to database
        db_agent = db_repo.create_agent(
            agent_id=agent_id,
            user_id=current_user["user_id"],
            name=request.name,
            persona=request.persona,
            detailed_persona=request.detailed_persona,
            model=request.model,
            is_public=request.is_public
        )

        return {
            "agent_id": agent_id,
            "message": f"NCF-enabled agent '{request.name}' created successfully with advanced memory and contextual understanding",
            "capabilities": ["ncf", "memory", "narrative_foundation", "rag", "reflector", "isolated_memory_system"],
            "features": [
                "Narrative Context Framing for deep conversations",
                "Isolated MemoryBlossom for personalized memory",
                "RAG (Retrieval-Augmented Generation) capabilities",
                "Automatic reflection and memory formation",
                "Context-aware responses with conversation history"
            ]
        }
    except Exception as e:
        logger.error(f"Error creating NCF agent: {e}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create NCF-enabled agent: {str(e)}"
        )


@app.get("/agents/list", response_model=List[AgentResponse])
async def list_agents(current_user: dict = Depends(verify_token)):
    """List all NCF-enabled agents for the authenticated user"""
    configs = agent_manager.list_user_agents(current_user["user_id"])

    return [
        AgentResponse(
            agent_id=config.agent_id,
            name=config.name,
            persona=config.persona,
            detailed_persona=config.detailed_persona,
            created_at=config.created_at.isoformat() if config.created_at else datetime.utcnow().isoformat(),
            is_public=False,
            owner_username=current_user["username"],
            capabilities=["ncf", "memory", "narrative_foundation", "rag", "reflector"]
        )
        for config in configs
    ]


@app.get("/agents/public", response_model=List[AgentResponse])
async def list_public_agents():
    """List all public NCF-enabled agents"""
    with db_repo.SessionLocal() as session:
        from sqlalchemy.orm import joinedload
        public_agents = session.query(db_repo.Agent).filter(
            db_repo.Agent.is_public == 1
        ).options(joinedload(db_repo.Agent.user)).all()

        return [
            AgentResponse(
                agent_id=agent.id,
                name=agent.name,
                persona=agent.persona,
                detailed_persona=agent.detailed_persona,
                created_at=agent.created_at.isoformat(),
                is_public=True,
                owner_username=agent.user.username if agent.user else None,
                capabilities=["ncf", "memory", "narrative_foundation", "rag", "reflector"]
            )
            for agent in public_agents
        ]


@app.get("/agents/{agent_id}", response_model=AgentResponse)
async def get_agent(agent_id: str, current_user: dict = Depends(verify_token)):
    """Get details of a specific NCF-enabled agent"""
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Check if user owns the agent or it's public
    config = agent.config
    if config.user_id != current_user["user_id"]:
        # Check if agent is public in database
        db_agent = db_repo.get_agent(agent_id)
        if not db_agent or not db_agent.is_public:
            raise HTTPException(status_code=403, detail="Access denied")

    return AgentResponse(
        agent_id=config.agent_id,
        name=config.name,
        persona=config.persona,
        detailed_persona=config.detailed_persona,
        created_at=config.created_at.isoformat() if config.created_at else datetime.utcnow().isoformat(),
        is_public=db_agent.is_public if db_agent else False,
        owner_username=current_user["username"],
        capabilities=["ncf", "memory", "narrative_foundation", "rag", "reflector"]
    )


@app.put("/agents/{agent_id}", response_model=dict)
async def update_agent(agent_id: str, request: UpdateAgentRequest, current_user: dict = Depends(verify_token)):
    """Update an existing NCF-enabled agent"""
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    # Update agent configuration
    config = agent.config
    if request.name is not None:
        config.name = request.name
    if request.persona is not None:
        config.persona = request.persona
    if request.detailed_persona is not None:
        config.detailed_persona = request.detailed_persona
    if request.settings is not None:
        config.settings.update(request.settings)

    # Save updated configuration
    agent_manager._save_agent_config(config)

    # Update in database if needed
    with db_repo.SessionLocal() as session:
        db_agent = session.query(db_repo.Agent).filter(db_repo.Agent.id == agent_id).first()
        if db_agent:
            if request.name is not None:
                db_agent.name = request.name
            if request.persona is not None:
                db_agent.persona = request.persona
            if request.detailed_persona is not None:
                db_agent.detailed_persona = request.detailed_persona
            if request.avatar_url is not None:
                db_agent.avatar_url = request.avatar_url
            if request.is_public is not None:
                db_agent.is_public = 1 if request.is_public else 0
            if request.settings is not None:
                db_agent.settings = request.settings

            session.commit()

    return {"message": "NCF-enabled agent updated successfully"}


@app.delete("/agents/{agent_id}")
async def delete_agent(agent_id: str, current_user: dict = Depends(verify_token)):
    """Delete an NCF-enabled agent"""
    success = agent_manager.delete_agent(agent_id, current_user["user_id"])
    if not success:
        raise HTTPException(status_code=404, detail="Agent not found or unauthorized")

    # Also delete from database
    db_repo.delete_agent(agent_id, current_user["user_id"])

    return {"message": "NCF-enabled agent and its isolated memory system deleted successfully"}


# --- Enhanced Agent Profile Management ---
@app.put("/agents/{agent_id}/profile", response_model=dict)
async def update_agent_profile(
        agent_id: str,
        request: EnhancedUpdateAgentRequest,
        current_user: dict = Depends(verify_token)
):
    """
    Enhanced agent profile update with comprehensive editing capabilities
    Allows editing: name, persona (short description), detailed_persona, model, and settings
    """
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    try:
        # Track what was updated
        updated_fields = []

        # Update agent configuration
        config = agent.config

        if request.name is not None:
            old_name = config.name
            config.name = request.name
            updated_fields.append(f"name: '{old_name}' → '{request.name}'")

        if request.persona is not None:
            old_persona = config.persona
            config.persona = request.persona
            updated_fields.append(f"persona: '{old_persona[:50]}...' → '{request.persona[:50]}...'")

        if request.detailed_persona is not None:
            old_detailed = config.detailed_persona
            config.detailed_persona = request.detailed_persona
            updated_fields.append(f"detailed_persona: '{old_detailed[:50]}...' → '{request.detailed_persona[:50]}...'")

        if request.model is not None:
            old_model = config.model
            config.model = request.model
            updated_fields.append(f"model: '{old_model}' → '{request.model}'")

        if request.settings is not None:
            config.settings.update(request.settings)
            updated_fields.append(f"settings: {list(request.settings.keys())}")

        # Save updated configuration to file system
        agent_manager._save_agent_config(config)

        # Update in database if it exists
        try:
            with db_repo.SessionLocal() as session:
                db_agent = session.query(db_repo.Agent).filter(db_repo.Agent.id == agent_id).first()
                if db_agent:
                    if request.name is not None:
                        db_agent.name = request.name
                    if request.persona is not None:
                        db_agent.persona = request.persona
                    if request.detailed_persona is not None:
                        db_agent.detailed_persona = request.detailed_persona
                    if request.avatar_url is not None:
                        db_agent.avatar_url = request.avatar_url
                    if request.is_public is not None:
                        db_agent.is_public = 1 if request.is_public else 0
                    if request.settings is not None:
                        db_agent.settings = request.settings

                    session.commit()
                    updated_fields.append("database record")
        except Exception as db_error:
            logger.warning(f"Database update failed: {db_error}")
            # Continue execution - file system update is more critical

        # Add memory about the profile update
        try:
            update_memory = {
                "content": f"Agent profile updated. Changed: {', '.join(updated_fields)}",
                "memory_type": "system_update",
                "emotion_score": 0.0,
                "initial_salience": 0.3,
                "custom_metadata": {
                    "source": "profile_update",
                    "updated_by": current_user["user_id"],
                    "timestamp": datetime.now().isoformat(),
                    "updated_fields": updated_fields
                }
            }

            agent.memory_blossom.add_memory(**update_memory)
            agent.memory_blossom.save_memories()
        except Exception as memory_error:
            logger.warning(f"Failed to add update memory: {memory_error}")

        return {
            "message": "Agent profile updated successfully",
            "agent_id": agent_id,
            "agent_name": config.name,
            "updated_fields": updated_fields,
            "timestamp": datetime.now().isoformat(),
            "ncf_enabled": True,
            "capabilities": ["ncf", "memory", "narrative_foundation", "rag", "reflector"]
        }

    except Exception as e:
        logger.error(f"Error updating agent profile {agent_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Profile update failed: {str(e)}")


@app.get("/agents/{agent_id}/profile", response_model=dict)
async def get_agent_profile(
        agent_id: str,
        current_user: dict = Depends(verify_token)
):
    """
    Get comprehensive agent profile information
    """
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Check access permissions
    config = agent.config
    if config.user_id != current_user["user_id"]:
        # Check if agent is public
        try:
            db_agent = db_repo.get_agent(agent_id)
            if not db_agent or not db_agent.is_public:
                raise HTTPException(status_code=403, detail="Access denied")
        except:
            raise HTTPException(status_code=403, detail="Access denied")

    # Get memory statistics
    memory_stats = {
        "total_memories": sum(len(m) for m in agent.memory_blossom.memory_stores.values()),
        "memory_types": list(agent.memory_blossom.memory_stores.keys()),
        "memory_breakdown": {
            mem_type: len(mem_list)
            for mem_type, mem_list in agent.memory_blossom.memory_stores.items()
        }
    }

    return {
        "agent_id": config.agent_id,
        "name": config.name,
        "persona": config.persona,
        "detailed_persona": config.detailed_persona,
        "model": config.model,
        "created_at": config.created_at.isoformat() if config.created_at else None,
        "settings": config.settings or {},
        "memory_stats": memory_stats,
        "ncf_enabled": True,
        "capabilities": ["ncf", "memory", "narrative_foundation", "rag", "reflector"],
        "owner_id": config.user_id,
        "is_owner": config.user_id == current_user["user_id"]
    }


@app.get("/agents/{agent_id}/adaptive-stats")
async def get_agent_adaptive_stats(agent_id: str, current_user: dict = Depends(verify_token)):
    """Get adaptive RAG statistics for an agent"""
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    try:
        adaptive_stats = agent.memory_blossom.get_adaptive_stats()
        return {
            "agent_id": agent_id,
            "agent_name": agent.config.name,
            "adaptive_stats": adaptive_stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting adaptive stats: {e}")
        raise HTTPException(status_code=500, detail="Failed to get adaptive stats")


@app.post("/agents/{agent_id}/upgrade-adaptive-rag")
async def upgrade_agent_adaptive_rag(agent_id: str, current_user: dict = Depends(verify_token)):
    """Upgrade an existing agent to use Enhanced MemoryBlossom"""
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    try:
        from enhanced_memory_system import upgrade_agent_to_adaptive_rag
        success = upgrade_agent_to_adaptive_rag(agent, enable_adaptive_rag=True)

        if success:
            return {
                "message": f"Agent '{agent.config.name}' successfully upgraded to Enhanced MemoryBlossom",
                "adaptive_rag_enabled": True,
                "features_added": [
                    "Domain-aware clustering",
                    "Performance-weighted retrieval",
                    "Adaptive concept formation",
                    "Multi-layer memory system"
                ]
            }
        else:
            raise HTTPException(status_code=500, detail="Upgrade failed")

    except Exception as e:
        logger.error(f"Error upgrading agent: {e}")
        raise HTTPException(status_code=500, detail=f"Upgrade failed: {str(e)}")



# --- Chat Routes ---
# --- Chat Routes ---
@app.post("/agents/{agent_id}/chat", response_model=ChatResponse)
async def chat_with_agent(agent_id: str, request: ChatRequest, current_user: dict = Depends(verify_token)):
    """Chat with a specific NCF-enabled agent"""
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Check if user owns the agent or it's public
    if agent.config.user_id != current_user["user_id"]:
        db_agent = db_repo.get_agent(agent_id)
        if not db_agent or not db_agent.is_public:
            raise HTTPException(status_code=403, detail="Access denied")

    # Generate session ID if not provided
    session_id = request.session_id or f"session_{current_user['user_id']}_{datetime.utcnow().timestamp()}"

    try:
        # FIXED: Properly await the async process_message method
        response = await agent.process_message(
            user_id=current_user["user_id"],
            session_id=session_id,
            message=request.message
        )

        return ChatResponse(
            response=response,
            session_id=session_id,
            ncf_enabled=True
        )
    except Exception as e:
        logger.error(f"Error in NCF chat: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"NCF chat error: {str(e)}"
        )

# --- Memory Management Routes ---
@app.get("/agents/{agent_id}/memories")
async def get_agent_memories(
        agent_id: str,
        memory_type: Optional[str] = None,
        limit: int = 50,
        current_user: dict = Depends(verify_token)
):
    """Get memories for a specific NCF-enabled agent"""
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    # Get memories from the agent's isolated memory blossom
    memories = []
    if memory_type:
        if memory_type in agent.memory_blossom.memory_stores:
            memories = agent.memory_blossom.memory_stores[memory_type][:limit]
    else:
        # Get all memories
        for mem_type, mem_list in agent.memory_blossom.memory_stores.items():
            memories.extend(mem_list[:limit // len(agent.memory_blossom.memory_stores)])

    return {
        "agent_id": agent_id,
        "agent_name": agent.config.name,
        "total_memories": sum(len(m) for m in agent.memory_blossom.memory_stores.values()),
        "memory_types": list(agent.memory_blossom.memory_stores.keys()),
        "isolated_memory_system": True,
        "ncf_enabled": True,
        "memories": [mem.to_dict() for mem in memories[:limit]]
    }


@app.post("/agents/{agent_id}/memories/search")
async def search_agent_memories(
        agent_id: str,
        request: MemorySearchRequest,
        current_user: dict = Depends(verify_token)
):
    """Search memories for a specific NCF-enabled agent"""
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    # Search memories using the agent's isolated memory system
    results = agent.memory_blossom.retrieve_memories(
        query=request.query,
        target_memory_types=request.memory_types,
        top_k=request.limit or 10
    )

    return {
        "agent_id": agent_id,
        "agent_name": agent.config.name,
        "query": request.query,
        "isolated_memory_system": True,
        "ncf_enabled": True,
        "results": [mem.to_dict() for mem in results]
    }


@app.delete("/agents/{agent_id}/memories/{memory_id}")
async def delete_memory(
        agent_id: str,
        memory_id: str,
        current_user: dict = Depends(verify_token)
):
    """Delete a specific memory from an NCF-enabled agent"""
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    # Find and remove the memory from the agent's isolated memory system
    memory_found = False
    for mem_type, mem_list in agent.memory_blossom.memory_stores.items():
        for i, mem in enumerate(mem_list):
            if mem.id == memory_id:
                mem_list.pop(i)
                memory_found = True
                agent.memory_blossom.save_memories()
                break
        if memory_found:
            break

    if not memory_found:
        raise HTTPException(status_code=404, detail="Memory not found")

    return {"message": "Memory deleted successfully from NCF-enabled agent"}


# --- Enhanced Memory Management Routes ---
@app.get("/agents/{agent_id}/memories/export", response_model=MemoryExportResponse)
async def export_agent_memories(
        agent_id: str,
        format: Optional[str] = "json",  # json, csv, or zip
        memory_types: Optional[str] = None,  # comma-separated list
        current_user: dict = Depends(verify_token)
):
    """
    Export all memories for a specific agent
    Supports JSON, CSV, and ZIP formats
    """
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    try:
        # Get all memories from the agent's isolated memory system
        all_memories = []
        memory_types_filter = memory_types.split(",") if memory_types else None

        for mem_type, mem_list in agent.memory_blossom.memory_stores.items():
            if memory_types_filter and mem_type not in memory_types_filter:
                continue

            for memory in mem_list:
                memory_dict = memory.to_dict()
                memory_dict['memory_type'] = mem_type  # Ensure type is included
                all_memories.append(memory_dict)

        export_data = {
            "agent_id": agent_id,
            "agent_name": agent.config.name,
            "export_timestamp": datetime.now().isoformat(),
            "total_memories": len(all_memories),
            "memory_types": list(agent.memory_blossom.memory_stores.keys()),
            "memories": all_memories
        }

        if format.lower() == "json":
            return MemoryExportResponse(**export_data)

        elif format.lower() == "csv":
            # Convert to CSV format
            try:
                import pandas as pd
            except ImportError:
                raise HTTPException(status_code=500, detail="pandas not installed. Please install: pip install pandas")

            if not all_memories:
                raise HTTPException(status_code=404, detail="No memories found to export")

            # Flatten memories for CSV
            flattened_memories = []
            for memory in all_memories:
                flat_memory = {
                    'id': memory.get('id'),
                    'content': memory.get('content'),
                    'memory_type': memory.get('memory_type'),
                    'emotion_score': memory.get('emotion_score', 0.0),
                    'coherence_score': memory.get('coherence_score', 0.5),
                    'novelty_score': memory.get('novelty_score', 0.5),
                    'salience': memory.get('salience', 0.5),
                    'created_at': memory.get('created_at'),
                    'accessed_count': memory.get('accessed_count', 0),
                    'last_accessed': memory.get('last_accessed'),
                    'custom_metadata': json.dumps(memory.get('custom_metadata', {}))
                }
                flattened_memories.append(flat_memory)

            df = pd.DataFrame(flattened_memories)
            csv_buffer = io.StringIO()
            df.to_csv(csv_buffer, index=False)
            csv_content = csv_buffer.getvalue()

            return StreamingResponse(
                io.StringIO(csv_content),
                media_type="text/csv",
                headers={"Content-Disposition": f"attachment; filename={agent.config.name}_memories.csv"}
            )

        elif format.lower() == "zip":
            # Create ZIP file with JSON + CSV
            zip_buffer = io.BytesIO()

            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                # Add JSON file
                json_content = json.dumps(export_data, indent=2, ensure_ascii=False)
                zip_file.writestr(f"{agent.config.name}_memories.json", json_content)

                # Add CSV file if memories exist
                if all_memories:
                    try:
                        import pandas as pd
                        flattened_memories = []
                        for memory in all_memories:
                            flat_memory = {
                                'id': memory.get('id'),
                                'content': memory.get('content'),
                                'memory_type': memory.get('memory_type'),
                                'emotion_score': memory.get('emotion_score', 0.0),
                                'coherence_score': memory.get('coherence_score', 0.5),
                                'novelty_score': memory.get('novelty_score', 0.5),
                                'salience': memory.get('salience', 0.5),
                                'created_at': memory.get('created_at'),
                                'accessed_count': memory.get('accessed_count', 0),
                                'last_accessed': memory.get('last_accessed'),
                                'custom_metadata': json.dumps(memory.get('custom_metadata', {}))
                            }
                            flattened_memories.append(flat_memory)

                        df = pd.DataFrame(flattened_memories)
                        csv_content = df.to_csv(index=False)
                        zip_file.writestr(f"{agent.config.name}_memories.csv", csv_content)
                    except ImportError:
                        logger.warning("pandas not available for CSV export in ZIP")

                # Add metadata file
                metadata = {
                    "agent_id": agent_id,
                    "agent_name": agent.config.name,
                    "persona": agent.config.persona,
                    "detailed_persona": agent.config.detailed_persona,
                    "export_timestamp": datetime.now().isoformat(),
                    "total_memories": len(all_memories),
                    "memory_types": list(agent.memory_blossom.memory_stores.keys())
                }
                zip_file.writestr("metadata.json", json.dumps(metadata, indent=2))

            zip_buffer.seek(0)

            return StreamingResponse(
                io.BytesIO(zip_buffer.read()),
                media_type="application/zip",
                headers={"Content-Disposition": f"attachment; filename={agent.config.name}_memories.zip"}
            )

        else:
            raise HTTPException(status_code=400, detail="Unsupported format. Use 'json', 'csv', or 'zip'")

    except Exception as e:
        logger.error(f"Error exporting memories for agent {agent_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Export failed: {str(e)}")


@app.post("/agents/{agent_id}/memories/upload", response_model=BulkMemoryUploadResponse)
async def upload_agent_memories(
        agent_id: str,
        request: MemoryUploadRequest,
        current_user: dict = Depends(verify_token)
):
    """
    Upload new memories to a specific agent
    Supports bulk upload with validation and error handling
    """
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    try:
        successful_uploads = 0
        failed_uploads = 0
        errors = []
        memory_types_added = set()

        for i, memory_data in enumerate(request.memories):
            try:
                # Validate required fields
                if request.validate_format:
                    required_fields = ['content', 'memory_type']
                    missing_fields = [field for field in required_fields if field not in memory_data]
                    if missing_fields:
                        error_msg = f"Memory {i}: Missing required fields: {missing_fields}"
                        errors.append(error_msg)
                        failed_uploads += 1
                        continue

                # Set default values for optional fields
                memory_content = memory_data['content']
                memory_type = memory_data['memory_type']
                emotion_score = float(memory_data.get('emotion_score', 0.0))
                initial_salience = float(memory_data.get('initial_salience', 0.5))
                custom_metadata = memory_data.get('custom_metadata', {})

                # Add upload metadata
                custom_metadata.update({
                    "source": "user_upload",
                    "uploaded_by": current_user["user_id"],
                    "upload_timestamp": datetime.now().isoformat()
                })

                # Check for existing memory if not overwriting
                if not request.overwrite_existing:
                    existing_memories = agent.memory_blossom.memory_stores.get(memory_type, [])
                    content_exists = any(mem.content == memory_content for mem in existing_memories)
                    if content_exists:
                        error_msg = f"Memory {i}: Content already exists (use overwrite_existing=true to force)"
                        errors.append(error_msg)
                        failed_uploads += 1
                        continue

                # Add memory to agent's isolated memory system
                agent.memory_blossom.add_memory(
                    content=memory_content,
                    memory_type=memory_type,
                    emotion_score=emotion_score,
                    initial_salience=initial_salience,
                    custom_metadata=custom_metadata
                )

                memory_types_added.add(memory_type)
                successful_uploads += 1

            except Exception as e:
                error_msg = f"Memory {i}: Failed to upload - {str(e)}"
                errors.append(error_msg)
                failed_uploads += 1
                logger.error(f"Error uploading memory {i}: {e}")

        # Save all memories to disk
        if successful_uploads > 0:
            agent.memory_blossom.save_memories()

        return BulkMemoryUploadResponse(
            agent_id=agent_id,
            agent_name=agent.config.name,
            upload_timestamp=datetime.now().isoformat(),
            total_uploaded=len(request.memories),
            successful_uploads=successful_uploads,
            failed_uploads=failed_uploads,
            errors=errors,
            memory_types_added=list(memory_types_added)
        )

    except Exception as e:
        logger.error(f"Error uploading memories for agent {agent_id}: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")


@app.post("/agents/{agent_id}/memories/upload/file")
async def upload_memories_from_file(
        agent_id: str,
        file: UploadFile = File(...),
        overwrite_existing: bool = False,
        current_user: dict = Depends(verify_token)
):
    """
    Upload memories from a JSON or CSV file
    """
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    try:
        # Read file content
        content = await file.read()

        if file.filename.endswith('.json'):
            # Parse JSON file
            try:
                data = json.loads(content.decode('utf-8'))

                # Handle different JSON structures
                if isinstance(data, dict) and 'memories' in data:
                    memories = data['memories']  # Export format
                elif isinstance(data, list):
                    memories = data  # Direct list format
                else:
                    raise HTTPException(status_code=400, detail="Invalid JSON format")

            except json.JSONDecodeError as e:
                raise HTTPException(status_code=400, detail=f"Invalid JSON: {str(e)}")

        elif file.filename.endswith('.csv'):
            # Parse CSV file
            try:
                import pandas as pd
            except ImportError:
                raise HTTPException(status_code=500, detail="pandas not installed. Please install: pip install pandas")

            try:
                df = pd.read_csv(io.StringIO(content.decode('utf-8')))
                memories = []

                for _, row in df.iterrows():
                    memory = {
                        'content': row.get('content'),
                        'memory_type': row.get('memory_type'),
                        'emotion_score': float(row.get('emotion_score', 0.0)),
                        'initial_salience': float(row.get('salience', 0.5)),
                    }

                    # Parse custom_metadata if it exists
                    if 'custom_metadata' in row and pd.notna(row['custom_metadata']):
                        try:
                            memory['custom_metadata'] = json.loads(row['custom_metadata'])
                        except:
                            memory['custom_metadata'] = {}

                    memories.append(memory)

            except Exception as e:
                raise HTTPException(status_code=400, detail=f"Invalid CSV: {str(e)}")

        else:
            raise HTTPException(status_code=400, detail="Unsupported file format. Use .json or .csv")

        # Create upload request
        upload_request = MemoryUploadRequest(
            memories=memories,
            overwrite_existing=overwrite_existing,
            validate_format=True
        )

        # Use existing upload logic
        return await upload_agent_memories(agent_id, upload_request, current_user)

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing file upload: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"File processing failed: {str(e)}")


# --- Memory Analytics ---
@app.get("/agents/{agent_id}/memories/analytics")
async def get_memory_analytics(
        agent_id: str,
        current_user: dict = Depends(verify_token)
):
    """
    Get detailed analytics about agent's memory system
    """
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    try:
        analytics = {
            "agent_id": agent_id,
            "agent_name": agent.config.name,
            "analysis_timestamp": datetime.now().isoformat(),
            "total_memories": 0,
            "memory_types": {},
            "emotion_distribution": {"positive": 0, "neutral": 0, "negative": 0},
            "salience_distribution": {"high": 0, "medium": 0, "low": 0},
            "recent_activity": {"last_7_days": 0, "last_30_days": 0},
            "top_memory_sources": {},
            "memory_timeline": []
        }

        all_memories = []

        # Collect all memories
        for mem_type, mem_list in agent.memory_blossom.memory_stores.items():
            analytics["memory_types"][mem_type] = len(mem_list)
            all_memories.extend([(mem, mem_type) for mem in mem_list])

        analytics["total_memories"] = len(all_memories)

        # Analyze memories
        for memory, mem_type in all_memories:
            memory_dict = memory.to_dict()

            # Emotion analysis
            emotion_score = memory_dict.get('emotion_score', 0.0)
            if emotion_score > 0.1:
                analytics["emotion_distribution"]["positive"] += 1
            elif emotion_score < -0.1:
                analytics["emotion_distribution"]["negative"] += 1
            else:
                analytics["emotion_distribution"]["neutral"] += 1

            # Salience analysis
            salience = memory_dict.get('salience', 0.5)
            if salience > 0.7:
                analytics["salience_distribution"]["high"] += 1
            elif salience > 0.3:
                analytics["salience_distribution"]["medium"] += 1
            else:
                analytics["salience_distribution"]["low"] += 1

            # Source analysis
            metadata = memory_dict.get('custom_metadata', {})
            source = metadata.get('source', 'unknown')
            analytics["top_memory_sources"][source] = analytics["top_memory_sources"].get(source, 0) + 1

            # Timeline analysis (if created_at exists)
            created_at = memory_dict.get('created_at')
            if created_at:
                try:
                    if isinstance(created_at, str):
                        created_date = datetime.fromisoformat(created_at.replace('Z', '+00:00'))
                    else:
                        created_date = created_at

                    days_ago = (datetime.now() - created_date.replace(tzinfo=None)).days

                    if days_ago <= 7:
                        analytics["recent_activity"]["last_7_days"] += 1
                    if days_ago <= 30:
                        analytics["recent_activity"]["last_30_days"] += 1

                except:
                    pass

        return analytics

    except Exception as e:
        logger.error(f"Error generating memory analytics: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Analytics generation failed: {str(e)}")


# --- Helper Endpoints ---
@app.get("/agents/{agent_id}/memory-types")
async def get_agent_memory_types(
        agent_id: str,
        current_user: dict = Depends(verify_token)
):
    """
    Get list of memory types available for this agent
    """
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Verify ownership
    if agent.config.user_id != current_user["user_id"]:
        raise HTTPException(status_code=403, detail="Access denied")

    memory_types = {}
    for mem_type, mem_list in agent.memory_blossom.memory_stores.items():
        memory_types[mem_type] = {
            "count": len(mem_list),
            "description": f"Memories of type '{mem_type}'"
        }

    return {
        "agent_id": agent_id,
        "agent_name": agent.config.name,
        "memory_types": memory_types,
        "total_types": len(memory_types)
    }


# --- NCF Information Routes ---
@app.get("/agents/{agent_id}/ncf-status")
async def get_ncf_status(agent_id: str, current_user: dict = Depends(verify_token)):
    """Get NCF capabilities status for a specific agent"""
    agent = agent_manager.get_agent_instance(agent_id)
    if not agent:
        raise HTTPException(status_code=404, detail="Agent not found")

    # Check access permissions
    if agent.config.user_id != current_user["user_id"]:
        db_agent = db_repo.get_agent(agent_id)
        if not db_agent or not db_agent.is_public:
            raise HTTPException(status_code=403, detail="Access denied")

    return {
        "agent_id": agent_id,
        "agent_name": agent.config.name,
        "ncf_enabled": True,
        "capabilities": {
            "narrative_foundation": True,
            "rag_retrieval": True,
            "reflector_analysis": True,
            "isolated_memory_system": True,
            "contextual_prompting": True,
            "conversation_history_tracking": True,
            "memory_types_supported": ["Explicit", "Emotional", "Procedural", "Flashbulb", "Liminal", "Generative"]
        },
        "memory_statistics": {
            "total_memories": sum(len(m) for m in agent.memory_blossom.memory_stores.values()),
            "memory_stores": {mem_type: len(mem_list) for mem_type, mem_list in
                              agent.memory_blossom.memory_stores.items()},
            "memory_persistence_path": agent.config.memory_path
        },
        "model_info": {
            "model": agent.config.model,
            "llm_instance": "LiteLlm"
        }
    }


# --- Frontend Routes ---
# Serve the frontend HTML
frontend_path = Path(__file__).parent.parent / "frontend"
if frontend_path.exists():
    app.mount("/static", StaticFiles(directory=str(frontend_path)), name="static")


    @app.get("/")
    async def read_index():
        index_path = frontend_path / "index.html"
        if index_path.exists():
            return FileResponse(str(index_path))
        else:
            return {"message": "Aura Multi-Agent API (NCF-Enabled) is running. Frontend not found."}
else:
    @app.get("/")
    async def read_root():
        return {
            "message": "Aura Multi-Agent API (NCF-Enabled) is running",
            "version": "2.0.0",
            "ncf_features": [
                "Narrative Context Framing for every agent",
                "Isolated MemoryBlossom per agent",
                "RAG (Retrieval-Augmented Generation)",
                "Automatic reflection and memory formation",
                "Advanced contextual understanding"
            ],
            "endpoints": {
                "auth": "/auth/register, /auth/login, /auth/me",
                "agents": "/agents/create, /agents/list, /agents/{agent_id}",
                "agent_profile": "/agents/{agent_id}/profile",
                "chat": "/agents/{agent_id}/chat",
                "memories": "/agents/{agent_id}/memories",
                "memory_export": "/agents/{agent_id}/memories/export",
                "memory_upload": "/agents/{agent_id}/memories/upload",
                "memory_analytics": "/agents/{agent_id}/memories/analytics",
                "ncf": "/agents/{agent_id}/ncf-status"
            }
        }


# --- Health Check ---
@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "services": {
            "api": "running",
            "database": "connected",
            "agent_manager": "initialized_with_ncf",
            "ncf_capabilities": "enabled"
        },
        "version": "2.0.0",
        "ncf_enabled": True,
        "enhanced_features": [
            "memory_export_import",
            "agent_profile_editing",
            "memory_analytics",
            "bulk_memory_operations"
        ]
    }


# --- Error Handlers ---
@app.exception_handler(HTTPException)
async def http_exception_handler(request, exc):
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": exc.detail,
            "status_code": exc.status_code,
            "ncf_enabled": True
        }
    )


@app.exception_handler(Exception)
async def general_exception_handler(request, exc):
    logger.error(f"Unhandled exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal server error",
            "status_code": 500,
            "ncf_enabled": True
        }
    )


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8000)

# -------------------- models.py --------------------

# database/models.py
from sqlalchemy import create_engine, Column, String, DateTime, JSON, ForeignKey, Text, Float, Integer
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship, sessionmaker
from datetime import datetime
import uuid

Base = declarative_base()


class User(Base):
    __tablename__ = 'users'

    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    email = Column(String, unique=True, nullable=False)
    username = Column(String, unique=True, nullable=False)
    password_hash = Column(String, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    agents = relationship("Agent", back_populates="user", cascade="all, delete-orphan")
    sessions = relationship("ChatSession", back_populates="user")


class Agent(Base):
    __tablename__ = 'agents'

    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    user_id = Column(String, ForeignKey('users.id'), nullable=False)
    name = Column(String, nullable=False)
    persona = Column(String, nullable=False)
    detailed_persona = Column(Text)
    avatar_url = Column(String)
    model = Column(String, default="openrouter/openai/gpt-4o-mini")
    settings = Column(JSON, default=dict)
    is_public = Column(Integer, default=0)  # 0 = private, 1 = public
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # Memory configuration
    memory_temperature = Column(Float, default=0.7)
    coherence_bias = Column(Float, default=0.6)
    novelty_bias = Column(Float, default=0.4)

    # Relationships
    user = relationship("User", back_populates="agents")
    sessions = relationship("ChatSession", back_populates="agent", cascade="all, delete-orphan")
    memories = relationship("Memory", back_populates="agent", cascade="all, delete-orphan")


class ChatSession(Base):
    __tablename__ = 'sessions'

    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    user_id = Column(String, ForeignKey('users.id'), nullable=False)
    agent_id = Column(String, ForeignKey('agents.id'), nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    last_active = Column(DateTime, default=datetime.utcnow)

    # Relationships
    user = relationship("User", back_populates="sessions")
    agent = relationship("Agent", back_populates="sessions")
    messages = relationship("Message", back_populates="session", cascade="all, delete-orphan")


class Message(Base):
    __tablename__ = 'messages'

    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    session_id = Column(String, ForeignKey('sessions.id'), nullable=False)
    role = Column(String, nullable=False)  # 'user' or 'assistant'
    content = Column(Text, nullable=False)
    created_at = Column(DateTime, default=datetime.utcnow)

    # Relationships
    session = relationship("ChatSession", back_populates="messages")


class Memory(Base): # This is database.models.Memory
    __tablename__ = 'memories'

    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    agent_id = Column(String, ForeignKey('agents.id'), nullable=False)
    content = Column(Text, nullable=False)
    memory_type = Column(String, nullable=False)
    custom_metadata = Column(JSON, default=dict) # RENAMED from metadata

    # Scores
    emotion_score = Column(Float, default=0.0)
    coherence_score = Column(Float, default=0.5)
    novelty_score = Column(Float, default=0.5)
    salience = Column(Float, default=0.5)
    decay_factor = Column(Float, default=1.0)

    # Timestamps
    created_at = Column(DateTime, default=datetime.utcnow)
    last_accessed = Column(DateTime, default=datetime.utcnow)
    access_count = Column(Integer, default=0)

    # Embedding stored as JSON array
    embedding = Column(JSON)

    # Relationships
    agent = relationship("Agent", back_populates="memories")
    connections = relationship("MemoryConnection",
                               foreign_keys="MemoryConnection.source_memory_id",
                               back_populates="source_memory",
                               cascade="all, delete-orphan")


class MemoryConnection(Base):
    __tablename__ = 'memory_connections'

    id = Column(String, primary_key=True, default=lambda: str(uuid.uuid4()))
    source_memory_id = Column(String, ForeignKey('memories.id'), nullable=False)
    target_memory_id = Column(String, ForeignKey('memories.id'), nullable=False)
    strength = Column(Float, default=0.5)
    relation_type = Column(String)

    # Relationships
    source_memory = relationship("Memory", foreign_keys=[source_memory_id])
    target_memory = relationship("Memory", foreign_keys=[target_memory_id])


# Database Repository
class AgentRepository:
    def __init__(self, db_url: str = "sqlite:///aura_agents.db"):
        self.engine = create_engine(db_url)
        Base.metadata.create_all(self.engine)
        self.SessionLocal = sessionmaker(bind=self.engine)

    def create_agent(self, agent_id: str, user_id: str, name: str, persona: str,
                     detailed_persona: str, model: str = None, is_public: bool = False) -> Agent:
        with self.SessionLocal() as session:
            agent = Agent(
                id=agent_id,
                user_id=user_id,
                name=name,
                persona=persona,
                detailed_persona=detailed_persona,
                model=model or "openrouter/openai/gpt-4o-mini",
                is_public=1 if is_public else 0
            )
            session.add(agent)
            session.commit()
            session.refresh(agent)
            return agent

    def get_agent(self, agent_id: str) -> Agent:
        with self.SessionLocal() as session:
            return session.query(Agent).filter(Agent.id == agent_id).first()

    def list_user_agents(self, user_id: str):
        with self.SessionLocal() as session:
            return session.query(Agent).filter(Agent.user_id == user_id).all()

    def delete_agent(self, agent_id: str, user_id: str) -> bool:
        with self.SessionLocal() as session:
            agent = session.query(Agent).filter(
                Agent.id == agent_id,
                Agent.user_id == user_id
            ).first()
            if agent:
                session.delete(agent)
                session.commit()
                return True
            return False

    def save_memory(self, agent_id: str, memory_data: dict): # memory_data is from memory_system.memory_models.Memory.to_dict()
        with self.SessionLocal() as session:
            memory = Memory( # This is database.models.Memory
                agent_id=agent_id,
                content=memory_data['content'],
                memory_type=memory_data['memory_type'],
                custom_metadata=memory_data.get('custom_metadata', {}), # UPDATED KEY
                emotion_score=memory_data.get('emotion_score', 0.0),
                coherence_score=memory_data.get('coherence_score', 0.5),
                novelty_score=memory_data.get('novelty_score', 0.5),
                salience=memory_data.get('salience', 0.5), # Ensure this aligns with memory_system.memory_models.Memory
                embedding=memory_data.get('embedding', []) # Ensure this aligns
            )
            session.add(memory)
            session.commit()
            return memory

    def get_agent_memories(self, agent_id: str, memory_type: str = None, limit: int = 100):
        with self.SessionLocal() as session:
            query = session.query(Memory).filter(Memory.agent_id == agent_id)
            if memory_type:
                query = query.filter(Memory.memory_type == memory_type)
            return query.order_by(Memory.created_at.desc()).limit(limit).all()

    def save_message(self, session_id: str, role: str, content: str):
        with self.SessionLocal() as session:
            message = Message(
                session_id=session_id,
                role=role,
                content=content
            )
            session.add(message)
            session.commit()
            return message

    def get_session_messages(self, session_id: str, limit: int = 50):
        with self.SessionLocal() as session:
            return session.query(Message).filter(
                Message.session_id == session_id
            ).order_by(Message.created_at.desc()).limit(limit).all()

# -------------------- embedding_utils.py --------------------

# memory_system/embedding_utils.py
import numpy as np
from sentence_transformers import SentenceTransformer
from typing import Dict, Optional, List

# Global cache for SentenceTransformer models
_model_cache: Dict[str, SentenceTransformer] = {}

DEFAULT_EMBEDDING_MODEL = "all-MiniLM-L6-v2" # A good general-purpose small model

# --- Model Configuration (can be externalized to a config file) ---
# Based on your MemoryBlossom description
EMBEDDING_MODELS_CONFIG = {
    "Explicit": "BAAI/bge-small-en-v1.5",        # Factual, precise
    "Emotional": "all-MiniLM-L6-v2",             # General, captures affect well enough for a start
                                                 # instructor-xl is large, start smaller
    "Procedural": "sentence-transformers/multi-qa-MiniLM-L6-cos-v1", # Good for how-to, steps
    "Flashbulb": "nomic-ai/nomic-embed-text-v1.5", # High significance
    "Somatic": "clip-ViT-B-32",                  # For multimodal, but use text part for now
                                                 # If SentenceTransformer can't load CLIP directly,
                                                 # you might need a different library or a text-proxy.
                                                 # For simplicity, we'll use a text model if CLIP fails.
    "Liminal": "mixedbread-ai/mxbai-embed-large-v1", # High entropy, exploratory
    "Generative": "all-MiniLM-L6-v2",            # Creative content
    "Default": DEFAULT_EMBEDDING_MODEL
}

def get_embedding_model(model_name_or_type: str) -> SentenceTransformer:
    """
    Loads a SentenceTransformer model from cache or downloads it.
    Can take a memory type (e.g., "Emotional") or a direct model name.
    """
    resolved_model_name = EMBEDDING_MODELS_CONFIG.get(model_name_or_type, model_name_or_type)
    if resolved_model_name not in _model_cache:
        print(f"Loading embedding model: {resolved_model_name} (for type/name: {model_name_or_type})")
        try:
            # nomic-embed-text requires trust_remote_code=True
            trust_code = "nomic-ai" in resolved_model_name
            _model_cache[resolved_model_name] = SentenceTransformer(resolved_model_name, trust_remote_code=trust_code)
        except Exception as e:
            print(f"Warning: Could not load model {resolved_model_name}. Error: {e}. Falling back to {DEFAULT_EMBEDDING_MODEL}.")
            if DEFAULT_EMBEDDING_MODEL not in _model_cache: # Load default if not already loaded
                 _model_cache[DEFAULT_EMBEDDING_MODEL] = SentenceTransformer(DEFAULT_EMBEDDING_MODEL)
            _model_cache[resolved_model_name] = _model_cache[DEFAULT_EMBEDDING_MODEL] # Assign default to failed model name
    return _model_cache[resolved_model_name]

def generate_embedding(text: str, memory_type: Optional[str] = "Default") -> Optional[np.ndarray]:
    """Generates an embedding for the given text using the specified memory_type's model."""
    try:
        model_key = memory_type if memory_type in EMBEDDING_MODELS_CONFIG else "Default"
        model = get_embedding_model(model_key)
        embedding = model.encode(text)
        return embedding
    except Exception as e:
        print(f"Error generating embedding for type '{memory_type}': {e}")
        return None

def cosine_similarity_np(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """Computes cosine similarity between two numpy vectors."""
    if vec1 is None or vec2 is None:
        return 0.0
    # Ensure they are 2D for sklearn's cosine_similarity
    vec1_2d = vec1.reshape(1, -1)
    vec2_2d = vec2.reshape(1, -1)
    # sklearn's cosine_similarity returns a 2D array, e.g., [[similarity]]
    return float(np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2)))


def compute_adaptive_similarity(embedding1: Optional[np.ndarray], embedding2: Optional[np.ndarray]) -> float:
    """
    Compute similarity between embeddings, handling potential None values and dimension differences.
    This is a simplified version. A more advanced one might try to project to a common space
    if dimensions differ significantly and models are known.
    """
    if embedding1 is None or embedding2 is None:
        return 0.0  # Or some other default for missing embeddings

    if embedding1.shape[0] != embedding2.shape[0]:
        # print(f"Warning: Comparing embeddings of different dimensions: {embedding1.shape[0]} vs {embedding2.shape[0]}. Truncating.")
        min_dim = min(embedding1.shape[0], embedding2.shape[0])
        truncated_emb1 = embedding1[:min_dim]
        truncated_emb2 = embedding2[:min_dim]
        # Apply a penalty for dimension mismatch
        dim_difference_ratio = abs(embedding1.shape[0] - embedding2.shape[0]) / max(embedding1.shape[0], embedding2.shape[0])
        similarity_penalty = 0.1 * dim_difference_ratio # e.g. 10% penalty factor for difference
        return max(0.0, cosine_similarity_np(truncated_emb1, truncated_emb2) - similarity_penalty)

    return cosine_similarity_np(embedding1, embedding2)

# -------------------- memory_blossom.py --------------------

# memory_system/memory_blossom.py
import json
import os
from datetime import datetime, timezone
from typing import Dict, List, Any, Optional
import numpy as np
from collections import defaultdict

from .memory_models import Memory # This is memory_system.memory_models.Memory
from .embedding_utils import generate_embedding, compute_adaptive_similarity
# MemoryConnector will be imported conditionally to avoid circular dependency during initialization
# from .memory_connector import MemoryConnector


class MemoryBlossom:
    DEFAULT_MEMORY_STORE_PATH = "memory_blossom_data.json"

    def __init__(self, persistence_path: Optional[str] = None):
        self.memory_stores: Dict[str, List[Memory]] = defaultdict(list)
        # For now, MemoryConnector is initialized after MemoryBlossom
        self.memory_connector = None # To be set by set_memory_connector
        self.persistence_path = persistence_path or self.DEFAULT_MEMORY_STORE_PATH

        # Criticality parameters (can be tuned)
        self.memory_temperature: float = 0.7  # Randomness in selection
        self.coherence_bias: float = 0.6      # Favor structured memories
        self.novelty_bias: float = 0.4        # Favor unique memories

        # Meta-memory (can be expanded)
        self.memory_statistics: Dict[str, int] = defaultdict(int)
        self.memory_transitions: Dict[tuple[str, str], int] = defaultdict(int) # (from_type, to_type) -> count

        self.load_memories() # Load at startup

    def set_memory_connector(self, connector): # Type hint later when MemoryConnector is defined
        """Sets the memory connector after initialization to break circular dependency."""
        self.memory_connector = connector
        if self.memory_connector:
            print("MemoryBlossom: MemoryConnector set. Analyzing all memories.")
            self.memory_connector.analyze_all_memories()


    def add_memory(self,
                   content: str,
                   memory_type: str, # User now specifies type
                   custom_metadata: Optional[Dict[str, Any]] = None, # RENAMED from metadata
                   emotion_score: float = 0.0,
                   coherence_score: float = 0.5,
                   novelty_score: float = 0.5,
                   initial_salience: float = 0.5) -> Memory:
        """
        Add a new memory to the appropriate memory store.
        The caller is responsible for determining the memory_type.
        """
        embedding = generate_embedding(content, memory_type)
        if embedding is None:
            print(f"Warning: Could not generate embedding for memory content: {content[:50]}...")
            # Optionally, decide if memory should still be added without embedding or raise error

        memory = Memory( # This instantiates memory_system.memory_models.Memory
            content=content,
            memory_type=memory_type,
            custom_metadata=custom_metadata, # RENAMED from metadata
            emotion_score=emotion_score,
            embedding=embedding,
            coherence_score=coherence_score,
            novelty_score=novelty_score,
            initial_salience=initial_salience
        )
        self.memory_stores[memory_type].append(memory)
        self.memory_statistics[memory_type] += 1
        print(f"MemoryBlossom: Added '{memory.memory_type}' memory: '{memory.content[:30]}...'")

        if self.memory_connector: # If connector exists, re-analyze
             self.memory_connector.analyze_specific_memory(memory) # Or full re-analysis if cheaper

        return memory

    def retrieve_memories(self,
                          query: str,
                          target_memory_types: Optional[List[str]] = None,
                          top_k: int = 5,
                          min_similarity_threshold: float = 0.3,
                          apply_criticality: bool = True,
                          conversation_context: Optional[List[Dict[str,str]]] = None
                         ) -> List[Memory]:
        """
        Retrieve relevant memories based on query, type, and criticality.
        Now also considers conversation_context for contextual_embedding (conceptual).
        """
        query_embedding_default = generate_embedding(query, "Default")
        if query_embedding_default is None:
            return []

        candidate_memories: List[tuple[Memory, float]] = []
        search_types = target_memory_types if target_memory_types else list(self.memory_stores.keys())

        context_text = ""
        if conversation_context:
            context_text = " ".join([msg.get('content', '') for msg in conversation_context[-3:]]) # Last 3 messages

        for mem_type in search_types:
            if mem_type not in self.memory_stores:
                continue

            # Generate query embedding specific to this memory type's model for best results
            query_embedding_typed = generate_embedding(query, mem_type)
            if query_embedding_typed is None: # Fallback to default if typed model fails
                query_embedding_typed = query_embedding_default

            for memory in self.memory_stores[mem_type]:
                memory.apply_decay() # Apply decay before calculating salience for retrieval

                # Concept: Generate a contextual embedding for the memory if context is available
                # This is a placeholder for a more advanced contextualization step.
                # For now, we'll just use the primary embedding.
                # if context_text and memory.content:
                #     combined_for_contextual_embedding = f"Context: {context_text}\n\nMemory: {memory.content}"
                #     memory.contextual_embedding = generate_embedding(combined_for_contextual_embedding, mem_type)
                # else:
                #     memory.contextual_embedding = memory.embedding

                # Use primary embedding for similarity calculation for now
                similarity = compute_adaptive_similarity(query_embedding_typed, memory.embedding)

                if similarity >= min_similarity_threshold:
                    effective_salience = memory.get_effective_salience()
                    score = similarity * 0.6 + effective_salience * 0.4 # Weighted score

                    if apply_criticality:
                        # Apply criticality to memory selection (edge of chaos)
                        coherence_component = memory.coherence_score * self.coherence_bias
                        novelty_component = memory.novelty_score * self.novelty_bias
                        noise = np.random.normal(0, self.memory_temperature * 0.1) # Smaller noise
                        score = score * 0.7 + coherence_component * 0.1 + novelty_component * 0.1 + noise * 0.1
                    candidate_memories.append((memory, score))

        # Sort by combined score
        candidate_memories.sort(key=lambda x: x[1], reverse=True)
        retrieved = [mem for mem, score in candidate_memories[:top_k]]

        # Enhance with connected memories if MemoryConnector is available
        if self.memory_connector and retrieved:
            # Pass current query and context if available to enhance_retrieval
            retrieved = self.memory_connector.enhance_retrieval(retrieved, query, conversation_context)


        for mem in retrieved: # Update access only for finally retrieved memories
            mem.update_access()

        # Track memory transitions (simplified for now)
        if len(retrieved) > 1:
            self.memory_transitions[(retrieved[0].memory_type, retrieved[1].memory_type)] +=1

        return retrieved

    def get_memory_by_id(self, memory_id: str) -> Optional[Memory]:
        for mem_list in self.memory_stores.values():
            for memory in mem_list:
                if memory.id == memory_id:
                    return memory
        return None

    def update_memory_content(self, memory_id: str, new_content: str) -> bool:
        memory = self.get_memory_by_id(memory_id)
        if memory:
            memory.content = new_content
            # Re-embed
            new_embedding = generate_embedding(new_content, memory.memory_type)
            if new_embedding is not None:
                memory.embedding = new_embedding
            memory.last_accessed = datetime.now(timezone.utc) # Treat update as access
            # Potentially re-evaluate coherence/novelty
            print(f"MemoryBlossom: Updated content for memory {memory_id}")
            return True
        return False

    def add_connection(self, mem_id1: str, mem_id2: str, strength: float, relation_type: str):
        mem1 = self.get_memory_by_id(mem_id1)
        mem2 = self.get_memory_by_id(mem_id2)
        if mem1 and mem2:
            mem1.connections.append((mem_id2, strength, relation_type))
            mem2.connections.append((mem_id1, strength, relation_type)) # Bidirectional
            print(f"MemoryBlossom: Added connection between {mem_id1[:8]} and {mem_id2[:8]}")

    def save_memories(self):
        """Saves all memory stores to a JSON file."""
        data_to_save = {
            "memory_stores": {
                mem_type: [mem.to_dict() for mem in memories] # Uses updated to_dict()
                for mem_type, memories in self.memory_stores.items()
            },
            "memory_statistics": dict(self.memory_statistics),
            "memory_transitions": {str(k): v for k,v in self.memory_transitions.items()}, # Convert tuple keys to str
            "criticality_params": {
                "temperature": self.memory_temperature,
                "coherence_bias": self.coherence_bias,
                "novelty_bias": self.novelty_bias
            }
        }
        try:
            with open(self.persistence_path, 'w') as f:
                json.dump(data_to_save, f, indent=2)
            print(f"MemoryBlossom: Saved memories to {self.persistence_path}")
        except IOError as e:
            print(f"Error saving memories to {self.persistence_path}: {e}")

    def load_memories(self):
        """Loads memories from a JSON file."""
        if not os.path.exists(self.persistence_path):
            print(f"MemoryBlossom: No persistence file found at {self.persistence_path}. Starting fresh.")
            return

        try:
            with open(self.persistence_path, 'r') as f:
                loaded_data = json.load(f)

            raw_memory_stores = loaded_data.get("memory_stores", {})
            for mem_type, mem_data_list in raw_memory_stores.items():
                self.memory_stores[mem_type] = [Memory.from_dict(data) for data in mem_data_list] # Uses updated from_dict()

            self.memory_statistics = defaultdict(int, loaded_data.get("memory_statistics", {}))

            raw_transitions = loaded_data.get("memory_transitions", {})
            self.memory_transitions = defaultdict(int)
            for k_str, v in raw_transitions.items(): # Convert str keys back to tuples
                try:
                    # Example key: "('Explicit', 'Emotional')"
                    key_tuple = tuple(part.strip().strip("'") for part in k_str.strip("()").split(","))
                    if len(key_tuple) == 2:
                         self.memory_transitions[key_tuple] = v
                except Exception as e_parse:
                    print(f"Warning: Could not parse transition key '{k_str}': {e_parse}")


            crit_params = loaded_data.get("criticality_params", {})
            self.memory_temperature = crit_params.get("temperature", 0.7)
            self.coherence_bias = crit_params.get("coherence_bias", 0.6)
            self.novelty_bias = crit_params.get("novelty_bias", 0.4)

            print(f"MemoryBlossom: Loaded memories from {self.persistence_path}")
            # After loading, if a connector is set, it should re-analyze
            if self.memory_connector:
                print("MemoryBlossom: Re-analyzing connections for loaded memories.")
                self.memory_connector.analyze_all_memories()

        except (IOError, json.JSONDecodeError) as e:
            print(f"Error loading memories from {self.persistence_path}: {e}. Starting fresh.")
            self.memory_stores = defaultdict(list) # Reset if loading fails

# -------------------- memory_connector.py --------------------

# memory_system/memory_connector.py
from typing import List, Dict, Tuple, Optional, Set
from collections import defaultdict
import numpy as np

from .memory_models import Memory
from .embedding_utils import compute_adaptive_similarity #, cosine_similarity_np

# Forward declaration for type hinting MemoryBlossom
class MemoryBlossom:
    pass

class MemoryConnector:
    def __init__(self, memory_blossom_instance: MemoryBlossom):
        self.memory_blossom = memory_blossom_instance
        # memory_id -> List of (connected_memory_id, similarity_score, relation_type_str)
        self.connection_graph: Dict[str, List[Tuple[str, float, str]]] = defaultdict(list)
        self.memory_clusters: List[Set[str]] = [] # List of sets, each set is a cluster of memory_ids
        self.semantic_fields: Dict[str, List[str]] = {} # field_name -> list_of_memory_ids

    def _infer_relation_type(self, mem1: Memory, mem2: Memory, similarity: float) -> str:
        """Infers the type of relationship between two memories."""
        # Time-based
        time_diff_hours = abs((mem1.creation_time - mem2.creation_time).total_seconds()) / 3600
        if time_diff_hours < 1 and similarity > 0.6: # Within 1 hour and reasonably similar
            return "temporal_proximity"
        if time_diff_hours < 24 and similarity > 0.5:
            return "same_day_context"

        # Content-based (simple heuristics)
        if mem1.memory_type == mem2.memory_type and similarity > 0.75:
            return f"strong_{mem1.memory_type.lower()}_link"
        if "how to" in mem1.content.lower() and "how to" in mem2.content.lower() and similarity > 0.6:
            return "procedural_similarity"
        if mem1.emotion_score > 0.7 and mem2.emotion_score > 0.7 and similarity > 0.5:
            return "shared_high_emotion"

        # Default semantic link
        if similarity > 0.8: return "strong_semantic"
        if similarity > 0.65: return "moderate_semantic"
        if similarity > 0.5: return "weak_semantic"

        return "related" # Generic fallback

    def analyze_all_memories(self):
        """Analyzes all memories to build the connection graph and identify clusters."""
        print("MemoryConnector: Starting analysis of all memories...")
        all_memories_flat: List[Memory] = []
        for mem_list in self.memory_blossom.memory_stores.values():
            all_memories_flat.extend(mem_list)

        if not all_memories_flat:
            print("MemoryConnector: No memories to analyze.")
            return

        self.connection_graph.clear() # Reset graph

        for i in range(len(all_memories_flat)):
            mem1 = all_memories_flat[i]
            if mem1.embedding is None: continue
            for j in range(i + 1, len(all_memories_flat)):
                mem2 = all_memories_flat[j]
                if mem2.embedding is None: continue

                similarity = compute_adaptive_similarity(mem1.embedding, mem2.embedding)
                if similarity > 0.4: # Threshold for considering a connection
                    relation_type = self._infer_relation_type(mem1, mem2, similarity)
                    self.connection_graph[mem1.id].append((mem2.id, similarity, relation_type))
                    self.connection_graph[mem2.id].append((mem1.id, similarity, relation_type))
        print(f"MemoryConnector: Connection graph built. {len(self.connection_graph)} memories have connections.")
        self._detect_memory_clusters(all_memories_flat)
        self._identify_semantic_fields(all_memories_flat) # conceptual

    def analyze_specific_memory(self, new_memory: Memory):
        """Analyzes a new memory and updates connections involving it."""
        if new_memory.embedding is None: return

        print(f"MemoryConnector: Analyzing new memory {new_memory.id[:8]}...")
        # Remove old connections for this memory if it's an update (not strictly necessary for new)
        if new_memory.id in self.connection_graph:
            for connected_id, _, _ in self.connection_graph[new_memory.id]:
                if connected_id in self.connection_graph:
                    self.connection_graph[connected_id] = [
                        conn for conn in self.connection_graph[connected_id] if conn[0] != new_memory.id
                    ]
            del self.connection_graph[new_memory.id]

        all_memories_flat: List[Memory] = []
        for mem_list in self.memory_blossom.memory_stores.values():
            all_memories_flat.extend(mem_list)

        for existing_memory in all_memories_flat:
            if existing_memory.id == new_memory.id or existing_memory.embedding is None:
                continue
            similarity = compute_adaptive_similarity(new_memory.embedding, existing_memory.embedding)
            if similarity > 0.4:
                relation_type = self._infer_relation_type(new_memory, existing_memory, similarity)
                self.connection_graph[new_memory.id].append((existing_memory.id, similarity, relation_type))
                self.connection_graph[existing_memory.id].append((new_memory.id, similarity, relation_type))
        # Optionally, re-run clustering/semantic field identification if many memories are added/updated frequently
        # For a single new memory, a full re-cluster might be too much.
        # Consider incremental clustering or periodic full re-analysis.


    def _detect_memory_clusters(self, all_memories_flat: List[Memory]):
        """Detects clusters of highly interconnected memories (simplified)."""
        # This is a very simplified clustering. Real applications might use graph algorithms (e.g., Louvain).
        self.memory_clusters.clear()
        visited_ids: Set[str] = set()
        for mem_id in self.connection_graph:
            if mem_id in visited_ids:
                continue

            current_cluster: Set[str] = set()
            queue = [mem_id]
            visited_ids.add(mem_id)
            current_cluster.add(mem_id)

            head = 0
            while head < len(queue):
                curr = queue[head]
                head += 1
                for neighbor_id, similarity, _ in self.connection_graph.get(curr, []):
                    if neighbor_id not in visited_ids and similarity > 0.7: # Higher threshold for strong cluster
                        visited_ids.add(neighbor_id)
                        current_cluster.add(neighbor_id)
                        queue.append(neighbor_id)
            if len(current_cluster) > 1: # Only consider clusters with more than one memory
                self.memory_clusters.append(current_cluster)
        print(f"MemoryConnector: Detected {len(self.memory_clusters)} memory clusters.")

    def _identify_semantic_fields(self, all_memories_flat: List[Memory]):
        """Placeholder for identifying broader semantic fields."""
        # This would be more complex, potentially using dimensionality reduction (PCA/t-SNE)
        # then clustering in the reduced space to find "regions" of meaning.
        # For now, we can make a simplification:
        self.semantic_fields.clear()
        temp_fields = defaultdict(list)
        if not all_memories_flat or not hasattr(all_memories_flat[0], 'memory_type'): return

        for mem in all_memories_flat:
            # Group by primary memory type as a very rough proxy for semantic field
            temp_fields[mem.memory_type].append(mem.id)

        # Filter for fields with enough memories
        for field_name, mem_ids in temp_fields.items():
            if len(mem_ids) > 2: # Arbitrary threshold
                self.semantic_fields[field_name] = mem_ids
        print(f"MemoryConnector: Identified {len(self.semantic_fields)} pseudo-semantic fields (by type).")


    def enhance_retrieval(self,
                          initial_memories: List[Memory],
                          query: str, # Query used for initial retrieval
                          conversation_context: Optional[List[Dict[str,str]]] = None,
                          max_enhanced_memories: int = 20 # Total to return
                          ) -> List[Memory]:
        """Enhances a list of retrieved memories by adding connected/related ones."""
        if not initial_memories:
            return []

        enhanced_set: Dict[str, Memory] = {mem.id: mem for mem in initial_memories}
        # Keep track of scores for sorting later
        scores: Dict[str, float] = {mem.id: 1.0 for mem in initial_memories} # Initial memories have highest "base" score

        # 1. Add directly connected memories
        for mem in initial_memories:
            if mem.id in self.connection_graph:
                # Sort connections by similarity, prefer stronger connections
                sorted_connections = sorted(self.connection_graph[mem.id], key=lambda x: x[1], reverse=True)
                for connected_id, similarity, rel_type in sorted_connections:
                    if connected_id not in enhanced_set and len(enhanced_set) < max_enhanced_memories:
                        connected_mem_obj = self.memory_blossom.get_memory_by_id(connected_id)
                        if connected_mem_obj:
                            enhanced_set[connected_id] = connected_mem_obj
                            scores[connected_id] = scores.get(connected_id, 0) + similarity * 0.8 # Weight connected higher
                    if len(enhanced_set) >= max_enhanced_memories: break
            if len(enhanced_set) >= max_enhanced_memories: break

        # 2. Add memories from shared clusters
        if len(enhanced_set) < max_enhanced_memories:
            for cluster in self.memory_clusters:
                # Check if any of the initial memories are in this cluster
                if any(mem.id in cluster for mem in initial_memories):
                    for mem_id_in_cluster in cluster:
                        if mem_id_in_cluster not in enhanced_set and len(enhanced_set) < max_enhanced_memories:
                            cluster_mem_obj = self.memory_blossom.get_memory_by_id(mem_id_in_cluster)
                            if cluster_mem_obj:
                                enhanced_set[mem_id_in_cluster] = cluster_mem_obj
                                scores[mem_id_in_cluster] = scores.get(mem_id_in_cluster, 0) + 0.5 # Lower weight for cluster
                        if len(enhanced_set) >= max_enhanced_memories: break
                if len(enhanced_set) >= max_enhanced_memories: break

        # 3. Story-Based/Familiar-to-Novel Bridging (Conceptual - needs more sophisticated logic)
        # This part is more about how the *agent uses* the memories,
        # but MemoryConnector can provide candidates.
        # For now, let's try to find "bridge" memories that connect diverse initial memories.
        if len(initial_memories) > 1 and len(enhanced_set) < max_enhanced_memories:
            # Find memories that connect two *different* initial memories
            # This is computationally intensive, simplify for now.
            pass


        # Sort all collected memories by their assigned scores (descending)
        # and return the top N
        final_list = sorted(list(enhanced_set.values()), key=lambda m: scores.get(m.id, 0.0), reverse=True)

        print(f"MemoryConnector: Enhanced retrieval from {len(initial_memories)} to {len(final_list)} (max {max_enhanced_memories})")
        return final_list[:max_enhanced_memories]

# -------------------- memory_models.py --------------------

# memory_system/memory_models.py - FIXED VERSION
import uuid
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional
import numpy as np


class Memory:  # This is memory_system.memory_models.Memory
    """
    A rich memory representation with multiple attributes and embeddings.
    FIXED: Standardized to use 'custom_metadata' consistently.
    """

    def __init__(self,
                 content: str,
                 memory_type: str,  # e.g., Explicit, Emotional, Procedural, etc.
                 custom_metadata: Optional[Dict[str, Any]] = None,  # FIXED: Consistently use custom_metadata
                 emotion_score: float = 0.0,  # Normalized 0-1
                 embedding: Optional[np.ndarray] = None,  # Primary embedding
                 contextual_embedding: Optional[np.ndarray] = None,  # Context-aware
                 coherence_score: float = 0.5,  # How well-structured
                 novelty_score: float = 0.5,  # How unique
                 source_document_id: Optional[str] = None,  # If from a document
                 initial_salience: float = 0.5  # Initial importance
                 ):
        self.id: str = str(uuid.uuid4())
        self.content: str = content
        self.memory_type: str = memory_type
        self.custom_metadata: Dict[str, Any] = custom_metadata or {}  # FIXED: Consistently use custom_metadata
        self.emotion_score: float = emotion_score  # Could be a dict for complex emotions
        self.creation_time: datetime = datetime.now(timezone.utc)
        self.last_accessed: datetime = self.creation_time
        self.access_count: int = 0
        self.decay_factor: float = 1.0  # Memory decay over time (1.0 = no decay)
        self.salience: float = initial_salience  # Importance/prominence of the memory

        self.coherence_score: float = coherence_score
        self.novelty_score: float = novelty_score

        self.embedding: Optional[np.ndarray] = embedding
        self.contextual_embedding: Optional[np.ndarray] = contextual_embedding

        # Connections to other memories: List of tuples (target_memory_id, strength, type_of_relation)
        self.connections: List[tuple[str, float, str]] = []
        self.source_document_id = source_document_id

        # FIXED: Ensure source is set in custom_metadata
        self.custom_metadata.setdefault('source', 'agent_interaction')  # Default source in custom_metadata

    def update_access(self):
        """Update memory access statistics and adjust salience."""
        try:
            self.last_accessed = datetime.now(timezone.utc)
            self.access_count += 1
            # Simple salience boost on access, more complex logic can be added
            self.salience = min(1.0,
                                self.salience + 0.05 * (1 / (1 + self.access_count * 0.1)))  # Diminishing returns boost
        except Exception as e:
            # Log error but don't crash
            import logging
            logging.error(f"Error updating memory access for {self.id}: {e}")

    def apply_decay(self, rate: float = 0.01, time_since_last_access_seconds: Optional[float] = None):
        """Apply memory decay based on time elapsed since last access."""
        try:
            if time_since_last_access_seconds is None:
                time_since_access = (datetime.now(timezone.utc) - self.last_accessed).total_seconds()
            else:
                time_since_access = time_since_last_access_seconds

            # More gradual decay, less aggressive than pure log for very long times
            # Decay is stronger for less salient memories
            decay_amount = rate * (np.log1p(time_since_access / 86400.0)) * (1.0 - self.salience * 0.5)
            self.decay_factor = max(0.1, self.decay_factor - decay_amount)  # Ensure decay_factor doesn't go too low
        except Exception as e:
            # Log error but don't crash
            import logging
            logging.error(f"Error applying decay to memory {self.id}: {e}")

    def get_effective_salience(self) -> float:
        """Get the effective salience considering emotion, access count, and decay."""
        try:
            # Emotional memories might decay slower or have higher base salience
            emotion_boost = 1.0 + (self.emotion_score * 0.2)  # Max 20% boost from emotion

            # Access count increases salience (diminishing returns)
            access_factor = min(1.5, 1.0 + (0.05 * np.log1p(self.access_count)))  # Max 50% boost from access

            return self.salience * emotion_boost * access_factor * self.decay_factor
        except Exception as e:
            # Log error and return base salience
            import logging
            logging.error(f"Error calculating effective salience for memory {self.id}: {e}")
            return self.salience

    def to_dict(self) -> Dict[str, Any]:
        """Convert memory to dictionary representation.
        FIXED: Consistently use 'custom_metadata' in output.
        """
        try:
            return {
                "id": self.id,
                "content": self.content,
                "memory_type": self.memory_type,
                "custom_metadata": self.custom_metadata,  # FIXED: Use custom_metadata consistently
                "emotion_score": self.emotion_score,
                "creation_time": self.creation_time.isoformat(),
                "last_accessed": self.last_accessed.isoformat(),
                "access_count": self.access_count,
                "decay_factor": self.decay_factor,
                "salience": self.salience,
                "coherence_score": self.coherence_score,
                "novelty_score": self.novelty_score,
                "connections": self.connections,
                "source_document_id": self.source_document_id,
                "embedding": self.embedding.tolist() if self.embedding is not None else None,
                "contextual_embedding": self.contextual_embedding.tolist() if self.contextual_embedding is not None else None,
            }
        except Exception as e:
            # Log error and return minimal representation
            import logging
            logging.error(f"Error converting memory {self.id} to dict: {e}")
            return {
                "id": self.id,
                "content": self.content,
                "memory_type": self.memory_type,
                "custom_metadata": self.custom_metadata,
                "error": f"Conversion error: {str(e)}"
            }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Memory':
        """Create memory from dictionary representation.
        FIXED: Handle both 'metadata' and 'custom_metadata' for backward compatibility.
        """
        try:
            # FIXED: Handle both old 'metadata' and new 'custom_metadata' keys for backward compatibility
            custom_metadata = data.get("custom_metadata")
            if custom_metadata is None:
                # Fallback to old 'metadata' key if custom_metadata not found
                custom_metadata = data.get("metadata", {})

            memory = cls(
                content=data["content"],
                memory_type=data["memory_type"],
                custom_metadata=custom_metadata,  # Use the resolved metadata
                emotion_score=data.get("emotion_score", 0.0),
                coherence_score=data.get("coherence_score", 0.5),
                novelty_score=data.get("novelty_score", 0.5),
                source_document_id=data.get("source_document_id"),
                initial_salience=data.get("salience", 0.5)
            )

            # Set additional fields
            memory.id = data["id"]
            memory.creation_time = datetime.fromisoformat(data["creation_time"])
            memory.last_accessed = datetime.fromisoformat(data["last_accessed"])
            memory.access_count = data.get("access_count", 0)
            memory.decay_factor = data.get("decay_factor", 1.0)
            memory.connections = data.get("connections", [])

            # Handle embeddings
            if data.get("embedding") is not None:
                memory.embedding = np.array(data["embedding"])
            if data.get("contextual_embedding") is not None:
                memory.contextual_embedding = np.array(data["contextual_embedding"])

            return memory

        except Exception as e:
            # Log error and create minimal memory
            import logging
            logging.error(f"Error creating memory from dict: {e}")
            # Create a basic memory with error info
            return cls(
                content=data.get("content", "Error loading memory"),
                memory_type=data.get("memory_type", "Error"),
                custom_metadata={"error": f"Load error: {str(e)}", "original_id": data.get("id", "unknown")}
            )

    def __repr__(self):
        try:
            return f"<Memory(id={self.id[:8]}, type='{self.memory_type}', content='{self.content[:30]}...')>"
        except Exception as e:
            return f"<Memory(error={str(e)})>"

